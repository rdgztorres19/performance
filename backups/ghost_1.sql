-- MySQL dump 10.13  Distrib 8.0.44, for Linux (x86_64)
--
-- Host: localhost    Database: blog_prod
-- ------------------------------------------------------
-- Server version	8.0.44-0ubuntu0.22.04.1

/*!40101 SET @OLD_CHARACTER_SET_CLIENT=@@CHARACTER_SET_CLIENT */;
/*!40101 SET @OLD_CHARACTER_SET_RESULTS=@@CHARACTER_SET_RESULTS */;
/*!40101 SET @OLD_COLLATION_CONNECTION=@@COLLATION_CONNECTION */;
/*!50503 SET NAMES utf8mb4 */;
/*!40103 SET @OLD_TIME_ZONE=@@TIME_ZONE */;
/*!40103 SET TIME_ZONE='+00:00' */;
/*!40014 SET @OLD_UNIQUE_CHECKS=@@UNIQUE_CHECKS, UNIQUE_CHECKS=0 */;
/*!40014 SET @OLD_FOREIGN_KEY_CHECKS=@@FOREIGN_KEY_CHECKS, FOREIGN_KEY_CHECKS=0 */;
/*!40101 SET @OLD_SQL_MODE=@@SQL_MODE, SQL_MODE='NO_AUTO_VALUE_ON_ZERO' */;
/*!40111 SET @OLD_SQL_NOTES=@@SQL_NOTES, SQL_NOTES=0 */;

--
-- Table structure for table `actions`
--

DROP TABLE IF EXISTS `actions`;
/*!40101 SET @saved_cs_client     = @@character_set_client */;
/*!50503 SET character_set_client = utf8mb4 */;
CREATE TABLE `actions` (
  `id` varchar(24) NOT NULL,
  `resource_id` varchar(24) DEFAULT NULL,
  `resource_type` varchar(50) NOT NULL,
  `actor_id` varchar(24) NOT NULL,
  `actor_type` varchar(50) NOT NULL,
  `event` varchar(50) NOT NULL,
  `context` text,
  `created_at` datetime NOT NULL,
  PRIMARY KEY (`id`)
) ENGINE=InnoDB DEFAULT CHARSET=utf8mb4 COLLATE=utf8mb4_0900_ai_ci;
/*!40101 SET character_set_client = @saved_cs_client */;

--
-- Dumping data for table `actions`
--

LOCK TABLES `actions` WRITE;
/*!40000 ALTER TABLE `actions` DISABLE KEYS */;
INSERT INTO `actions` VALUES ('694f267cc3e44e650f0e60ab','694f2666c3e44e650f0e6033','setting','694f2665c3e44e650f0e5dd8','user','edited','{\"key\":\"description\",\"group\":\"site\"}','2025-12-27 00:21:16'),('694f267cc3e44e650f0e60ac','694f2666c3e44e650f0e6032','setting','694f2665c3e44e650f0e5dd8','user','edited','{\"key\":\"title\",\"group\":\"site\"}','2025-12-27 00:21:16'),('694f267dc3e44e650f0e60ae','694f2665c3e44e650f0e5dd8','user','694f2665c3e44e650f0e5dd8','user','edited','{\"primary_name\":\"Manuel A Rodriguez\"}','2025-12-27 00:21:17'),('694f267dc3e44e650f0e60af','694f2665c3e44e650f0e5dd8','user','694f2665c3e44e650f0e5dd8','user','edited','{\"primary_name\":\"Manuel A Rodriguez\"}','2025-12-27 00:21:17'),('694f267fc3e44e650f0e60b1','694f2665c3e44e650f0e5dd8','user','694f2665c3e44e650f0e5dd8','user','edited','{\"primary_name\":\"Manuel A Rodriguez\"}','2025-12-27 00:21:19'),('694f269bc3e44e650f0e60b2','694f2666c3e44e650f0e6037','setting','694f2665c3e44e650f0e5dd8','user','edited','{\"key\":\"cover_image\",\"group\":\"site\"}','2025-12-27 00:21:47'),('694f269dc3e44e650f0e60b3','694f2665c3e44e650f0e5dd8','user','694f2665c3e44e650f0e5dd8','user','edited','{\"primary_name\":\"Manuel A Rodriguez\"}','2025-12-27 00:21:49'),('694f26a5c3e44e650f0e60b4','694f2665c3e44e650f0e5dd8','user','694f2665c3e44e650f0e5dd8','user','edited','{\"primary_name\":\"Manuel A Rodriguez\"}','2025-12-27 00:21:57'),('694f26a6c3e44e650f0e60b8','694f26a6c3e44e650f0e60b6','member','694f2665c3e44e650f0e5dd8','user','added','{\"primary_name\":\"Manuel A Rodriguez\"}','2025-12-27 00:21:58'),('694f26aac3e44e650f0e60bc','694f2665c3e44e650f0e5dd8','user','694f2665c3e44e650f0e5dd8','user','edited','{\"primary_name\":\"Manuel A Rodriguez\"}','2025-12-27 00:22:02'),('694f26b1c3e44e650f0e60bd','694f2665c3e44e650f0e5dd8','user','694f2665c3e44e650f0e5dd8','user','edited','{\"primary_name\":\"Manuel A Rodriguez\"}','2025-12-27 00:22:09'),('694f2c677f4d7d685707e3f3','694f2c677f4d7d685707e3f2','api_key','694f2665c3e44e650f0e5dd8','user','added','{}','2025-12-27 00:46:31'),('694f2c677f4d7d685707e3f5','694f2c677f4d7d685707e3f4','api_key','694f2665c3e44e650f0e5dd8','user','added','{}','2025-12-27 00:46:31'),('694f2c677f4d7d685707e3f6','694f2c677f4d7d685707e3f1','integration','694f2665c3e44e650f0e5dd8','user','added','{\"primary_name\":\"performance\"}','2025-12-27 00:46:31'),('694f2cf27f4d7d685707e3f8','694f2cf27f4d7d685707e3f7','tag','694f2c677f4d7d685707e3f1','integration','added','{\"primary_name\":\"Hardware & Operating System\"}','2025-12-27 00:48:50'),('694f2cf37f4d7d685707e3fa','694f2cf37f4d7d685707e3f9','tag','694f2c677f4d7d685707e3f1','integration','added','{\"primary_name\":\"CPU Optimization\"}','2025-12-27 00:48:51'),('694f2cf37f4d7d685707e3fc','694f2cf37f4d7d685707e3fb','tag','694f2c677f4d7d685707e3f1','integration','added','{\"primary_name\":\"Performance\"}','2025-12-27 00:48:51'),('694f2cf37f4d7d685707e3fe','694f2cf37f4d7d685707e3fd','tag','694f2c677f4d7d685707e3f1','integration','added','{\"primary_name\":\"Optimization\"}','2025-12-27 00:48:51'),('694f2cf37f4d7d685707e400','694f2cf37f4d7d685707e3ff','tag','694f2c677f4d7d685707e3f1','integration','added','{\"primary_name\":\"System Design\"}','2025-12-27 00:48:51'),('694f2cf37f4d7d685707e402','694f2cf37f4d7d685707e401','tag','694f2c677f4d7d685707e3f1','integration','added','{\"primary_name\":\"Architecture\"}','2025-12-27 00:48:51'),('694f2cf47f4d7d685707e40c','694f2cf37f4d7d685707e403','post','694f2c677f4d7d685707e3f1','integration','added','{\"type\":\"post\",\"primary_name\":\"Prefer Fewer Fast CPU Cores Over Many Slow Ones for Latency-Sensitive Workloads\"}','2025-12-27 00:48:52'),('694f2d027f4d7d685707e40d','694f2665c3e44e650f0e5e68','post','694f2665c3e44e650f0e5dd8','user','deleted','{\"type\":\"post\",\"primary_name\":\"Coming soon\"}','2025-12-27 00:49:06'),('694f2db57f4d7d685707e40e','694f2666c3e44e650f0e603c','setting','694f2665c3e44e650f0e5dd8','user','edited','{\"key\":\"codeinjection_head\",\"group\":\"site\"}','2025-12-27 00:52:05'),('694f2e4f7f4d7d685707e412','694f2e4f7f4d7d685707e40f','post','694f2665c3e44e650f0e5dd8','user','added','{\"type\":\"post\",\"primary_name\":\"Tesmp\"}','2025-12-27 00:54:39'),('694f2e5c7f4d7d685707e414','694f2e4f7f4d7d685707e40f','post','694f2665c3e44e650f0e5dd8','user','edited','{\"type\":\"post\",\"primary_name\":\"Tesmp\"}','2025-12-27 00:54:52'),('694f2e617f4d7d685707e416','694f2e4f7f4d7d685707e40f','post','694f2665c3e44e650f0e5dd8','user','edited','{\"type\":\"post\",\"primary_name\":\"Tesmp\"}','2025-12-27 00:54:57'),('694f2e787f4d7d685707e417','694f2e4f7f4d7d685707e40f','post','694f2665c3e44e650f0e5dd8','user','deleted','{\"type\":\"post\",\"primary_name\":\"Tesmp\"}','2025-12-27 00:55:20'),('694f2fbd7f4d7d685707e418','694f2666c3e44e650f0e6033','setting','694f2665c3e44e650f0e5dd8','user','edited','{\"key\":\"description\",\"group\":\"site\"}','2025-12-27 01:00:45'),('694f2fbd7f4d7d685707e419','694f2666c3e44e650f0e6032','setting','694f2665c3e44e650f0e5dd8','user','edited','{\"key\":\"title\",\"group\":\"site\"}','2025-12-27 01:00:45'),('694f30937f4d7d685707e41b','694f2cf37f4d7d685707e403','post','694f2665c3e44e650f0e5dd8','user','edited','{\"type\":\"post\",\"primary_name\":\"Prefer Fewer Fast CPU Cores Over Many Slow Ones for Latency-Sensitive Workloads\"}','2025-12-27 01:04:19'),('694f36027f4d7d685707e425','694f36027f4d7d685707e41c','post','694f2c677f4d7d685707e3f1','integration','added','{\"type\":\"post\",\"primary_name\":\"Prefer Fewer Fast CPU Cores Over Many Slow Ones for Latency-Sensitive Workloads\"}','2025-12-27 01:27:30'),('694f36087f4d7d685707e427','694f36027f4d7d685707e41c','post','694f2665c3e44e650f0e5dd8','user','edited','{\"type\":\"post\",\"primary_name\":\"Prefer Fewer Fast CPU Cores Over Many Slow Ones for Latency-Sensitive Workloads\"}','2025-12-27 01:27:36'),('694f36147f4d7d685707e428','694f36027f4d7d685707e41c','post','694f2665c3e44e650f0e5dd8','user','deleted','{\"type\":\"post\",\"primary_name\":\"Prefer Fewer Fast CPU Cores Over Many Slow Ones for Latency-Sensitive Workloads\"}','2025-12-27 01:27:48'),('694f68dc7f4d7d685707e432','694f68dc7f4d7d685707e429','post','694f2c677f4d7d685707e3f1','integration','added','{\"type\":\"post\",\"primary_name\":\"Prefer Fewer Fast CPU Cores Over Many Slow Ones for Latency-Sensitive Workloads\"}','2025-12-27 05:04:28'),('694f68f77f4d7d685707e433','694f2cf37f4d7d685707e403','post','694f2665c3e44e650f0e5dd8','user','deleted','{\"type\":\"post\",\"primary_name\":\"Prefer Fewer Fast CPU Cores Over Many Slow Ones for Latency-Sensitive Workloads\"}','2025-12-27 05:04:55'),('694f68f97f4d7d685707e435','694f68dc7f4d7d685707e429','post','694f2665c3e44e650f0e5dd8','user','edited','{\"type\":\"post\",\"primary_name\":\"Prefer Fewer Fast CPU Cores Over Many Slow Ones for Latency-Sensitive Workloads\"}','2025-12-27 05:04:57'),('694ff07f19c94fae00e3734e','694ff07e19c94fae00e37345','post','694f2c677f4d7d685707e3f1','integration','added','{\"type\":\"post\",\"primary_name\":\"Prefer Fewer Fast CPU Cores Over Many Slow Ones for Latency-Sensitive Workloads\"}','2025-12-27 14:43:11'),('694ff0a119c94fae00e3734f','694f68dc7f4d7d685707e429','post','694f2665c3e44e650f0e5dd8','user','deleted','{\"type\":\"post\",\"primary_name\":\"Prefer Fewer Fast CPU Cores Over Many Slow Ones for Latency-Sensitive Workloads\"}','2025-12-27 14:43:45'),('694ff0a319c94fae00e37351','694ff07e19c94fae00e37345','post','694f2665c3e44e650f0e5dd8','user','edited','{\"type\":\"post\",\"primary_name\":\"Prefer Fewer Fast CPU Cores Over Many Slow Ones for Latency-Sensitive Workloads\"}','2025-12-27 14:43:47'),('69502ec119c94fae00e37357','69502ec119c94fae00e37352','post','694f2c677f4d7d685707e3f1','integration','added','{\"type\":\"post\",\"primary_name\":\"Reduce Context Switching\"}','2025-12-27 19:08:49'),('69502ee919c94fae00e37359','69502ec119c94fae00e37352','post','694f2665c3e44e650f0e5dd8','user','edited','{\"type\":\"post\",\"primary_name\":\"Reduce Context Switching\"}','2025-12-27 19:09:29'),('6950310819c94fae00e3735a','69502ec119c94fae00e37352','post','694f2665c3e44e650f0e5dd8','user','deleted','{\"type\":\"post\",\"primary_name\":\"Reduce Context Switching\"}','2025-12-27 19:18:32'),('695033b619c94fae00e37360','695033b619c94fae00e3735b','post','694f2c677f4d7d685707e3f1','integration','added','{\"type\":\"post\",\"primary_name\":\"Reduce Context Switching\"}','2025-12-27 19:29:58'),('6950348b19c94fae00e37361','695033b619c94fae00e3735b','post','694f2665c3e44e650f0e5dd8','user','deleted','{\"type\":\"post\",\"primary_name\":\"Reduce Context Switching\"}','2025-12-27 19:33:31'),('6950348e19c94fae00e37367','6950348e19c94fae00e37362','post','694f2c677f4d7d685707e3f1','integration','added','{\"type\":\"post\",\"primary_name\":\"Reduce Context Switching\"}','2025-12-27 19:33:34'),('695036d119c94fae00e37369','6950348e19c94fae00e37362','post','694f2665c3e44e650f0e5dd8','user','edited','{\"type\":\"post\",\"primary_name\":\"Reduce Context Switching\"}','2025-12-27 19:43:13'),('69503e1f19c94fae00e3736b','6950348e19c94fae00e37362','post','694f2665c3e44e650f0e5dd8','user','edited','{\"type\":\"post\",\"primary_name\":\"Reduce Context Switching\"}','2025-12-27 20:14:23'),('69503e2319c94fae00e3736d','6950348e19c94fae00e37362','post','694f2665c3e44e650f0e5dd8','user','edited','{\"type\":\"post\",\"primary_name\":\"Reduce Context Switching\"}','2025-12-27 20:14:27'),('69503e9419c94fae00e3736f','6950348e19c94fae00e37362','post','694f2665c3e44e650f0e5dd8','user','edited','{\"type\":\"post\",\"primary_name\":\"Reduce Context Switching\"}','2025-12-27 20:16:20'),('695081b219c94fae00e3737a','694f2666c3e44e650f0e604a','setting','694f2665c3e44e650f0e5dd8','user','edited','{\"key\":\"active_theme\",\"group\":\"theme\"}','2025-12-28 01:02:42'),('695081fa19c94fae00e37386','694f2666c3e44e650f0e604a','setting','694f2665c3e44e650f0e5dd8','user','edited','{\"key\":\"active_theme\",\"group\":\"theme\"}','2025-12-28 01:03:54'),('6950821719c94fae00e37387','694f2666c3e44e650f0e604a','setting','694f2665c3e44e650f0e5dd8','user','edited','{\"key\":\"active_theme\",\"group\":\"theme\"}','2025-12-28 01:04:23'),('6950823819c94fae00e3738d','694f2666c3e44e650f0e604a','setting','694f2665c3e44e650f0e5dd8','user','edited','{\"key\":\"active_theme\",\"group\":\"theme\"}','2025-12-28 01:04:56'),('6950825c19c94fae00e37398','694f2666c3e44e650f0e604a','setting','694f2665c3e44e650f0e5dd8','user','edited','{\"key\":\"active_theme\",\"group\":\"theme\"}','2025-12-28 01:05:32'),('6950826b19c94fae00e37399','694f2666c3e44e650f0e604a','setting','694f2665c3e44e650f0e5dd8','user','edited','{\"key\":\"active_theme\",\"group\":\"theme\"}','2025-12-28 01:05:47'),('6950827719c94fae00e3739a','694f2666c3e44e650f0e604a','setting','694f2665c3e44e650f0e5dd8','user','edited','{\"key\":\"active_theme\",\"group\":\"theme\"}','2025-12-28 01:05:59'),('6951521f19c94fae00e373a0','6951521f19c94fae00e3739b','post','694f2c677f4d7d685707e3f1','integration','added','{\"type\":\"post\",\"primary_name\":\"Pin Processes and Threads to Specific CPU Cores for Predictable Performance\"}','2025-12-28 15:51:59'),('6951932219c94fae00e373a1','694f2665c3e44e650f0e5dd8','user','694f2665c3e44e650f0e5dd8','user','edited','{\"primary_name\":\"Manuel A Rodriguez\"}','2025-12-28 20:29:22'),('695193f619c94fae00e373a4','695193f619c94fae00e373a3','api_key','694f2665c3e44e650f0e5dd8','user','added','{}','2025-12-28 20:32:54'),('695193f619c94fae00e373a6','695193f619c94fae00e373a5','api_key','694f2665c3e44e650f0e5dd8','user','added','{}','2025-12-28 20:32:54'),('695193f619c94fae00e373a7','695193f619c94fae00e373a2','integration','694f2665c3e44e650f0e5dd8','user','added','{\"primary_name\":\"Google Analytics\"}','2025-12-28 20:32:54'),('6951941c19c94fae00e373a8','695193f619c94fae00e373a3','api_key','694f2665c3e44e650f0e5dd8','user','deleted','{}','2025-12-28 20:33:32'),('6951941c19c94fae00e373a9','695193f619c94fae00e373a5','api_key','694f2665c3e44e650f0e5dd8','user','deleted','{}','2025-12-28 20:33:32'),('6951941c19c94fae00e373aa','695193f619c94fae00e373a2','integration','694f2665c3e44e650f0e5dd8','user','deleted','{\"primary_name\":\"Google Analytics\"}','2025-12-28 20:33:32'),('695194c019c94fae00e373ab','694f2666c3e44e650f0e603c','setting','694f2665c3e44e650f0e5dd8','user','edited','{\"key\":\"codeinjection_head\",\"group\":\"site\"}','2025-12-28 20:36:16'),('6951959119c94fae00e373ac','694f2666c3e44e650f0e603c','setting','694f2665c3e44e650f0e5dd8','user','edited','{\"key\":\"codeinjection_head\",\"group\":\"site\"}','2025-12-28 20:39:45'),('6951b07819c94fae00e373b0','6951b07819c94fae00e373ad','post','694f2665c3e44e650f0e5dd8','user','added','{\"type\":\"page\",\"primary_name\":\"Privacy Policy\"}','2025-12-28 22:34:32'),('6951b09719c94fae00e373b2','6951b07819c94fae00e373ad','post','694f2665c3e44e650f0e5dd8','user','edited','{\"type\":\"page\",\"primary_name\":\"Privacy Policy\"}','2025-12-28 22:35:03'),('6951b0a919c94fae00e373b4','6951b07819c94fae00e373ad','post','694f2665c3e44e650f0e5dd8','user','edited','{\"type\":\"page\",\"primary_name\":\"Privacy Policy\"}','2025-12-28 22:35:21'),('6951b0ef19c94fae00e373b8','6951b0ef19c94fae00e373b5','post','694f2665c3e44e650f0e5dd8','user','added','{\"type\":\"page\",\"primary_name\":\"Terms of Service\"}','2025-12-28 22:36:31'),('6951b0f819c94fae00e373ba','6951b0ef19c94fae00e373b5','post','694f2665c3e44e650f0e5dd8','user','edited','{\"type\":\"page\",\"primary_name\":\"Terms of Service\"}','2025-12-28 22:36:40'),('6951b10019c94fae00e373bc','6951b0ef19c94fae00e373b5','post','694f2665c3e44e650f0e5dd8','user','edited','{\"type\":\"page\",\"primary_name\":\"Terms of Service\"}','2025-12-28 22:36:48'),('6951b1c119c94fae00e373c0','6951b1c119c94fae00e373bd','post','694f2665c3e44e650f0e5dd8','user','added','{\"type\":\"page\",\"primary_name\":\"Contact\"}','2025-12-28 22:40:01'),('6951b1c819c94fae00e373c2','6951b1c119c94fae00e373bd','post','694f2665c3e44e650f0e5dd8','user','edited','{\"type\":\"page\",\"primary_name\":\"Contact\"}','2025-12-28 22:40:08'),('6951b1d319c94fae00e373c3','6951b1c119c94fae00e373bd','post','694f2665c3e44e650f0e5dd8','user','edited','{\"type\":\"page\",\"primary_name\":\"Contact\"}','2025-12-28 22:40:19'),('6951b1d319c94fae00e373c5','6951b1c119c94fae00e373bd','post','694f2665c3e44e650f0e5dd8','user','edited','{\"type\":\"page\",\"primary_name\":\"Contact\"}','2025-12-28 22:40:19'),('6951c83119c94fae00e373c6','694f2666c3e44e650f0e6040','setting','694f2665c3e44e650f0e5dd8','user','edited','{\"key\":\"navigation\",\"group\":\"site\"}','2025-12-29 00:15:45'),('6951c88119c94fae00e373c8','6951b07819c94fae00e373ad','post','694f2665c3e44e650f0e5dd8','user','edited','{\"type\":\"page\",\"primary_name\":\"Privacy Policy\"}','2025-12-29 00:17:05'),('6951c89a19c94fae00e373ca','6951b0ef19c94fae00e373b5','post','694f2665c3e44e650f0e5dd8','user','edited','{\"type\":\"page\",\"primary_name\":\"Terms of Service\"}','2025-12-29 00:17:30'),('6951c8ae19c94fae00e373cb','694f2666c3e44e650f0e6040','setting','694f2665c3e44e650f0e5dd8','user','edited','{\"key\":\"navigation\",\"group\":\"site\"}','2025-12-29 00:17:50'),('6951c94119c94fae00e373cd','6951b1c119c94fae00e373bd','post','694f2665c3e44e650f0e5dd8','user','edited','{\"type\":\"page\",\"primary_name\":\"Contact\"}','2025-12-29 00:20:17'),('69520b9b19c94fae00e373cf','6950348e19c94fae00e37362','post','694f2665c3e44e650f0e5dd8','user','edited','{\"type\":\"post\",\"primary_name\":\"Reduce Context Switching\"}','2025-12-29 05:03:23'),('69520baa19c94fae00e373d1','6950348e19c94fae00e37362','post','694f2665c3e44e650f0e5dd8','user','edited','{\"type\":\"post\",\"primary_name\":\"Reduce Context Switching\"}','2025-12-29 05:03:38'),('69520bc719c94fae00e373d3','6950348e19c94fae00e37362','post','694f2665c3e44e650f0e5dd8','user','edited','{\"type\":\"post\",\"primary_name\":\"Reduce Context Switching\"}','2025-12-29 05:04:07'),('69520df919c94fae00e373d5','6951521f19c94fae00e3739b','post','694f2665c3e44e650f0e5dd8','user','edited','{\"type\":\"post\",\"primary_name\":\"Pin Processes and Threads to Specific CPU Cores for Predictable Performance\"}','2025-12-29 05:13:29'),('69520eb019c94fae00e373d7','6951521f19c94fae00e3739b','post','694f2665c3e44e650f0e5dd8','user','edited','{\"type\":\"post\",\"primary_name\":\"Pin Processes and Threads to Specific CPU Cores for Predictable Performance\"}','2025-12-29 05:16:32'),('695210e019c94fae00e373d9','6951521f19c94fae00e3739b','post','694f2665c3e44e650f0e5dd8','user','edited','{\"type\":\"post\",\"primary_name\":\"Pin Processes and Threads to Specific CPU Cores for Predictable Performance\"}','2025-12-29 05:25:52'),('69532d98ebcfaae683eec989','69532d98ebcfaae683eec988','tag','694f2c677f4d7d685707e3f1','integration','added','{\"primary_name\":\"Concurrency\"}','2025-12-30 01:40:40'),('69532d98ebcfaae683eec98b','69532d98ebcfaae683eec98a','tag','694f2c677f4d7d685707e3f1','integration','added','{\"primary_name\":\"Threading\"}','2025-12-30 01:40:40'),('69532d98ebcfaae683eec98d','69532d98ebcfaae683eec98c','tag','694f2c677f4d7d685707e3f1','integration','added','{\"primary_name\":\"Memory Management\"}','2025-12-30 01:40:40'),('69532d99ebcfaae683eec98f','69532d99ebcfaae683eec98e','tag','694f2c677f4d7d685707e3f1','integration','added','{\"primary_name\":\"Cache Strategies\"}','2025-12-30 01:40:41'),('69532d99ebcfaae683eec991','69532d99ebcfaae683eec990','tag','694f2c677f4d7d685707e3f1','integration','added','{\"primary_name\":\"Scalability\"}','2025-12-30 01:40:41'),('69532d9aebcfaae683eec993','69532d9aebcfaae683eec992','tag','694f2c677f4d7d685707e3f1','integration','added','{\"primary_name\":\".NET Performance\"}','2025-12-30 01:40:42'),('69532d9aebcfaae683eec995','69532d9aebcfaae683eec994','tag','694f2c677f4d7d685707e3f1','integration','added','{\"primary_name\":\"C# Performance\"}','2025-12-30 01:40:42'),('69532d9bebcfaae683eec9a4','69532d9aebcfaae683eec996','post','694f2c677f4d7d685707e3f1','integration','added','{\"type\":\"post\",\"primary_name\":\"Avoid False Sharing and Cache Line Contention\"}','2025-12-30 01:40:43'),('69532daaebcfaae683eec9a6','69532d9aebcfaae683eec996','post','694f2665c3e44e650f0e5dd8','user','edited','{\"type\":\"post\",\"primary_name\":\"Avoid False Sharing and Cache Line Contention\"}','2025-12-30 01:40:58'),('69532e3aebcfaae683eec9a8','69532d9aebcfaae683eec996','post','694f2665c3e44e650f0e5dd8','user','edited','{\"type\":\"post\",\"primary_name\":\"Avoid False Sharing and Cache Line Contention\"}','2025-12-30 01:43:22'),('6953d77bebcfaae683eec9a9','694f2666c3e44e650f0e6038','setting','694f2665c3e44e650f0e5dd8','user','edited','{\"key\":\"icon\",\"group\":\"site\"}','2025-12-30 13:45:31'),('6953d7deebcfaae683eec9aa','694f2666c3e44e650f0e6036','setting','694f2665c3e44e650f0e5dd8','user','edited','{\"key\":\"logo\",\"group\":\"site\"}','2025-12-30 13:47:10'),('6953d7deebcfaae683eec9ab','694f2666c3e44e650f0e6038','setting','694f2665c3e44e650f0e5dd8','user','edited','{\"key\":\"icon\",\"group\":\"site\"}','2025-12-30 13:47:10'),('6954500cebcfaae683eec9ad','6954500cebcfaae683eec9ac','tag','694f2c677f4d7d685707e3f1','integration','added','{\"primary_name\":\"Compiler Optimization\"}','2025-12-30 22:19:56'),('6954500debcfaae683eec9af','6954500debcfaae683eec9ae','tag','694f2c677f4d7d685707e3f1','integration','added','{\"primary_name\":\"Profiling\"}','2025-12-30 22:19:57'),('6954500debcfaae683eec9b1','6954500debcfaae683eec9b0','tag','694f2c677f4d7d685707e3f1','integration','added','{\"primary_name\":\"Algorithms\"}','2025-12-30 22:19:57'),('6954500eebcfaae683eec9be','6954500debcfaae683eec9b2','post','694f2c677f4d7d685707e3f1','integration','added','{\"type\":\"post\",\"primary_name\":\"Optimize Branch Prediction for Better CPU Pipeline Utilization\"}','2025-12-30 22:19:58'),('69545028ebcfaae683eec9c0','6954500debcfaae683eec9b2','post','694f2665c3e44e650f0e5dd8','user','edited','{\"type\":\"post\",\"primary_name\":\"Optimize Branch Prediction for Better CPU Pipeline Utilization\"}','2025-12-30 22:20:24'),('69545034ebcfaae683eec9c2','6954500debcfaae683eec9b2','post','694f2665c3e44e650f0e5dd8','user','edited','{\"type\":\"post\",\"primary_name\":\"Optimize Branch Prediction for Better CPU Pipeline Utilization\"}','2025-12-30 22:20:36'),('6956d502ebcfaae683eec9c4','6956d502ebcfaae683eec9c3','tag','694f2c677f4d7d685707e3f1','integration','added','{\"primary_name\":\"Operating System Tuning\"}','2026-01-01 20:11:46'),('6956d502ebcfaae683eec9c6','6956d502ebcfaae683eec9c5','tag','694f2c677f4d7d685707e3f1','integration','added','{\"primary_name\":\"Thread Pools\"}','2026-01-01 20:11:46'),('6956d502ebcfaae683eec9c8','6956d502ebcfaae683eec9c7','tag','694f2c677f4d7d685707e3f1','integration','added','{\"primary_name\":\"Async/Await\"}','2026-01-01 20:11:46'),('6956d503ebcfaae683eec9d6','6956d503ebcfaae683eec9c9','post','694f2c677f4d7d685707e3f1','integration','added','{\"type\":\"post\",\"primary_name\":\"Avoid Busy-Wait Loops: Use Proper Synchronization Primitives\"}','2026-01-01 20:11:47'),('6956d50eebcfaae683eec9d8','6956d503ebcfaae683eec9c9','post','694f2665c3e44e650f0e5dd8','user','edited','{\"type\":\"post\",\"primary_name\":\"Avoid Busy-Wait Loops: Use Proper Synchronization Primitives\"}','2026-01-01 20:11:58'),('6956d518ebcfaae683eec9da','6956d503ebcfaae683eec9c9','post','694f2665c3e44e650f0e5dd8','user','edited','{\"type\":\"post\",\"primary_name\":\"Avoid Busy-Wait Loops: Use Proper Synchronization Primitives\"}','2026-01-01 20:12:08');
/*!40000 ALTER TABLE `actions` ENABLE KEYS */;
UNLOCK TABLES;

--
-- Table structure for table `api_keys`
--

DROP TABLE IF EXISTS `api_keys`;
/*!40101 SET @saved_cs_client     = @@character_set_client */;
/*!50503 SET character_set_client = utf8mb4 */;
CREATE TABLE `api_keys` (
  `id` varchar(24) NOT NULL,
  `type` varchar(50) NOT NULL,
  `secret` varchar(191) NOT NULL,
  `role_id` varchar(24) DEFAULT NULL,
  `integration_id` varchar(24) DEFAULT NULL,
  `user_id` varchar(24) DEFAULT NULL,
  `last_seen_at` datetime DEFAULT NULL,
  `last_seen_version` varchar(50) DEFAULT NULL,
  `created_at` datetime NOT NULL,
  `updated_at` datetime DEFAULT NULL,
  PRIMARY KEY (`id`),
  UNIQUE KEY `api_keys_secret_unique` (`secret`)
) ENGINE=InnoDB DEFAULT CHARSET=utf8mb4 COLLATE=utf8mb4_0900_ai_ci;
/*!40101 SET character_set_client = @saved_cs_client */;

--
-- Dumping data for table `api_keys`
--

LOCK TABLES `api_keys` WRITE;
/*!40000 ALTER TABLE `api_keys` DISABLE KEYS */;
INSERT INTO `api_keys` VALUES ('694f2666c3e44e650f0e5e6d','admin','cc2874d8e119186f7d8939f57ce4cd5defe14c5b337960421a7c0af313dffcfa','694f2665c3e44e650f0e5dde','694f2666c3e44e650f0e5e6c',NULL,NULL,NULL,'2025-12-27 00:20:54','2025-12-27 00:20:54'),('694f2666c3e44e650f0e5e6f','admin','075358cab5a2371275f6225ae750455f8123d374833b7860b82bc1b441b72c08','694f2665c3e44e650f0e5ddf','694f2666c3e44e650f0e5e6e',NULL,NULL,NULL,'2025-12-27 00:20:54','2025-12-27 00:20:54'),('694f2666c3e44e650f0e5e71','admin','e9092493b89c606900fe55f280ee46c2822ce1c0ab6f138e03656061e650b8b2','694f2665c3e44e650f0e5de0','694f2666c3e44e650f0e5e70',NULL,NULL,NULL,'2025-12-27 00:20:54','2025-12-27 00:20:54'),('694f2666c3e44e650f0e5e73','admin','59de193d906b60b0a90626580d14c506a4b464934427be1c44557db3d20268e4','694f2665c3e44e650f0e5de1','694f2666c3e44e650f0e5e72',NULL,NULL,NULL,'2025-12-27 00:20:54','2025-12-27 00:20:54'),('694f2666c3e44e650f0e5e75','admin','db88facae5da33462bbb1a4a7df8837a5e791888cc3a44cfcb6047f74cbeeb80','694f2665c3e44e650f0e5de2','694f2666c3e44e650f0e5e74',NULL,NULL,NULL,'2025-12-27 00:20:54','2025-12-27 00:20:54'),('694f2666c3e44e650f0e5e77','content','567ceef764cc169394967bc446',NULL,'694f2666c3e44e650f0e5e76',NULL,NULL,NULL,'2025-12-27 00:20:54','2025-12-27 00:20:54'),('694f2666c3e44e650f0e5e79','content','c549d6db9666a564bed4eda5f4',NULL,'694f2666c3e44e650f0e5e78',NULL,NULL,NULL,'2025-12-27 00:20:54','2025-12-27 00:20:54'),('694f2c677f4d7d685707e3f2','content','a726bf900fe10718d68468b462',NULL,'694f2c677f4d7d685707e3f1',NULL,NULL,NULL,'2025-12-27 00:46:32','2025-12-27 00:46:32'),('694f2c677f4d7d685707e3f4','admin','a8ecd428b0cfdb67d2f797d8980736629918825e38f56c3bd14f5be0ec3d9a92','694f2665c3e44e650f0e5dde','694f2c677f4d7d685707e3f1',NULL,NULL,NULL,'2025-12-27 00:46:32','2025-12-27 00:46:32');
/*!40000 ALTER TABLE `api_keys` ENABLE KEYS */;
UNLOCK TABLES;

--
-- Table structure for table `automated_emails`
--

DROP TABLE IF EXISTS `automated_emails`;
/*!40101 SET @saved_cs_client     = @@character_set_client */;
/*!50503 SET character_set_client = utf8mb4 */;
CREATE TABLE `automated_emails` (
  `id` varchar(24) NOT NULL,
  `status` varchar(50) NOT NULL DEFAULT 'inactive',
  `name` varchar(191) NOT NULL,
  `slug` varchar(191) NOT NULL,
  `subject` varchar(300) NOT NULL,
  `lexical` longtext,
  `sender_name` varchar(191) DEFAULT NULL,
  `sender_email` varchar(191) DEFAULT NULL,
  `sender_reply_to` varchar(191) DEFAULT NULL,
  `created_at` datetime NOT NULL,
  `updated_at` datetime DEFAULT NULL,
  PRIMARY KEY (`id`),
  UNIQUE KEY `automated_emails_name_unique` (`name`),
  UNIQUE KEY `automated_emails_slug_unique` (`slug`),
  KEY `automated_emails_slug_index` (`slug`),
  KEY `automated_emails_status_index` (`status`)
) ENGINE=InnoDB DEFAULT CHARSET=utf8mb4 COLLATE=utf8mb4_0900_ai_ci;
/*!40101 SET character_set_client = @saved_cs_client */;

--
-- Dumping data for table `automated_emails`
--

LOCK TABLES `automated_emails` WRITE;
/*!40000 ALTER TABLE `automated_emails` DISABLE KEYS */;
/*!40000 ALTER TABLE `automated_emails` ENABLE KEYS */;
UNLOCK TABLES;

--
-- Table structure for table `benefits`
--

DROP TABLE IF EXISTS `benefits`;
/*!40101 SET @saved_cs_client     = @@character_set_client */;
/*!50503 SET character_set_client = utf8mb4 */;
CREATE TABLE `benefits` (
  `id` varchar(24) NOT NULL,
  `name` varchar(191) NOT NULL,
  `slug` varchar(191) NOT NULL,
  `created_at` datetime NOT NULL,
  `updated_at` datetime DEFAULT NULL,
  PRIMARY KEY (`id`),
  UNIQUE KEY `benefits_slug_unique` (`slug`)
) ENGINE=InnoDB DEFAULT CHARSET=utf8mb4 COLLATE=utf8mb4_0900_ai_ci;
/*!40101 SET character_set_client = @saved_cs_client */;

--
-- Dumping data for table `benefits`
--

LOCK TABLES `benefits` WRITE;
/*!40000 ALTER TABLE `benefits` DISABLE KEYS */;
/*!40000 ALTER TABLE `benefits` ENABLE KEYS */;
UNLOCK TABLES;

--
-- Table structure for table `brute`
--

DROP TABLE IF EXISTS `brute`;
/*!40101 SET @saved_cs_client     = @@character_set_client */;
/*!50503 SET character_set_client = utf8mb4 */;
CREATE TABLE `brute` (
  `key` varchar(191) NOT NULL,
  `firstRequest` bigint NOT NULL,
  `lastRequest` bigint NOT NULL,
  `lifetime` bigint NOT NULL,
  `count` int NOT NULL,
  PRIMARY KEY (`key`)
) ENGINE=InnoDB DEFAULT CHARSET=utf8mb4 COLLATE=utf8mb4_0900_ai_ci;
/*!40101 SET character_set_client = @saved_cs_client */;

--
-- Dumping data for table `brute`
--

LOCK TABLES `brute` WRITE;
/*!40000 ALTER TABLE `brute` DISABLE KEYS */;
INSERT INTO `brute` VALUES ('QqKfOyBndHqC3EShEefPKlOq5CLOhpYjJNCGE0RTl80=',1767402606465,1767402606465,1767403606467,1);
/*!40000 ALTER TABLE `brute` ENABLE KEYS */;
UNLOCK TABLES;

--
-- Table structure for table `collections`
--

DROP TABLE IF EXISTS `collections`;
/*!40101 SET @saved_cs_client     = @@character_set_client */;
/*!50503 SET character_set_client = utf8mb4 */;
CREATE TABLE `collections` (
  `id` varchar(24) NOT NULL,
  `title` varchar(191) NOT NULL,
  `slug` varchar(191) NOT NULL,
  `description` varchar(2000) DEFAULT NULL,
  `type` varchar(50) NOT NULL,
  `filter` text,
  `feature_image` varchar(2000) DEFAULT NULL,
  `created_at` datetime NOT NULL,
  `updated_at` datetime DEFAULT NULL,
  PRIMARY KEY (`id`),
  UNIQUE KEY `collections_slug_unique` (`slug`)
) ENGINE=InnoDB DEFAULT CHARSET=utf8mb4 COLLATE=utf8mb4_0900_ai_ci;
/*!40101 SET character_set_client = @saved_cs_client */;

--
-- Dumping data for table `collections`
--

LOCK TABLES `collections` WRITE;
/*!40000 ALTER TABLE `collections` DISABLE KEYS */;
INSERT INTO `collections` VALUES ('694f2665c3e44e650f0e5de5','Latest','latest','All posts','automatic',NULL,NULL,'2025-12-27 00:20:53','2025-12-27 00:20:53'),('694f2665c3e44e650f0e5de6','Featured','featured','Featured posts','automatic','featured:true',NULL,'2025-12-27 00:20:53','2025-12-27 00:20:53');
/*!40000 ALTER TABLE `collections` ENABLE KEYS */;
UNLOCK TABLES;

--
-- Table structure for table `collections_posts`
--

DROP TABLE IF EXISTS `collections_posts`;
/*!40101 SET @saved_cs_client     = @@character_set_client */;
/*!50503 SET character_set_client = utf8mb4 */;
CREATE TABLE `collections_posts` (
  `id` varchar(24) NOT NULL,
  `collection_id` varchar(24) NOT NULL,
  `post_id` varchar(24) NOT NULL,
  `sort_order` int unsigned NOT NULL DEFAULT '0',
  PRIMARY KEY (`id`),
  KEY `collections_posts_collection_id_foreign` (`collection_id`),
  KEY `collections_posts_post_id_foreign` (`post_id`),
  CONSTRAINT `collections_posts_collection_id_foreign` FOREIGN KEY (`collection_id`) REFERENCES `collections` (`id`) ON DELETE CASCADE,
  CONSTRAINT `collections_posts_post_id_foreign` FOREIGN KEY (`post_id`) REFERENCES `posts` (`id`) ON DELETE CASCADE
) ENGINE=InnoDB DEFAULT CHARSET=utf8mb4 COLLATE=utf8mb4_0900_ai_ci;
/*!40101 SET character_set_client = @saved_cs_client */;

--
-- Dumping data for table `collections_posts`
--

LOCK TABLES `collections_posts` WRITE;
/*!40000 ALTER TABLE `collections_posts` DISABLE KEYS */;
/*!40000 ALTER TABLE `collections_posts` ENABLE KEYS */;
UNLOCK TABLES;

--
-- Table structure for table `comment_likes`
--

DROP TABLE IF EXISTS `comment_likes`;
/*!40101 SET @saved_cs_client     = @@character_set_client */;
/*!50503 SET character_set_client = utf8mb4 */;
CREATE TABLE `comment_likes` (
  `id` varchar(24) NOT NULL,
  `comment_id` varchar(24) NOT NULL,
  `member_id` varchar(24) NOT NULL,
  `created_at` datetime NOT NULL,
  `updated_at` datetime NOT NULL,
  PRIMARY KEY (`id`),
  KEY `comment_likes_comment_id_foreign` (`comment_id`),
  KEY `comment_likes_member_id_foreign` (`member_id`),
  CONSTRAINT `comment_likes_comment_id_foreign` FOREIGN KEY (`comment_id`) REFERENCES `comments` (`id`) ON DELETE CASCADE,
  CONSTRAINT `comment_likes_member_id_foreign` FOREIGN KEY (`member_id`) REFERENCES `members` (`id`) ON DELETE CASCADE
) ENGINE=InnoDB DEFAULT CHARSET=utf8mb4 COLLATE=utf8mb4_0900_ai_ci;
/*!40101 SET character_set_client = @saved_cs_client */;

--
-- Dumping data for table `comment_likes`
--

LOCK TABLES `comment_likes` WRITE;
/*!40000 ALTER TABLE `comment_likes` DISABLE KEYS */;
/*!40000 ALTER TABLE `comment_likes` ENABLE KEYS */;
UNLOCK TABLES;

--
-- Table structure for table `comment_reports`
--

DROP TABLE IF EXISTS `comment_reports`;
/*!40101 SET @saved_cs_client     = @@character_set_client */;
/*!50503 SET character_set_client = utf8mb4 */;
CREATE TABLE `comment_reports` (
  `id` varchar(24) NOT NULL,
  `comment_id` varchar(24) NOT NULL,
  `member_id` varchar(24) DEFAULT NULL,
  `created_at` datetime NOT NULL,
  `updated_at` datetime NOT NULL,
  PRIMARY KEY (`id`),
  KEY `comment_reports_comment_id_foreign` (`comment_id`),
  KEY `comment_reports_member_id_foreign` (`member_id`),
  CONSTRAINT `comment_reports_comment_id_foreign` FOREIGN KEY (`comment_id`) REFERENCES `comments` (`id`) ON DELETE CASCADE,
  CONSTRAINT `comment_reports_member_id_foreign` FOREIGN KEY (`member_id`) REFERENCES `members` (`id`) ON DELETE SET NULL
) ENGINE=InnoDB DEFAULT CHARSET=utf8mb4 COLLATE=utf8mb4_0900_ai_ci;
/*!40101 SET character_set_client = @saved_cs_client */;

--
-- Dumping data for table `comment_reports`
--

LOCK TABLES `comment_reports` WRITE;
/*!40000 ALTER TABLE `comment_reports` DISABLE KEYS */;
/*!40000 ALTER TABLE `comment_reports` ENABLE KEYS */;
UNLOCK TABLES;

--
-- Table structure for table `comments`
--

DROP TABLE IF EXISTS `comments`;
/*!40101 SET @saved_cs_client     = @@character_set_client */;
/*!50503 SET character_set_client = utf8mb4 */;
CREATE TABLE `comments` (
  `id` varchar(24) NOT NULL,
  `post_id` varchar(24) NOT NULL,
  `member_id` varchar(24) DEFAULT NULL,
  `parent_id` varchar(24) DEFAULT NULL,
  `in_reply_to_id` varchar(24) DEFAULT NULL,
  `status` varchar(50) NOT NULL DEFAULT 'published',
  `html` longtext,
  `edited_at` datetime DEFAULT NULL,
  `created_at` datetime NOT NULL,
  `updated_at` datetime NOT NULL,
  PRIMARY KEY (`id`),
  KEY `comments_post_id_foreign` (`post_id`),
  KEY `comments_member_id_foreign` (`member_id`),
  KEY `comments_parent_id_foreign` (`parent_id`),
  KEY `comments_in_reply_to_id_foreign` (`in_reply_to_id`),
  CONSTRAINT `comments_in_reply_to_id_foreign` FOREIGN KEY (`in_reply_to_id`) REFERENCES `comments` (`id`) ON DELETE SET NULL,
  CONSTRAINT `comments_member_id_foreign` FOREIGN KEY (`member_id`) REFERENCES `members` (`id`) ON DELETE SET NULL,
  CONSTRAINT `comments_parent_id_foreign` FOREIGN KEY (`parent_id`) REFERENCES `comments` (`id`) ON DELETE CASCADE,
  CONSTRAINT `comments_post_id_foreign` FOREIGN KEY (`post_id`) REFERENCES `posts` (`id`) ON DELETE CASCADE
) ENGINE=InnoDB DEFAULT CHARSET=utf8mb4 COLLATE=utf8mb4_0900_ai_ci;
/*!40101 SET character_set_client = @saved_cs_client */;

--
-- Dumping data for table `comments`
--

LOCK TABLES `comments` WRITE;
/*!40000 ALTER TABLE `comments` DISABLE KEYS */;
/*!40000 ALTER TABLE `comments` ENABLE KEYS */;
UNLOCK TABLES;

--
-- Table structure for table `custom_theme_settings`
--

DROP TABLE IF EXISTS `custom_theme_settings`;
/*!40101 SET @saved_cs_client     = @@character_set_client */;
/*!50503 SET character_set_client = utf8mb4 */;
CREATE TABLE `custom_theme_settings` (
  `id` varchar(24) NOT NULL,
  `theme` varchar(191) NOT NULL,
  `key` varchar(191) NOT NULL,
  `type` varchar(50) NOT NULL,
  `value` text,
  PRIMARY KEY (`id`)
) ENGINE=InnoDB DEFAULT CHARSET=utf8mb4 COLLATE=utf8mb4_0900_ai_ci;
/*!40101 SET character_set_client = @saved_cs_client */;

--
-- Dumping data for table `custom_theme_settings`
--

LOCK TABLES `custom_theme_settings` WRITE;
/*!40000 ALTER TABLE `custom_theme_settings` DISABLE KEYS */;
INSERT INTO `custom_theme_settings` VALUES ('694f2667c3e44e650f0e6093','source','navigation_layout','select','Logo in the middle'),('694f2667c3e44e650f0e6094','source','site_background_color','color','#ffffff'),('694f2667c3e44e650f0e6095','source','header_and_footer_color','select','Background color'),('694f2667c3e44e650f0e6096','source','title_font','select','Modern sans-serif'),('694f2667c3e44e650f0e6097','source','body_font','select','Modern sans-serif'),('694f2667c3e44e650f0e6098','source','signup_heading','text',NULL),('694f2667c3e44e650f0e6099','source','signup_subheading','text',NULL),('694f2667c3e44e650f0e609a','source','header_style','select','Landing'),('694f2667c3e44e650f0e609b','source','header_text','text',NULL),('694f2667c3e44e650f0e609c','source','background_image','boolean','true'),('694f2667c3e44e650f0e609d','source','show_featured_posts','boolean','false'),('694f2667c3e44e650f0e609e','source','post_feed_style','select','List'),('694f2667c3e44e650f0e609f','source','show_images_in_feed','boolean','true'),('694f2667c3e44e650f0e60a0','source','show_author','boolean','true'),('694f2667c3e44e650f0e60a1','source','show_publish_date','boolean','true'),('694f2667c3e44e650f0e60a2','source','show_publication_info_sidebar','boolean','false'),('694f2667c3e44e650f0e60a3','source','show_post_metadata','boolean','true'),('694f2667c3e44e650f0e60a4','source','enable_drop_caps_on_posts','boolean','false'),('694f2667c3e44e650f0e60a5','source','show_related_articles','boolean','true'),('695081b219c94fae00e37371','dawn','navigation_layout','select','Logo on the left'),('695081b219c94fae00e37372','dawn','title_font','select','Modern sans-serif'),('695081b219c94fae00e37373','dawn','body_font','select','Modern sans-serif'),('695081b219c94fae00e37374','dawn','color_scheme','select','Auto'),('695081b219c94fae00e37375','dawn','white_logo_for_dark_mode','image',NULL),('695081b219c94fae00e37376','dawn','show_featured_posts','boolean','true'),('695081b219c94fae00e37377','dawn','featured_title','text','Featured articles'),('695081b219c94fae00e37378','dawn','show_author','boolean','true'),('695081b219c94fae00e37379','dawn','show_related_posts','boolean','true'),('695081fa19c94fae00e3737c','casper','navigation_layout','select','Logo on cover'),('695081fa19c94fae00e3737d','casper','title_font','select','Modern sans-serif'),('695081fa19c94fae00e3737e','casper','body_font','select','Elegant serif'),('695081fa19c94fae00e3737f','casper','show_publication_cover','boolean','true'),('695081fa19c94fae00e37380','casper','header_style','select','Center aligned'),('695081fa19c94fae00e37381','casper','feed_layout','select','Classic'),('695081fa19c94fae00e37382','casper','color_scheme','select','Light'),('695081fa19c94fae00e37383','casper','post_image_style','select','Wide'),('695081fa19c94fae00e37384','casper','email_signup_text','text','Sign up for more like this.'),('695081fa19c94fae00e37385','casper','show_recent_posts_footer','boolean','true'),('6950823819c94fae00e37389','ease','navigation_layout','select','Logo on the left'),('6950823819c94fae00e3738a','ease','title_font','select','Modern sans-serif'),('6950823819c94fae00e3738b','ease','body_font','select','Modern sans-serif'),('6950823819c94fae00e3738c','ease','show_featured_posts','boolean','true'),('6950825c19c94fae00e3738f','headline','navigation_layout','select','Stacked'),('6950825c19c94fae00e37390','headline','header_style','select','Light'),('6950825c19c94fae00e37391','headline','title_font','select','Modern sans-serif'),('6950825c19c94fae00e37392','headline','body_font','select','Elegant serif'),('6950825c19c94fae00e37393','headline','white_publication_logo_for_transparent_header','image',NULL),('6950825c19c94fae00e37394','headline','email_signup_text','text','Don\'t miss out on the latest news. Sign up now to get access to the library of members-only articles.'),('6950825c19c94fae00e37395','headline','footer_text','text',NULL),('6950825c19c94fae00e37396','headline','enter_tag_slugs_for_primary_sections','text',NULL),('6950825c19c94fae00e37397','headline','enter_tag_slugs_for_secondary_sections','text',NULL);
/*!40000 ALTER TABLE `custom_theme_settings` ENABLE KEYS */;
UNLOCK TABLES;

--
-- Table structure for table `donation_payment_events`
--

DROP TABLE IF EXISTS `donation_payment_events`;
/*!40101 SET @saved_cs_client     = @@character_set_client */;
/*!50503 SET character_set_client = utf8mb4 */;
CREATE TABLE `donation_payment_events` (
  `id` varchar(24) NOT NULL,
  `name` varchar(191) DEFAULT NULL,
  `email` varchar(191) NOT NULL,
  `member_id` varchar(24) DEFAULT NULL,
  `amount` int NOT NULL,
  `currency` varchar(50) NOT NULL,
  `attribution_id` varchar(24) DEFAULT NULL,
  `attribution_type` varchar(50) DEFAULT NULL,
  `attribution_url` varchar(2000) DEFAULT NULL,
  `referrer_source` varchar(191) DEFAULT NULL,
  `referrer_medium` varchar(191) DEFAULT NULL,
  `referrer_url` varchar(2000) DEFAULT NULL,
  `utm_source` varchar(191) DEFAULT NULL,
  `utm_medium` varchar(191) DEFAULT NULL,
  `utm_campaign` varchar(191) DEFAULT NULL,
  `utm_term` varchar(191) DEFAULT NULL,
  `utm_content` varchar(191) DEFAULT NULL,
  `created_at` datetime NOT NULL,
  `donation_message` varchar(255) DEFAULT NULL,
  PRIMARY KEY (`id`),
  KEY `donation_payment_events_member_id_foreign` (`member_id`),
  CONSTRAINT `donation_payment_events_member_id_foreign` FOREIGN KEY (`member_id`) REFERENCES `members` (`id`) ON DELETE SET NULL
) ENGINE=InnoDB DEFAULT CHARSET=utf8mb4 COLLATE=utf8mb4_0900_ai_ci;
/*!40101 SET character_set_client = @saved_cs_client */;

--
-- Dumping data for table `donation_payment_events`
--

LOCK TABLES `donation_payment_events` WRITE;
/*!40000 ALTER TABLE `donation_payment_events` DISABLE KEYS */;
/*!40000 ALTER TABLE `donation_payment_events` ENABLE KEYS */;
UNLOCK TABLES;

--
-- Table structure for table `email_batches`
--

DROP TABLE IF EXISTS `email_batches`;
/*!40101 SET @saved_cs_client     = @@character_set_client */;
/*!50503 SET character_set_client = utf8mb4 */;
CREATE TABLE `email_batches` (
  `id` varchar(24) NOT NULL,
  `email_id` varchar(24) NOT NULL,
  `provider_id` varchar(255) DEFAULT NULL,
  `fallback_sending_domain` tinyint(1) NOT NULL DEFAULT '0',
  `status` varchar(50) NOT NULL DEFAULT 'pending',
  `member_segment` text,
  `error_status_code` int unsigned DEFAULT NULL,
  `error_message` varchar(2000) DEFAULT NULL,
  `error_data` longtext,
  `created_at` datetime NOT NULL,
  `updated_at` datetime NOT NULL,
  PRIMARY KEY (`id`),
  KEY `email_batches_email_id_foreign` (`email_id`),
  CONSTRAINT `email_batches_email_id_foreign` FOREIGN KEY (`email_id`) REFERENCES `emails` (`id`)
) ENGINE=InnoDB DEFAULT CHARSET=utf8mb4 COLLATE=utf8mb4_0900_ai_ci;
/*!40101 SET character_set_client = @saved_cs_client */;

--
-- Dumping data for table `email_batches`
--

LOCK TABLES `email_batches` WRITE;
/*!40000 ALTER TABLE `email_batches` DISABLE KEYS */;
/*!40000 ALTER TABLE `email_batches` ENABLE KEYS */;
UNLOCK TABLES;

--
-- Table structure for table `email_recipient_failures`
--

DROP TABLE IF EXISTS `email_recipient_failures`;
/*!40101 SET @saved_cs_client     = @@character_set_client */;
/*!50503 SET character_set_client = utf8mb4 */;
CREATE TABLE `email_recipient_failures` (
  `id` varchar(24) NOT NULL,
  `email_id` varchar(24) NOT NULL,
  `member_id` varchar(24) DEFAULT NULL,
  `email_recipient_id` varchar(24) NOT NULL,
  `code` int unsigned NOT NULL,
  `enhanced_code` varchar(50) DEFAULT NULL,
  `message` varchar(2000) NOT NULL,
  `severity` varchar(50) NOT NULL DEFAULT 'permanent',
  `failed_at` datetime NOT NULL,
  `event_id` varchar(255) DEFAULT NULL,
  PRIMARY KEY (`id`),
  KEY `email_recipient_failures_email_id_foreign` (`email_id`),
  KEY `email_recipient_failures_email_recipient_id_foreign` (`email_recipient_id`),
  CONSTRAINT `email_recipient_failures_email_id_foreign` FOREIGN KEY (`email_id`) REFERENCES `emails` (`id`),
  CONSTRAINT `email_recipient_failures_email_recipient_id_foreign` FOREIGN KEY (`email_recipient_id`) REFERENCES `email_recipients` (`id`)
) ENGINE=InnoDB DEFAULT CHARSET=utf8mb4 COLLATE=utf8mb4_0900_ai_ci;
/*!40101 SET character_set_client = @saved_cs_client */;

--
-- Dumping data for table `email_recipient_failures`
--

LOCK TABLES `email_recipient_failures` WRITE;
/*!40000 ALTER TABLE `email_recipient_failures` DISABLE KEYS */;
/*!40000 ALTER TABLE `email_recipient_failures` ENABLE KEYS */;
UNLOCK TABLES;

--
-- Table structure for table `email_recipients`
--

DROP TABLE IF EXISTS `email_recipients`;
/*!40101 SET @saved_cs_client     = @@character_set_client */;
/*!50503 SET character_set_client = utf8mb4 */;
CREATE TABLE `email_recipients` (
  `id` varchar(24) NOT NULL,
  `email_id` varchar(24) NOT NULL,
  `member_id` varchar(24) NOT NULL,
  `batch_id` varchar(24) NOT NULL,
  `processed_at` datetime DEFAULT NULL,
  `delivered_at` datetime DEFAULT NULL,
  `opened_at` datetime DEFAULT NULL,
  `failed_at` datetime DEFAULT NULL,
  `member_uuid` varchar(36) NOT NULL,
  `member_email` varchar(191) NOT NULL,
  `member_name` varchar(191) DEFAULT NULL,
  PRIMARY KEY (`id`),
  KEY `email_recipients_member_id_index` (`member_id`),
  KEY `email_recipients_batch_id_foreign` (`batch_id`),
  KEY `email_recipients_email_id_member_email_index` (`email_id`,`member_email`),
  KEY `email_recipients_email_id_delivered_at_index` (`email_id`,`delivered_at`),
  KEY `email_recipients_email_id_opened_at_index` (`email_id`,`opened_at`),
  KEY `email_recipients_email_id_failed_at_index` (`email_id`,`failed_at`),
  CONSTRAINT `email_recipients_batch_id_foreign` FOREIGN KEY (`batch_id`) REFERENCES `email_batches` (`id`),
  CONSTRAINT `email_recipients_email_id_foreign` FOREIGN KEY (`email_id`) REFERENCES `emails` (`id`)
) ENGINE=InnoDB DEFAULT CHARSET=utf8mb4 COLLATE=utf8mb4_0900_ai_ci;
/*!40101 SET character_set_client = @saved_cs_client */;

--
-- Dumping data for table `email_recipients`
--

LOCK TABLES `email_recipients` WRITE;
/*!40000 ALTER TABLE `email_recipients` DISABLE KEYS */;
/*!40000 ALTER TABLE `email_recipients` ENABLE KEYS */;
UNLOCK TABLES;

--
-- Table structure for table `email_spam_complaint_events`
--

DROP TABLE IF EXISTS `email_spam_complaint_events`;
/*!40101 SET @saved_cs_client     = @@character_set_client */;
/*!50503 SET character_set_client = utf8mb4 */;
CREATE TABLE `email_spam_complaint_events` (
  `id` varchar(24) NOT NULL,
  `member_id` varchar(24) NOT NULL,
  `email_id` varchar(24) NOT NULL,
  `email_address` varchar(191) NOT NULL,
  `created_at` datetime NOT NULL,
  PRIMARY KEY (`id`),
  UNIQUE KEY `email_spam_complaint_events_email_id_member_id_unique` (`email_id`,`member_id`),
  KEY `email_spam_complaint_events_member_id_foreign` (`member_id`),
  CONSTRAINT `email_spam_complaint_events_email_id_foreign` FOREIGN KEY (`email_id`) REFERENCES `emails` (`id`),
  CONSTRAINT `email_spam_complaint_events_member_id_foreign` FOREIGN KEY (`member_id`) REFERENCES `members` (`id`) ON DELETE CASCADE
) ENGINE=InnoDB DEFAULT CHARSET=utf8mb4 COLLATE=utf8mb4_0900_ai_ci;
/*!40101 SET character_set_client = @saved_cs_client */;

--
-- Dumping data for table `email_spam_complaint_events`
--

LOCK TABLES `email_spam_complaint_events` WRITE;
/*!40000 ALTER TABLE `email_spam_complaint_events` DISABLE KEYS */;
/*!40000 ALTER TABLE `email_spam_complaint_events` ENABLE KEYS */;
UNLOCK TABLES;

--
-- Table structure for table `emails`
--

DROP TABLE IF EXISTS `emails`;
/*!40101 SET @saved_cs_client     = @@character_set_client */;
/*!50503 SET character_set_client = utf8mb4 */;
CREATE TABLE `emails` (
  `id` varchar(24) NOT NULL,
  `post_id` varchar(24) NOT NULL,
  `uuid` varchar(36) NOT NULL,
  `status` varchar(50) NOT NULL DEFAULT 'pending',
  `recipient_filter` text NOT NULL,
  `error` varchar(2000) DEFAULT NULL,
  `error_data` longtext,
  `email_count` int unsigned NOT NULL DEFAULT '0',
  `csd_email_count` int unsigned DEFAULT NULL,
  `delivered_count` int unsigned NOT NULL DEFAULT '0',
  `opened_count` int unsigned NOT NULL DEFAULT '0',
  `failed_count` int unsigned NOT NULL DEFAULT '0',
  `subject` varchar(300) DEFAULT NULL,
  `from` varchar(2000) DEFAULT NULL,
  `reply_to` varchar(2000) DEFAULT NULL,
  `html` longtext,
  `plaintext` longtext,
  `source` longtext,
  `source_type` varchar(50) NOT NULL DEFAULT 'html',
  `track_opens` tinyint(1) NOT NULL DEFAULT '0',
  `track_clicks` tinyint(1) NOT NULL DEFAULT '0',
  `feedback_enabled` tinyint(1) NOT NULL DEFAULT '0',
  `submitted_at` datetime NOT NULL,
  `newsletter_id` varchar(24) DEFAULT NULL,
  `created_at` datetime NOT NULL,
  `updated_at` datetime DEFAULT NULL,
  PRIMARY KEY (`id`),
  UNIQUE KEY `emails_post_id_unique` (`post_id`),
  KEY `emails_post_id_index` (`post_id`),
  KEY `emails_newsletter_id_foreign` (`newsletter_id`),
  CONSTRAINT `emails_newsletter_id_foreign` FOREIGN KEY (`newsletter_id`) REFERENCES `newsletters` (`id`)
) ENGINE=InnoDB DEFAULT CHARSET=utf8mb4 COLLATE=utf8mb4_0900_ai_ci;
/*!40101 SET character_set_client = @saved_cs_client */;

--
-- Dumping data for table `emails`
--

LOCK TABLES `emails` WRITE;
/*!40000 ALTER TABLE `emails` DISABLE KEYS */;
/*!40000 ALTER TABLE `emails` ENABLE KEYS */;
UNLOCK TABLES;

--
-- Table structure for table `integrations`
--

DROP TABLE IF EXISTS `integrations`;
/*!40101 SET @saved_cs_client     = @@character_set_client */;
/*!50503 SET character_set_client = utf8mb4 */;
CREATE TABLE `integrations` (
  `id` varchar(24) NOT NULL,
  `type` varchar(50) NOT NULL DEFAULT 'custom',
  `name` varchar(191) NOT NULL,
  `slug` varchar(191) NOT NULL,
  `icon_image` varchar(2000) DEFAULT NULL,
  `description` varchar(2000) DEFAULT NULL,
  `created_at` datetime NOT NULL,
  `updated_at` datetime DEFAULT NULL,
  PRIMARY KEY (`id`),
  UNIQUE KEY `integrations_slug_unique` (`slug`)
) ENGINE=InnoDB DEFAULT CHARSET=utf8mb4 COLLATE=utf8mb4_0900_ai_ci;
/*!40101 SET character_set_client = @saved_cs_client */;

--
-- Dumping data for table `integrations`
--

LOCK TABLES `integrations` WRITE;
/*!40000 ALTER TABLE `integrations` DISABLE KEYS */;
INSERT INTO `integrations` VALUES ('694f2666c3e44e650f0e5e6c','builtin','Zapier','zapier',NULL,'Built-in Zapier integration','2025-12-27 00:20:54','2025-12-27 00:20:54'),('694f2666c3e44e650f0e5e6e','core','Ghost Explore','ghost-explore',NULL,'Built-in Ghost Explore integration','2025-12-27 00:20:54','2025-12-27 00:20:54'),('694f2666c3e44e650f0e5e70','core','Self-Serve Migration Integration','self-serve-migration',NULL,'Core Self-Serve Migration integration','2025-12-27 00:20:54','2025-12-27 00:20:54'),('694f2666c3e44e650f0e5e72','internal','Ghost Backup','ghost-backup',NULL,'Internal DB Backup integration','2025-12-27 00:20:54','2025-12-27 00:20:54'),('694f2666c3e44e650f0e5e74','internal','Ghost Scheduler','ghost-scheduler',NULL,'Internal Scheduler integration','2025-12-27 00:20:54','2025-12-27 00:20:54'),('694f2666c3e44e650f0e5e76','internal','Ghost Internal Frontend','ghost-internal-frontend',NULL,'Internal frontend integration','2025-12-27 00:20:54','2025-12-27 00:20:54'),('694f2666c3e44e650f0e5e78','core','Ghost Core Content API','ghost-core-content',NULL,'Internal Content API integration for Admin access','2025-12-27 00:20:54','2025-12-27 00:20:54'),('694f2666c3e44e650f0e5e7a','internal','Ghost ActivityPub','ghost-activitypub',NULL,'Internal Integration for ActivityPub','2025-12-27 00:20:54','2025-12-27 00:20:54'),('694f2c677f4d7d685707e3f1','custom','performance','performance',NULL,NULL,'2025-12-27 00:46:31','2025-12-27 00:46:31');
/*!40000 ALTER TABLE `integrations` ENABLE KEYS */;
UNLOCK TABLES;

--
-- Table structure for table `invites`
--

DROP TABLE IF EXISTS `invites`;
/*!40101 SET @saved_cs_client     = @@character_set_client */;
/*!50503 SET character_set_client = utf8mb4 */;
CREATE TABLE `invites` (
  `id` varchar(24) NOT NULL,
  `role_id` varchar(24) NOT NULL,
  `status` varchar(50) NOT NULL DEFAULT 'pending',
  `token` varchar(191) NOT NULL,
  `email` varchar(191) NOT NULL,
  `expires` bigint NOT NULL,
  `created_at` datetime NOT NULL,
  `updated_at` datetime DEFAULT NULL,
  PRIMARY KEY (`id`),
  UNIQUE KEY `invites_token_unique` (`token`),
  UNIQUE KEY `invites_email_unique` (`email`)
) ENGINE=InnoDB DEFAULT CHARSET=utf8mb4 COLLATE=utf8mb4_0900_ai_ci;
/*!40101 SET character_set_client = @saved_cs_client */;

--
-- Dumping data for table `invites`
--

LOCK TABLES `invites` WRITE;
/*!40000 ALTER TABLE `invites` DISABLE KEYS */;
/*!40000 ALTER TABLE `invites` ENABLE KEYS */;
UNLOCK TABLES;

--
-- Table structure for table `jobs`
--

DROP TABLE IF EXISTS `jobs`;
/*!40101 SET @saved_cs_client     = @@character_set_client */;
/*!50503 SET character_set_client = utf8mb4 */;
CREATE TABLE `jobs` (
  `id` varchar(24) NOT NULL,
  `name` varchar(191) NOT NULL,
  `status` varchar(50) NOT NULL DEFAULT 'queued',
  `started_at` datetime DEFAULT NULL,
  `finished_at` datetime DEFAULT NULL,
  `created_at` datetime NOT NULL,
  `updated_at` datetime DEFAULT NULL,
  `metadata` varchar(2000) DEFAULT NULL,
  `queue_entry` int unsigned DEFAULT NULL,
  PRIMARY KEY (`id`),
  UNIQUE KEY `jobs_name_unique` (`name`)
) ENGINE=InnoDB DEFAULT CHARSET=utf8mb4 COLLATE=utf8mb4_0900_ai_ci;
/*!40101 SET character_set_client = @saved_cs_client */;

--
-- Dumping data for table `jobs`
--

LOCK TABLES `jobs` WRITE;
/*!40000 ALTER TABLE `jobs` DISABLE KEYS */;
INSERT INTO `jobs` VALUES ('694f2667c3e44e650f0e60a6','members-migrations','finished','2025-12-27 00:20:55','2025-12-27 00:20:55','2025-12-27 00:20:55','2025-12-27 00:20:55',NULL,NULL);
/*!40000 ALTER TABLE `jobs` ENABLE KEYS */;
UNLOCK TABLES;

--
-- Table structure for table `labels`
--

DROP TABLE IF EXISTS `labels`;
/*!40101 SET @saved_cs_client     = @@character_set_client */;
/*!50503 SET character_set_client = utf8mb4 */;
CREATE TABLE `labels` (
  `id` varchar(24) NOT NULL,
  `name` varchar(191) NOT NULL,
  `slug` varchar(191) NOT NULL,
  `created_at` datetime NOT NULL,
  `updated_at` datetime DEFAULT NULL,
  PRIMARY KEY (`id`),
  UNIQUE KEY `labels_name_unique` (`name`),
  UNIQUE KEY `labels_slug_unique` (`slug`)
) ENGINE=InnoDB DEFAULT CHARSET=utf8mb4 COLLATE=utf8mb4_0900_ai_ci;
/*!40101 SET character_set_client = @saved_cs_client */;

--
-- Dumping data for table `labels`
--

LOCK TABLES `labels` WRITE;
/*!40000 ALTER TABLE `labels` DISABLE KEYS */;
/*!40000 ALTER TABLE `labels` ENABLE KEYS */;
UNLOCK TABLES;

--
-- Table structure for table `members`
--

DROP TABLE IF EXISTS `members`;
/*!40101 SET @saved_cs_client     = @@character_set_client */;
/*!50503 SET character_set_client = utf8mb4 */;
CREATE TABLE `members` (
  `id` varchar(24) NOT NULL,
  `uuid` varchar(36) NOT NULL,
  `transient_id` varchar(191) NOT NULL,
  `email` varchar(191) NOT NULL,
  `status` varchar(50) NOT NULL DEFAULT 'free',
  `name` varchar(191) DEFAULT NULL,
  `expertise` varchar(191) DEFAULT NULL,
  `note` varchar(2000) DEFAULT NULL,
  `geolocation` varchar(2000) DEFAULT NULL,
  `enable_comment_notifications` tinyint(1) NOT NULL DEFAULT '1',
  `email_count` int unsigned NOT NULL DEFAULT '0',
  `email_opened_count` int unsigned NOT NULL DEFAULT '0',
  `email_open_rate` int unsigned DEFAULT NULL,
  `email_disabled` tinyint(1) NOT NULL DEFAULT '0',
  `last_seen_at` datetime DEFAULT NULL,
  `last_commented_at` datetime DEFAULT NULL,
  `created_at` datetime NOT NULL,
  `updated_at` datetime DEFAULT NULL,
  PRIMARY KEY (`id`),
  UNIQUE KEY `members_uuid_unique` (`uuid`),
  UNIQUE KEY `members_transient_id_unique` (`transient_id`),
  UNIQUE KEY `members_email_unique` (`email`),
  KEY `members_email_open_rate_index` (`email_open_rate`),
  KEY `members_email_disabled_index` (`email_disabled`)
) ENGINE=InnoDB DEFAULT CHARSET=utf8mb4 COLLATE=utf8mb4_0900_ai_ci;
/*!40101 SET character_set_client = @saved_cs_client */;

--
-- Dumping data for table `members`
--

LOCK TABLES `members` WRITE;
/*!40000 ALTER TABLE `members` DISABLE KEYS */;
INSERT INTO `members` VALUES ('694f26a6c3e44e650f0e60b6','ec211e6b-343e-4ab1-9bb5-3a324df0027e','e63b2bcd-7293-4666-96b9-09d3534bea0c','rdgztorres19@gmail.com','free','Manuel A Rodriguez',NULL,NULL,NULL,1,0,0,NULL,0,NULL,NULL,'2025-12-27 00:21:58','2025-12-27 00:21:58');
/*!40000 ALTER TABLE `members` ENABLE KEYS */;
UNLOCK TABLES;

--
-- Table structure for table `members_cancel_events`
--

DROP TABLE IF EXISTS `members_cancel_events`;
/*!40101 SET @saved_cs_client     = @@character_set_client */;
/*!50503 SET character_set_client = utf8mb4 */;
CREATE TABLE `members_cancel_events` (
  `id` varchar(24) NOT NULL,
  `member_id` varchar(24) NOT NULL,
  `from_plan` varchar(255) NOT NULL,
  `created_at` datetime NOT NULL,
  PRIMARY KEY (`id`),
  KEY `members_cancel_events_member_id_foreign` (`member_id`),
  CONSTRAINT `members_cancel_events_member_id_foreign` FOREIGN KEY (`member_id`) REFERENCES `members` (`id`) ON DELETE CASCADE
) ENGINE=InnoDB DEFAULT CHARSET=utf8mb4 COLLATE=utf8mb4_0900_ai_ci;
/*!40101 SET character_set_client = @saved_cs_client */;

--
-- Dumping data for table `members_cancel_events`
--

LOCK TABLES `members_cancel_events` WRITE;
/*!40000 ALTER TABLE `members_cancel_events` DISABLE KEYS */;
/*!40000 ALTER TABLE `members_cancel_events` ENABLE KEYS */;
UNLOCK TABLES;

--
-- Table structure for table `members_click_events`
--

DROP TABLE IF EXISTS `members_click_events`;
/*!40101 SET @saved_cs_client     = @@character_set_client */;
/*!50503 SET character_set_client = utf8mb4 */;
CREATE TABLE `members_click_events` (
  `id` varchar(24) NOT NULL,
  `member_id` varchar(24) NOT NULL,
  `redirect_id` varchar(24) NOT NULL,
  `created_at` datetime NOT NULL,
  PRIMARY KEY (`id`),
  KEY `members_click_events_member_id_foreign` (`member_id`),
  KEY `members_click_events_redirect_id_foreign` (`redirect_id`),
  CONSTRAINT `members_click_events_member_id_foreign` FOREIGN KEY (`member_id`) REFERENCES `members` (`id`) ON DELETE CASCADE,
  CONSTRAINT `members_click_events_redirect_id_foreign` FOREIGN KEY (`redirect_id`) REFERENCES `redirects` (`id`) ON DELETE CASCADE
) ENGINE=InnoDB DEFAULT CHARSET=utf8mb4 COLLATE=utf8mb4_0900_ai_ci;
/*!40101 SET character_set_client = @saved_cs_client */;

--
-- Dumping data for table `members_click_events`
--

LOCK TABLES `members_click_events` WRITE;
/*!40000 ALTER TABLE `members_click_events` DISABLE KEYS */;
/*!40000 ALTER TABLE `members_click_events` ENABLE KEYS */;
UNLOCK TABLES;

--
-- Table structure for table `members_created_events`
--

DROP TABLE IF EXISTS `members_created_events`;
/*!40101 SET @saved_cs_client     = @@character_set_client */;
/*!50503 SET character_set_client = utf8mb4 */;
CREATE TABLE `members_created_events` (
  `id` varchar(24) NOT NULL,
  `created_at` datetime NOT NULL,
  `member_id` varchar(24) NOT NULL,
  `attribution_id` varchar(24) DEFAULT NULL,
  `attribution_type` varchar(50) DEFAULT NULL,
  `attribution_url` varchar(2000) DEFAULT NULL,
  `referrer_source` varchar(191) DEFAULT NULL,
  `referrer_medium` varchar(191) DEFAULT NULL,
  `referrer_url` varchar(2000) DEFAULT NULL,
  `utm_source` varchar(191) DEFAULT NULL,
  `utm_medium` varchar(191) DEFAULT NULL,
  `utm_campaign` varchar(191) DEFAULT NULL,
  `utm_term` varchar(191) DEFAULT NULL,
  `utm_content` varchar(191) DEFAULT NULL,
  `source` varchar(50) NOT NULL,
  `batch_id` varchar(24) DEFAULT NULL,
  PRIMARY KEY (`id`),
  KEY `members_created_events_member_id_foreign` (`member_id`),
  KEY `members_created_events_attribution_id_index` (`attribution_id`),
  CONSTRAINT `members_created_events_member_id_foreign` FOREIGN KEY (`member_id`) REFERENCES `members` (`id`) ON DELETE CASCADE
) ENGINE=InnoDB DEFAULT CHARSET=utf8mb4 COLLATE=utf8mb4_0900_ai_ci;
/*!40101 SET character_set_client = @saved_cs_client */;

--
-- Dumping data for table `members_created_events`
--

LOCK TABLES `members_created_events` WRITE;
/*!40000 ALTER TABLE `members_created_events` DISABLE KEYS */;
INSERT INTO `members_created_events` VALUES ('694f26a6c3e44e650f0e60bb','2025-12-27 00:21:58','694f26a6c3e44e650f0e60b6',NULL,NULL,NULL,'Created manually','Ghost Admin',NULL,NULL,NULL,NULL,NULL,NULL,'admin','694f26a6c3e44e650f0e60b5');
/*!40000 ALTER TABLE `members_created_events` ENABLE KEYS */;
UNLOCK TABLES;

--
-- Table structure for table `members_email_change_events`
--

DROP TABLE IF EXISTS `members_email_change_events`;
/*!40101 SET @saved_cs_client     = @@character_set_client */;
/*!50503 SET character_set_client = utf8mb4 */;
CREATE TABLE `members_email_change_events` (
  `id` varchar(24) NOT NULL,
  `member_id` varchar(24) NOT NULL,
  `to_email` varchar(191) NOT NULL,
  `from_email` varchar(191) NOT NULL,
  `created_at` datetime NOT NULL,
  PRIMARY KEY (`id`),
  KEY `members_email_change_events_member_id_foreign` (`member_id`),
  CONSTRAINT `members_email_change_events_member_id_foreign` FOREIGN KEY (`member_id`) REFERENCES `members` (`id`) ON DELETE CASCADE
) ENGINE=InnoDB DEFAULT CHARSET=utf8mb4 COLLATE=utf8mb4_0900_ai_ci;
/*!40101 SET character_set_client = @saved_cs_client */;

--
-- Dumping data for table `members_email_change_events`
--

LOCK TABLES `members_email_change_events` WRITE;
/*!40000 ALTER TABLE `members_email_change_events` DISABLE KEYS */;
/*!40000 ALTER TABLE `members_email_change_events` ENABLE KEYS */;
UNLOCK TABLES;

--
-- Table structure for table `members_feedback`
--

DROP TABLE IF EXISTS `members_feedback`;
/*!40101 SET @saved_cs_client     = @@character_set_client */;
/*!50503 SET character_set_client = utf8mb4 */;
CREATE TABLE `members_feedback` (
  `id` varchar(24) NOT NULL,
  `score` int unsigned NOT NULL DEFAULT '0',
  `member_id` varchar(24) NOT NULL,
  `post_id` varchar(24) NOT NULL,
  `created_at` datetime NOT NULL,
  `updated_at` datetime DEFAULT NULL,
  PRIMARY KEY (`id`),
  KEY `members_feedback_member_id_foreign` (`member_id`),
  KEY `members_feedback_post_id_foreign` (`post_id`),
  CONSTRAINT `members_feedback_member_id_foreign` FOREIGN KEY (`member_id`) REFERENCES `members` (`id`) ON DELETE CASCADE,
  CONSTRAINT `members_feedback_post_id_foreign` FOREIGN KEY (`post_id`) REFERENCES `posts` (`id`) ON DELETE CASCADE
) ENGINE=InnoDB DEFAULT CHARSET=utf8mb4 COLLATE=utf8mb4_0900_ai_ci;
/*!40101 SET character_set_client = @saved_cs_client */;

--
-- Dumping data for table `members_feedback`
--

LOCK TABLES `members_feedback` WRITE;
/*!40000 ALTER TABLE `members_feedback` DISABLE KEYS */;
/*!40000 ALTER TABLE `members_feedback` ENABLE KEYS */;
UNLOCK TABLES;

--
-- Table structure for table `members_labels`
--

DROP TABLE IF EXISTS `members_labels`;
/*!40101 SET @saved_cs_client     = @@character_set_client */;
/*!50503 SET character_set_client = utf8mb4 */;
CREATE TABLE `members_labels` (
  `id` varchar(24) NOT NULL,
  `member_id` varchar(24) NOT NULL,
  `label_id` varchar(24) NOT NULL,
  `sort_order` int unsigned NOT NULL DEFAULT '0',
  PRIMARY KEY (`id`),
  KEY `members_labels_member_id_foreign` (`member_id`),
  KEY `members_labels_label_id_foreign` (`label_id`),
  CONSTRAINT `members_labels_label_id_foreign` FOREIGN KEY (`label_id`) REFERENCES `labels` (`id`) ON DELETE CASCADE,
  CONSTRAINT `members_labels_member_id_foreign` FOREIGN KEY (`member_id`) REFERENCES `members` (`id`) ON DELETE CASCADE
) ENGINE=InnoDB DEFAULT CHARSET=utf8mb4 COLLATE=utf8mb4_0900_ai_ci;
/*!40101 SET character_set_client = @saved_cs_client */;

--
-- Dumping data for table `members_labels`
--

LOCK TABLES `members_labels` WRITE;
/*!40000 ALTER TABLE `members_labels` DISABLE KEYS */;
/*!40000 ALTER TABLE `members_labels` ENABLE KEYS */;
UNLOCK TABLES;

--
-- Table structure for table `members_login_events`
--

DROP TABLE IF EXISTS `members_login_events`;
/*!40101 SET @saved_cs_client     = @@character_set_client */;
/*!50503 SET character_set_client = utf8mb4 */;
CREATE TABLE `members_login_events` (
  `id` varchar(24) NOT NULL,
  `member_id` varchar(24) NOT NULL,
  `created_at` datetime NOT NULL,
  PRIMARY KEY (`id`),
  KEY `members_login_events_member_id_foreign` (`member_id`),
  CONSTRAINT `members_login_events_member_id_foreign` FOREIGN KEY (`member_id`) REFERENCES `members` (`id`) ON DELETE CASCADE
) ENGINE=InnoDB DEFAULT CHARSET=utf8mb4 COLLATE=utf8mb4_0900_ai_ci;
/*!40101 SET character_set_client = @saved_cs_client */;

--
-- Dumping data for table `members_login_events`
--

LOCK TABLES `members_login_events` WRITE;
/*!40000 ALTER TABLE `members_login_events` DISABLE KEYS */;
/*!40000 ALTER TABLE `members_login_events` ENABLE KEYS */;
UNLOCK TABLES;

--
-- Table structure for table `members_newsletters`
--

DROP TABLE IF EXISTS `members_newsletters`;
/*!40101 SET @saved_cs_client     = @@character_set_client */;
/*!50503 SET character_set_client = utf8mb4 */;
CREATE TABLE `members_newsletters` (
  `id` varchar(24) NOT NULL,
  `member_id` varchar(24) NOT NULL,
  `newsletter_id` varchar(24) NOT NULL,
  PRIMARY KEY (`id`),
  KEY `members_newsletters_member_id_foreign` (`member_id`),
  KEY `members_newsletters_newsletter_id_member_id_index` (`newsletter_id`,`member_id`),
  CONSTRAINT `members_newsletters_member_id_foreign` FOREIGN KEY (`member_id`) REFERENCES `members` (`id`) ON DELETE CASCADE,
  CONSTRAINT `members_newsletters_newsletter_id_foreign` FOREIGN KEY (`newsletter_id`) REFERENCES `newsletters` (`id`) ON DELETE CASCADE
) ENGINE=InnoDB DEFAULT CHARSET=utf8mb4 COLLATE=utf8mb4_0900_ai_ci;
/*!40101 SET character_set_client = @saved_cs_client */;

--
-- Dumping data for table `members_newsletters`
--

LOCK TABLES `members_newsletters` WRITE;
/*!40000 ALTER TABLE `members_newsletters` DISABLE KEYS */;
INSERT INTO `members_newsletters` VALUES ('694f26a6c3e44e650f0e60b7','694f26a6c3e44e650f0e60b6','694f2665c3e44e650f0e5de9');
/*!40000 ALTER TABLE `members_newsletters` ENABLE KEYS */;
UNLOCK TABLES;

--
-- Table structure for table `members_paid_subscription_events`
--

DROP TABLE IF EXISTS `members_paid_subscription_events`;
/*!40101 SET @saved_cs_client     = @@character_set_client */;
/*!50503 SET character_set_client = utf8mb4 */;
CREATE TABLE `members_paid_subscription_events` (
  `id` varchar(24) NOT NULL,
  `type` varchar(50) DEFAULT NULL,
  `member_id` varchar(24) NOT NULL,
  `subscription_id` varchar(24) DEFAULT NULL,
  `from_plan` varchar(255) DEFAULT NULL,
  `to_plan` varchar(255) DEFAULT NULL,
  `currency` varchar(191) NOT NULL,
  `source` varchar(50) NOT NULL,
  `mrr_delta` int NOT NULL,
  `created_at` datetime NOT NULL,
  PRIMARY KEY (`id`),
  KEY `members_paid_subscription_events_member_id_foreign` (`member_id`),
  CONSTRAINT `members_paid_subscription_events_member_id_foreign` FOREIGN KEY (`member_id`) REFERENCES `members` (`id`) ON DELETE CASCADE
) ENGINE=InnoDB DEFAULT CHARSET=utf8mb4 COLLATE=utf8mb4_0900_ai_ci;
/*!40101 SET character_set_client = @saved_cs_client */;

--
-- Dumping data for table `members_paid_subscription_events`
--

LOCK TABLES `members_paid_subscription_events` WRITE;
/*!40000 ALTER TABLE `members_paid_subscription_events` DISABLE KEYS */;
/*!40000 ALTER TABLE `members_paid_subscription_events` ENABLE KEYS */;
UNLOCK TABLES;

--
-- Table structure for table `members_payment_events`
--

DROP TABLE IF EXISTS `members_payment_events`;
/*!40101 SET @saved_cs_client     = @@character_set_client */;
/*!50503 SET character_set_client = utf8mb4 */;
CREATE TABLE `members_payment_events` (
  `id` varchar(24) NOT NULL,
  `member_id` varchar(24) NOT NULL,
  `amount` int NOT NULL,
  `currency` varchar(191) NOT NULL,
  `source` varchar(50) NOT NULL,
  `created_at` datetime NOT NULL,
  PRIMARY KEY (`id`),
  KEY `members_payment_events_member_id_foreign` (`member_id`),
  CONSTRAINT `members_payment_events_member_id_foreign` FOREIGN KEY (`member_id`) REFERENCES `members` (`id`) ON DELETE CASCADE
) ENGINE=InnoDB DEFAULT CHARSET=utf8mb4 COLLATE=utf8mb4_0900_ai_ci;
/*!40101 SET character_set_client = @saved_cs_client */;

--
-- Dumping data for table `members_payment_events`
--

LOCK TABLES `members_payment_events` WRITE;
/*!40000 ALTER TABLE `members_payment_events` DISABLE KEYS */;
/*!40000 ALTER TABLE `members_payment_events` ENABLE KEYS */;
UNLOCK TABLES;

--
-- Table structure for table `members_product_events`
--

DROP TABLE IF EXISTS `members_product_events`;
/*!40101 SET @saved_cs_client     = @@character_set_client */;
/*!50503 SET character_set_client = utf8mb4 */;
CREATE TABLE `members_product_events` (
  `id` varchar(24) NOT NULL,
  `member_id` varchar(24) NOT NULL,
  `product_id` varchar(24) NOT NULL,
  `action` varchar(50) DEFAULT NULL,
  `created_at` datetime NOT NULL,
  PRIMARY KEY (`id`),
  KEY `members_product_events_member_id_foreign` (`member_id`),
  KEY `members_product_events_product_id_foreign` (`product_id`),
  CONSTRAINT `members_product_events_member_id_foreign` FOREIGN KEY (`member_id`) REFERENCES `members` (`id`) ON DELETE CASCADE,
  CONSTRAINT `members_product_events_product_id_foreign` FOREIGN KEY (`product_id`) REFERENCES `products` (`id`)
) ENGINE=InnoDB DEFAULT CHARSET=utf8mb4 COLLATE=utf8mb4_0900_ai_ci;
/*!40101 SET character_set_client = @saved_cs_client */;

--
-- Dumping data for table `members_product_events`
--

LOCK TABLES `members_product_events` WRITE;
/*!40000 ALTER TABLE `members_product_events` DISABLE KEYS */;
/*!40000 ALTER TABLE `members_product_events` ENABLE KEYS */;
UNLOCK TABLES;

--
-- Table structure for table `members_products`
--

DROP TABLE IF EXISTS `members_products`;
/*!40101 SET @saved_cs_client     = @@character_set_client */;
/*!50503 SET character_set_client = utf8mb4 */;
CREATE TABLE `members_products` (
  `id` varchar(24) NOT NULL,
  `member_id` varchar(24) NOT NULL,
  `product_id` varchar(24) NOT NULL,
  `sort_order` int unsigned NOT NULL DEFAULT '0',
  `expiry_at` datetime DEFAULT NULL,
  PRIMARY KEY (`id`),
  KEY `members_products_member_id_foreign` (`member_id`),
  KEY `members_products_product_id_foreign` (`product_id`),
  CONSTRAINT `members_products_member_id_foreign` FOREIGN KEY (`member_id`) REFERENCES `members` (`id`) ON DELETE CASCADE,
  CONSTRAINT `members_products_product_id_foreign` FOREIGN KEY (`product_id`) REFERENCES `products` (`id`) ON DELETE CASCADE
) ENGINE=InnoDB DEFAULT CHARSET=utf8mb4 COLLATE=utf8mb4_0900_ai_ci;
/*!40101 SET character_set_client = @saved_cs_client */;

--
-- Dumping data for table `members_products`
--

LOCK TABLES `members_products` WRITE;
/*!40000 ALTER TABLE `members_products` DISABLE KEYS */;
/*!40000 ALTER TABLE `members_products` ENABLE KEYS */;
UNLOCK TABLES;

--
-- Table structure for table `members_status_events`
--

DROP TABLE IF EXISTS `members_status_events`;
/*!40101 SET @saved_cs_client     = @@character_set_client */;
/*!50503 SET character_set_client = utf8mb4 */;
CREATE TABLE `members_status_events` (
  `id` varchar(24) NOT NULL,
  `member_id` varchar(24) NOT NULL,
  `from_status` varchar(50) DEFAULT NULL,
  `to_status` varchar(50) DEFAULT NULL,
  `created_at` datetime NOT NULL,
  PRIMARY KEY (`id`),
  KEY `members_status_events_member_id_foreign` (`member_id`),
  CONSTRAINT `members_status_events_member_id_foreign` FOREIGN KEY (`member_id`) REFERENCES `members` (`id`) ON DELETE CASCADE
) ENGINE=InnoDB DEFAULT CHARSET=utf8mb4 COLLATE=utf8mb4_0900_ai_ci;
/*!40101 SET character_set_client = @saved_cs_client */;

--
-- Dumping data for table `members_status_events`
--

LOCK TABLES `members_status_events` WRITE;
/*!40000 ALTER TABLE `members_status_events` DISABLE KEYS */;
INSERT INTO `members_status_events` VALUES ('694f26a6c3e44e650f0e60b9','694f26a6c3e44e650f0e60b6',NULL,'free','2025-12-27 00:21:58');
/*!40000 ALTER TABLE `members_status_events` ENABLE KEYS */;
UNLOCK TABLES;

--
-- Table structure for table `members_stripe_customers`
--

DROP TABLE IF EXISTS `members_stripe_customers`;
/*!40101 SET @saved_cs_client     = @@character_set_client */;
/*!50503 SET character_set_client = utf8mb4 */;
CREATE TABLE `members_stripe_customers` (
  `id` varchar(24) NOT NULL,
  `member_id` varchar(24) NOT NULL,
  `customer_id` varchar(255) NOT NULL,
  `name` varchar(191) DEFAULT NULL,
  `email` varchar(191) DEFAULT NULL,
  `created_at` datetime NOT NULL,
  `updated_at` datetime DEFAULT NULL,
  PRIMARY KEY (`id`),
  UNIQUE KEY `members_stripe_customers_customer_id_unique` (`customer_id`),
  KEY `members_stripe_customers_member_id_foreign` (`member_id`),
  CONSTRAINT `members_stripe_customers_member_id_foreign` FOREIGN KEY (`member_id`) REFERENCES `members` (`id`) ON DELETE CASCADE
) ENGINE=InnoDB DEFAULT CHARSET=utf8mb4 COLLATE=utf8mb4_0900_ai_ci;
/*!40101 SET character_set_client = @saved_cs_client */;

--
-- Dumping data for table `members_stripe_customers`
--

LOCK TABLES `members_stripe_customers` WRITE;
/*!40000 ALTER TABLE `members_stripe_customers` DISABLE KEYS */;
/*!40000 ALTER TABLE `members_stripe_customers` ENABLE KEYS */;
UNLOCK TABLES;

--
-- Table structure for table `members_stripe_customers_subscriptions`
--

DROP TABLE IF EXISTS `members_stripe_customers_subscriptions`;
/*!40101 SET @saved_cs_client     = @@character_set_client */;
/*!50503 SET character_set_client = utf8mb4 */;
CREATE TABLE `members_stripe_customers_subscriptions` (
  `id` varchar(24) NOT NULL,
  `customer_id` varchar(255) NOT NULL,
  `ghost_subscription_id` varchar(24) DEFAULT NULL,
  `subscription_id` varchar(255) NOT NULL,
  `stripe_price_id` varchar(255) NOT NULL DEFAULT '',
  `status` varchar(50) NOT NULL,
  `cancel_at_period_end` tinyint(1) NOT NULL DEFAULT '0',
  `cancellation_reason` varchar(500) DEFAULT NULL,
  `current_period_end` datetime NOT NULL,
  `start_date` datetime NOT NULL,
  `default_payment_card_last4` varchar(4) DEFAULT NULL,
  `created_at` datetime NOT NULL,
  `updated_at` datetime DEFAULT NULL,
  `mrr` int unsigned NOT NULL DEFAULT '0',
  `offer_id` varchar(24) DEFAULT NULL,
  `trial_start_at` datetime DEFAULT NULL,
  `trial_end_at` datetime DEFAULT NULL,
  `plan_id` varchar(255) NOT NULL,
  `plan_nickname` varchar(50) NOT NULL,
  `plan_interval` varchar(50) NOT NULL,
  `plan_amount` int NOT NULL,
  `plan_currency` varchar(191) NOT NULL,
  PRIMARY KEY (`id`),
  UNIQUE KEY `members_stripe_customers_subscriptions_subscription_id_unique` (`subscription_id`),
  KEY `members_stripe_customers_subscriptions_customer_id_foreign` (`customer_id`),
  KEY `mscs_ghost_subscription_id_foreign` (`ghost_subscription_id`),
  KEY `members_stripe_customers_subscriptions_stripe_price_id_index` (`stripe_price_id`),
  KEY `members_stripe_customers_subscriptions_offer_id_foreign` (`offer_id`),
  CONSTRAINT `members_stripe_customers_subscriptions_customer_id_foreign` FOREIGN KEY (`customer_id`) REFERENCES `members_stripe_customers` (`customer_id`) ON DELETE CASCADE,
  CONSTRAINT `members_stripe_customers_subscriptions_offer_id_foreign` FOREIGN KEY (`offer_id`) REFERENCES `offers` (`id`),
  CONSTRAINT `mscs_ghost_subscription_id_foreign` FOREIGN KEY (`ghost_subscription_id`) REFERENCES `subscriptions` (`id`) ON DELETE CASCADE
) ENGINE=InnoDB DEFAULT CHARSET=utf8mb4 COLLATE=utf8mb4_0900_ai_ci;
/*!40101 SET character_set_client = @saved_cs_client */;

--
-- Dumping data for table `members_stripe_customers_subscriptions`
--

LOCK TABLES `members_stripe_customers_subscriptions` WRITE;
/*!40000 ALTER TABLE `members_stripe_customers_subscriptions` DISABLE KEYS */;
/*!40000 ALTER TABLE `members_stripe_customers_subscriptions` ENABLE KEYS */;
UNLOCK TABLES;

--
-- Table structure for table `members_subscribe_events`
--

DROP TABLE IF EXISTS `members_subscribe_events`;
/*!40101 SET @saved_cs_client     = @@character_set_client */;
/*!50503 SET character_set_client = utf8mb4 */;
CREATE TABLE `members_subscribe_events` (
  `id` varchar(24) NOT NULL,
  `member_id` varchar(24) NOT NULL,
  `subscribed` tinyint(1) NOT NULL DEFAULT '1',
  `created_at` datetime NOT NULL,
  `source` varchar(50) DEFAULT NULL,
  `newsletter_id` varchar(24) DEFAULT NULL,
  PRIMARY KEY (`id`),
  KEY `members_subscribe_events_member_id_foreign` (`member_id`),
  KEY `members_subscribe_events_newsletter_id_created_at_index` (`newsletter_id`,`created_at`),
  CONSTRAINT `members_subscribe_events_member_id_foreign` FOREIGN KEY (`member_id`) REFERENCES `members` (`id`) ON DELETE CASCADE,
  CONSTRAINT `members_subscribe_events_newsletter_id_foreign` FOREIGN KEY (`newsletter_id`) REFERENCES `newsletters` (`id`)
) ENGINE=InnoDB DEFAULT CHARSET=utf8mb4 COLLATE=utf8mb4_0900_ai_ci;
/*!40101 SET character_set_client = @saved_cs_client */;

--
-- Dumping data for table `members_subscribe_events`
--

LOCK TABLES `members_subscribe_events` WRITE;
/*!40000 ALTER TABLE `members_subscribe_events` DISABLE KEYS */;
INSERT INTO `members_subscribe_events` VALUES ('694f26a6c3e44e650f0e60ba','694f26a6c3e44e650f0e60b6',1,'2025-12-27 00:21:58','admin','694f2665c3e44e650f0e5de9');
/*!40000 ALTER TABLE `members_subscribe_events` ENABLE KEYS */;
UNLOCK TABLES;

--
-- Table structure for table `members_subscription_created_events`
--

DROP TABLE IF EXISTS `members_subscription_created_events`;
/*!40101 SET @saved_cs_client     = @@character_set_client */;
/*!50503 SET character_set_client = utf8mb4 */;
CREATE TABLE `members_subscription_created_events` (
  `id` varchar(24) NOT NULL,
  `created_at` datetime NOT NULL,
  `member_id` varchar(24) NOT NULL,
  `subscription_id` varchar(24) NOT NULL,
  `attribution_id` varchar(24) DEFAULT NULL,
  `attribution_type` varchar(50) DEFAULT NULL,
  `attribution_url` varchar(2000) DEFAULT NULL,
  `referrer_source` varchar(191) DEFAULT NULL,
  `referrer_medium` varchar(191) DEFAULT NULL,
  `referrer_url` varchar(2000) DEFAULT NULL,
  `utm_source` varchar(191) DEFAULT NULL,
  `utm_medium` varchar(191) DEFAULT NULL,
  `utm_campaign` varchar(191) DEFAULT NULL,
  `utm_term` varchar(191) DEFAULT NULL,
  `utm_content` varchar(191) DEFAULT NULL,
  `batch_id` varchar(24) DEFAULT NULL,
  PRIMARY KEY (`id`),
  KEY `members_subscription_created_events_member_id_foreign` (`member_id`),
  KEY `members_subscription_created_events_subscription_id_foreign` (`subscription_id`),
  KEY `members_subscription_created_events_attribution_id_index` (`attribution_id`),
  CONSTRAINT `members_subscription_created_events_member_id_foreign` FOREIGN KEY (`member_id`) REFERENCES `members` (`id`) ON DELETE CASCADE,
  CONSTRAINT `members_subscription_created_events_subscription_id_foreign` FOREIGN KEY (`subscription_id`) REFERENCES `members_stripe_customers_subscriptions` (`id`) ON DELETE CASCADE
) ENGINE=InnoDB DEFAULT CHARSET=utf8mb4 COLLATE=utf8mb4_0900_ai_ci;
/*!40101 SET character_set_client = @saved_cs_client */;

--
-- Dumping data for table `members_subscription_created_events`
--

LOCK TABLES `members_subscription_created_events` WRITE;
/*!40000 ALTER TABLE `members_subscription_created_events` DISABLE KEYS */;
/*!40000 ALTER TABLE `members_subscription_created_events` ENABLE KEYS */;
UNLOCK TABLES;

--
-- Table structure for table `mentions`
--

DROP TABLE IF EXISTS `mentions`;
/*!40101 SET @saved_cs_client     = @@character_set_client */;
/*!50503 SET character_set_client = utf8mb4 */;
CREATE TABLE `mentions` (
  `id` varchar(24) NOT NULL,
  `source` varchar(2000) NOT NULL,
  `source_title` varchar(2000) DEFAULT NULL,
  `source_site_title` varchar(2000) DEFAULT NULL,
  `source_excerpt` varchar(2000) DEFAULT NULL,
  `source_author` varchar(2000) DEFAULT NULL,
  `source_featured_image` varchar(2000) DEFAULT NULL,
  `source_favicon` varchar(2000) DEFAULT NULL,
  `target` varchar(2000) NOT NULL,
  `resource_id` varchar(24) DEFAULT NULL,
  `resource_type` varchar(50) DEFAULT NULL,
  `created_at` datetime NOT NULL,
  `payload` text,
  `deleted` tinyint(1) NOT NULL DEFAULT '0',
  `verified` tinyint(1) NOT NULL DEFAULT '0',
  PRIMARY KEY (`id`)
) ENGINE=InnoDB DEFAULT CHARSET=utf8mb4 COLLATE=utf8mb4_0900_ai_ci;
/*!40101 SET character_set_client = @saved_cs_client */;

--
-- Dumping data for table `mentions`
--

LOCK TABLES `mentions` WRITE;
/*!40000 ALTER TABLE `mentions` DISABLE KEYS */;
/*!40000 ALTER TABLE `mentions` ENABLE KEYS */;
UNLOCK TABLES;

--
-- Table structure for table `migrations`
--

DROP TABLE IF EXISTS `migrations`;
/*!40101 SET @saved_cs_client     = @@character_set_client */;
/*!50503 SET character_set_client = utf8mb4 */;
CREATE TABLE `migrations` (
  `id` int unsigned NOT NULL AUTO_INCREMENT,
  `name` varchar(120) NOT NULL,
  `version` varchar(70) NOT NULL,
  `currentVersion` varchar(255) DEFAULT NULL,
  PRIMARY KEY (`id`),
  UNIQUE KEY `migrations_name_version_unique` (`name`,`version`)
) ENGINE=InnoDB AUTO_INCREMENT=241 DEFAULT CHARSET=utf8mb4 COLLATE=utf8mb4_0900_ai_ci;
/*!40101 SET character_set_client = @saved_cs_client */;

--
-- Dumping data for table `migrations`
--

LOCK TABLES `migrations` WRITE;
/*!40000 ALTER TABLE `migrations` DISABLE KEYS */;
INSERT INTO `migrations` VALUES (1,'1-create-tables.js','init','6.10'),(2,'2-create-fixtures.js','init','6.10'),(3,'01-final-v1.js','1.25','6.10'),(4,'02-noop.js','1.25','6.10'),(5,'01-final-v2.js','2.37','6.10'),(6,'01-final-v3.js','3.41','6.10'),(7,'2022-05-03-15-30-final-v4.js','4.47','6.10'),(8,'2022-05-04-10-03-no-op.js','4.47','6.10'),(9,'2022-03-14-12-33-delete-duplicate-offer-redemptions.js','5.0','6.10'),(10,'2022-03-28-15-25-backfill-mrr-adjustments-for-offers.js','5.0','6.10'),(11,'2022-04-25-10-32-backfill-mrr-for-discounted-subscriptions.js','5.0','6.10'),(12,'2022-04-26-15-44-backfill-mrr-events-for-canceled-subscriptions.js','5.0','6.10'),(13,'2022-04-27-11-26-backfill-mrr-for-canceled-subscriptions.js','5.0','6.10'),(14,'2022-04-28-03-26-remove-author-id-column-from-posts-table.js','5.0','6.10'),(15,'2022-05-03-09-39-drop-nullable-subscribe-event-newsletter-id.js','5.0','6.10'),(16,'2022-05-04-15-24-map-existing-emails-to-default-newsletter.js','5.0','6.10'),(17,'2022-05-05-13-13-migrate-legacy-recipient-filters.js','5.0','6.10'),(18,'2022-05-05-13-29-add-newsletters-admin-integration-permission-roles.js','5.0','6.10'),(19,'2022-05-05-15-17-drop-oauth-table.js','5.0','6.10'),(20,'2022-05-06-08-16-cleanup-client-subscriber-permissions.js','5.0','6.10'),(21,'2022-05-06-13-22-add-frontend-integration.js','5.0','6.10'),(22,'2022-05-09-10-00-drop-members-subscribed-column.js','5.0','6.10'),(23,'2022-05-09-14-17-cleanup-invalid-users-status.js','5.0','6.10'),(24,'2022-05-10-08-33-drop-members-analytics-table.js','5.0','6.10'),(25,'2022-05-10-14-57-cleanup-invalid-posts-status.js','5.0','6.10'),(26,'2022-05-11-12-08-drop-webhooks-status-column.js','5.0','6.10'),(27,'2022-05-11-13-12-rename-settings.js','5.0','6.10'),(28,'2022-05-11-16-36-remove-unused-settings.js','5.0','6.10'),(29,'2022-05-12-10-29-add-newsletter-permissions-for-editors-and-authors.js','5.0','6.10'),(30,'2022-05-12-13-51-add-label-permissions-for-authors.js','5.0','6.10'),(31,'2022-05-13-11-38-drop-none-email-recipient-filter.js','5.0','6.10'),(32,'2022-05-21-00-00-regenerate-posts-html.js','5.0','6.10'),(33,'2022-07-04-13-49-add-comments-table.js','5.3','6.10'),(34,'2022-07-05-09-36-add-comments-likes-table.js','5.3','6.10'),(35,'2022-07-05-09-47-add-comments-reports-table.js','5.3','6.10'),(36,'2022-07-05-10-00-add-comment-related-fields-to-members.js','5.3','6.10'),(37,'2022-07-05-12-55-add-comments-crud-permissions.js','5.3','6.10'),(38,'2022-07-05-15-35-add-comment-notifications-field-to-users-table.js','5.3','6.10'),(39,'2022-07-06-07-26-add-comments-enabled-setting.js','5.3','6.10'),(40,'2022-07-06-07-58-add-ghost-explore-integration-role.js','5.3','6.10'),(41,'2022-07-06-09-13-add-ghost-explore-integration-role-permissions.js','5.3','6.10'),(42,'2022-07-06-09-17-add-ghost-explore-integration.js','5.3','6.10'),(43,'2022-07-06-09-26-add-ghost-explore-integration-api-key.js','5.3','6.10'),(44,'2022-07-18-14-29-add-comment-reporting-permissions.js','5.5','6.10'),(45,'2022-07-18-14-31-drop-reports-reason.js','5.5','6.10'),(46,'2022-07-18-14-32-drop-nullable-member-id-from-likes.js','5.5','6.10'),(47,'2022-07-18-14-33-fix-comments-on-delete-foreign-keys.js','5.5','6.10'),(48,'2022-07-21-08-56-add-jobs-table.js','5.5','6.10'),(49,'2022-07-27-13-40-change-explore-type.js','5.6','6.10'),(50,'2022-08-02-06-09-add-trial-period-days-column-to-tiers.js','5.8','6.10'),(51,'2022-08-03-15-28-add-trial-start-column-to-stripe-subscriptions.js','5.8','6.10'),(52,'2022-08-03-15-29-add-trial-end-column-to-stripe-subscriptions.js','5.8','6.10'),(53,'2022-08-09-08-32-added-new-integration-type.js','5.9','6.10'),(54,'2022-08-15-05-34-add-expiry-at-column-to-members-products.js','5.10','6.10'),(55,'2022-08-16-14-25-add-member-created-events-table.js','5.10','6.10'),(56,'2022-08-16-14-25-add-subscription-created-events-table.js','5.10','6.10'),(57,'2022-08-19-14-15-fix-comments-deletion-strategy.js','5.10','6.10'),(58,'2022-08-22-11-03-add-member-alert-settings-columns-to-users.js','5.11','6.10'),(59,'2022-08-23-13-41-backfill-members-created-events.js','5.11','6.10'),(60,'2022-08-23-13-59-fix-page-resource-type.js','5.11','6.10'),(61,'2022-09-02-12-55-rename-members-bio-to-expertise.js','5.14','6.10'),(62,'2022-09-12-16-10-add-posts-lexical-column.js','5.15','6.10'),(63,'2022-09-14-12-46-add-email-track-clicks-setting.js','5.15','6.10'),(64,'2022-09-16-08-22-add-post-revisions-table.js','5.15','6.10'),(65,'2022-09-19-09-04-add-link-redirects-table.js','5.16','6.10'),(66,'2022-09-19-09-05-add-members-link-click-events-table.js','5.16','6.10'),(67,'2022-09-19-17-44-add-referrer-columns-to-member-events-table.js','5.16','6.10'),(68,'2022-09-19-17-44-add-referrer-columns-to-subscription-events-table.js','5.16','6.10'),(69,'2022-09-27-13-53-remove-click-tracking-tables.js','5.17','6.10'),(70,'2022-09-27-13-55-add-redirects-table.js','5.17','6.10'),(71,'2022-09-27-13-56-add-members-click-events-table.js','5.17','6.10'),(72,'2022-09-27-16-49-set-track-clicks-based-on-opens.js','5.17','6.10'),(73,'2022-09-29-12-39-add-track-clicks-column-to-emails.js','5.17','6.10'),(74,'2022-09-02-20-25-add-columns-to-products-table.js','5.19','6.10'),(75,'2022-09-02-20-52-backfill-new-product-columns.js','5.19','6.10'),(76,'2022-10-10-06-58-add-subscriptions-table.js','5.19','6.10'),(77,'2022-10-10-10-05-add-members-feedback-table.js','5.19','6.10'),(78,'2022-10-11-10-38-add-feedback-enabled-column-to-newsletters.js','5.19','6.10'),(79,'2022-10-18-05-39-drop-nullable-tier-id.js','5.20','6.10'),(80,'2022-10-18-10-13-add-ghost-subscription-id-column-to-mscs.js','5.20','6.10'),(81,'2022-10-19-11-17-add-link-browse-permissions.js','5.20','6.10'),(82,'2022-10-20-02-52-add-link-edit-permissions.js','5.20','6.10'),(83,'2022-10-24-07-23-disable-feedback-enabled.js','5.21','6.10'),(84,'2022-10-25-12-05-backfill-missed-products-columns.js','5.21','6.10'),(85,'2022-10-26-04-49-add-batch-id-members-created-events.js','5.21','6.10'),(86,'2022-10-26-04-49-add-batch-id-subscription-created-events.js','5.21','6.10'),(87,'2022-10-26-04-50-member-subscription-created-batch-id.js','5.21','6.10'),(88,'2022-10-26-09-32-add-feedback-enabled-column-to-emails.js','5.21','6.10'),(89,'2022-10-27-09-50-add-member-track-source-setting.js','5.21','6.10'),(90,'2022-10-31-12-03-backfill-new-product-columns.js','5.22','6.10'),(91,'2022-11-21-09-32-add-source-columns-to-emails-table.js','5.24','6.10'),(92,'2022-11-21-15-03-populate-source-column-with-html-for-emails.js','5.24','6.10'),(93,'2022-11-21-15-57-add-error-columns-for-email-batches.js','5.24','6.10'),(94,'2022-11-24-10-36-add-suppressions-table.js','5.25','6.10'),(95,'2022-11-24-10-37-add-email-spam-complaint-events-table.js','5.25','6.10'),(96,'2022-11-29-08-30-add-error-recipient-failures-table.js','5.25','6.10'),(97,'2022-12-13-16-15-add-usage-colums-to-tokens.js','5.27','6.10'),(98,'2023-01-04-04-12-drop-suppressions-table.js','5.27','6.10'),(99,'2023-01-04-04-13-add-suppressions-table.js','5.27','6.10'),(100,'2023-01-05-15-13-add-active-theme-permissions.js','5.28','6.10'),(101,'2023-01-11-02-45-truncate-suppressions.js','5.29','6.10'),(102,'2023-01-13-04-25-unsubscribe-suppressed-emails.js','5.30','6.10'),(103,'2022-12-05-09-56-update-newsletter-subscriptions.js','5.31','6.10'),(104,'2023-01-17-14-59-add-outbound-link-tagging-setting.js','5.31','6.10'),(105,'2023-01-19-07-46-add-mentions-table.js','5.31','6.10'),(106,'2023-01-24-08-00-fix-invalid-tier-expiry-for-paid-members.js','5.32','6.10'),(107,'2023-01-24-08-09-restore-incorrect-expired-tiers-for-members.js','5.32','6.10'),(108,'2023-01-30-07-27-add-mentions-permission.js','5.34','6.10'),(109,'2023-02-08-03-08-add-mentions-notifications-column.js','5.34','6.10'),(110,'2023-02-08-22-32-add-mentions-delete-column.js','5.34','6.10'),(111,'2023-02-13-06-24-add-mentions-verified-column.js','5.35','6.10'),(112,'2023-02-20-12-22-add-milestones-table.js','5.36','6.10'),(113,'2023-02-21-12-29-add-milestone-notifications-column.js','5.36','6.10'),(114,'2023-02-23-10-40-set-outbound-link-tagging-based-on-source-tracking.js','5.36','6.10'),(115,'2023-03-13-09-29-add-newsletter-show-post-title-section.js','5.39','6.10'),(116,'2023-03-13-13-11-add-newsletter-show-comment-cta.js','5.39','6.10'),(117,'2023-03-13-14-30-add-newsletter-show-subscription-details.js','5.39','6.10'),(118,'2023-03-14-12-26-add-last-mentions-email-report-timestamp-setting.js','5.39','6.10'),(119,'2023-03-13-14-05-add-newsletter-show-latest-posts.js','5.40','6.10'),(120,'2023-03-21-18-42-add-self-serve-integration-role.js','5.40','6.10'),(121,'2023-03-21-18-43-add-self-serve-migration-and-permissions.js','5.40','6.10'),(122,'2023-03-21-18-52-add-self-serve-integration.js','5.40','6.10'),(123,'2023-03-21-19-02-add-self-serve-integration-api-key.js','5.40','6.10'),(124,'2023-03-27-15-00-add-newsletter-colors.js','5.41','6.10'),(125,'2023-03-27-17-51-fix-self-serve-integration-api-key-type.js','5.41','6.10'),(126,'2023-04-04-07-03-add-portal-terms-settings.js','5.42','6.10'),(127,'2023-04-14-04-17-add-snippets-lexical-column.js','5.44','6.10'),(128,'2023-04-17-11-05-add-post-revision-author.js','5.45','6.10'),(129,'2023-04-18-12-56-add-announcement-settings.js','5.45','6.10'),(130,'2023-04-19-13-45-add-pintura-settings.js','5.45','6.10'),(131,'2023-04-20-14-19-add-announcement-visibility-setting.js','5.45','6.10'),(132,'2023-04-21-08-54-add-post-revision-status.js','5.45','6.10'),(133,'2023-04-21-10-30-add-feature-image-to-revisions.js','5.45','6.10'),(134,'2023-04-21-13-01-add-feature-image-meta-to-post-revisions.js','5.45','6.10'),(135,'2023-05-30-19-03-update-pintura-setting.js','5.51','6.10'),(136,'2023-06-07-10-17-add-collections-crud-persmissions.js','5.51','6.10'),(137,'2023-06-13-12-24-add-temp-mail-events-table.js','5.53','6.10'),(138,'2023-06-20-10-18-add-collections-table.js','5.53','6.10'),(139,'2023-06-20-10-19-add-collections-posts-table.js','5.53','6.10'),(140,'2023-07-07-11-57-add-show-title-and-feature-image-column-to-posts.js','5.54','6.10'),(141,'2023-07-10-05-15-55-add-built-in-collections.js','5.55','6.10'),(142,'2023-07-10-05-16-55-add-built-in-collection-posts.js','5.55','6.10'),(143,'2023-07-14-10-11-12-add-email-disabled-field-to-members.js','5.56','6.10'),(144,'2023-07-15-10-11-12-update-members-email-disabled-field.js','5.56','6.10'),(145,'2023-07-26-12-44-stripe-products-nullable-product.js','5.57','6.10'),(146,'2023-07-27-11-47-49-create-donation-events.js','5.57','6.10'),(147,'2023-08-02-09-42-add-donation-settings.js','5.58','6.10'),(148,'2023-08-07-10-42-add-donation-notifications-column.js','5.59','6.10'),(149,'2023-08-07-11-17-05-add-posts-published-at-index.js','5.59','6.10'),(150,'2023-08-29-10-17-add-recommendations-crud-permissions.js','5.61','6.10'),(151,'2023-08-29-11-39-10-add-recommendations-table.js','5.61','6.10'),(152,'2023-08-30-07-37-04-add-recommendations-enabled-settings.js','5.61','6.10'),(153,'2023-09-12-11-22-10-add-recommendation-click-events-table.js','5.63','6.10'),(154,'2023-09-12-11-22-11-add-recommendation-subscribe-events-table.js','5.63','6.10'),(155,'2023-09-13-13-03-10-add-ghost-core-content-integration.js','5.63','6.10'),(156,'2023-09-13-13-34-11-add-ghost-core-content-integration-key.js','5.63','6.10'),(157,'2023-09-19-04-25-40-truncate-stale-built-in-collections-posts.js','5.64','6.10'),(158,'2023-09-19-04-34-10-repopulate-built-in-collection-posts.js','5.64','6.10'),(159,'2023-09-22-06-42-15-truncate-stale-built-in-collections-posts.js','5.65','6.10'),(160,'2023-09-22-06-42-55-repopulate-built-in-featured-collection-posts.js','5.65','6.10'),(161,'2023-09-22-14-15-add-recommendation-notifications-column.js','5.66','6.10'),(162,'2023-10-03-00-32-32-rollback-source-theme.js','5.67','6.10'),(163,'2023-10-06-15-06-00-rename-recommendations-reason-to-description.js','5.69','6.10'),(164,'2023-10-31-11-06-00-members-created-attribution-id-index.js','5.72','6.10'),(165,'2023-10-31-11-06-01-members-subscription-created-attribution-id-index.js','5.72','6.10'),(166,'2023-11-14-11-15-00-add-transient-id-column-nullable.js','5.74','6.10'),(167,'2023-11-14-11-16-00-fill-transient-id-column.js','5.74','6.10'),(168,'2023-11-14-11-17-00-drop-nullable-transient-id-column.js','5.74','6.10'),(169,'2023-11-27-15-55-add-members-newsletters-index.js','5.75','6.10'),(170,'2023-12-05-11-00-add-portal-default-plan-setting.js','5.76','6.10'),(171,'2024-01-30-19-36-44-fix-discrepancy-in-free-tier-visibility.js','5.79','6.10'),(172,'2024-03-18-16-20-add-missing-post-permissions.js','5.81','6.10'),(173,'2024-03-25-16-46-10-add-email-recipients-email-id-indexes.js','5.82','6.10'),(174,'2024-03-25-16-51-29-drop-email-recipients-non-email-id-indexes.js','5.82','6.10'),(175,'2024-05-28-02-20-55-add-show-subhead-column-newsletters.js','5.83','6.10'),(176,'2024-06-04-09-13-33-rename-newsletters-show-subhead.js','5.84','6.10'),(177,'2024-06-04-11-10-37-add-custom-excerpt-to-post-revisions.js','5.84','6.10'),(178,'2024-06-05-08-42-34-populate-post-revisions-custom-excerpt.js','5.84','6.10'),(179,'2024-06-05-13-48-35-rename-newsletters-show-subtitle.js','5.84','6.10'),(180,'2024-06-10-14-53-31-add-posts-updated-at-index.js','5.85','6.10'),(181,'2024-06-25-12-08-20-add-posts-tags-post-tag-index.js','5.87','6.10'),(182,'2024-06-25-12-08-45-add-posts-type-status-updated-at-index.js','5.87','6.10'),(183,'2024-07-30-19-51-06-backfill-offer-redemptions.js','5.89','6.10'),(184,'2024-08-20-09-40-24-update-default-donations-suggested-amount.js','5.90','6.10'),(185,'2024-08-28-05-28-22-add-donation-message-column-to-donation-payment-events.js','5.91','6.10'),(186,'2024-09-03-18-51-01-update-stripe-prices-nickname-length.js','5.93','6.10'),(187,'2024-09-03-20-09-40-null-analytics-jobs-timings.js','5.94','6.10'),(188,'2024-10-08-14-25-27-added-body-font-settings.js','5.97','6.10'),(189,'2024-10-08-14-36-58-added-heading-font-setting.js','5.97','6.10'),(190,'2024-10-09-14-04-10-add-session-verification-field.js','5.97','6.10'),(191,'2024-10-10-01-02-03-add-signin-urls-permissions.js','5.97','6.10'),(192,'2024-10-31-15-27-42-add-jobs-queue-columns.js','5.100','6.10'),(193,'2024-11-05-14-48-08-add-comments-in-reply-to-id.js','5.100','6.10'),(194,'2024-11-06-04-45-15-add-activitypub-integration.js','5.100','6.10'),(195,'2024-12-02-17-32-40-alter-length-redirects-from.js','5.102','6.10'),(196,'2024-12-02-17-48-40-add-index-redirects-from.js','5.102','6.10'),(197,'2025-01-23-02-51-10-add-blocked-email-domains-setting.js','5.108','6.10'),(198,'2025-03-05-16-36-39-add-captcha-setting.js','5.111','6.10'),(199,'2025-03-10-10-01-01-add-require-mfa-setting.js','5.112','6.10'),(200,'2025-03-07-12-24-00-add-super-editor.js','5.113','6.10'),(201,'2025-03-07-12-25-00-add-member-perms-to-super-editor.js','5.113','6.10'),(202,'2025-03-19-03-13-04-add-index-to-posts-uuid.js','5.114','6.10'),(203,'2025-03-24-07-19-27-add-identity-read-permission-to-administrators.js','5.115','6.10'),(204,'2025-04-14-02-36-30-add-additional-social-accounts-columns-to-user-table.js','5.117','6.10'),(205,'2025-04-30-13-01-28-remove-captcha-setting.js','5.119','6.10'),(206,'2025-05-07-14-57-38-add-newsletters-button-corners-column.js','5.120','6.10'),(207,'2025-05-13-17-36-56-add-newsletters-button-style-column.js','5.120','6.10'),(208,'2025-05-14-20-00-15-add-newsletters-setting-columns.js','5.120','6.10'),(209,'2025-05-26-08-59-26-drop-newsletters-border-color-column.js','5.121','6.10'),(210,'2025-05-26-09-10-30-rename-newsletters-title-color-column.js','5.121','6.10'),(211,'2025-05-26-12-03-24-add-newsletters-color-columns.js','5.121','6.10'),(212,'2025-05-29-08-41-04-add-member-export-permissions-to-backup-integration.js','5.121','6.10'),(213,'2025-06-03-19-32-57-change-default-for-newsletters-button-color.js','5.122','6.10'),(214,'2025-06-06-23-12-11-create-site-uuid-setting.js','5.124','6.10'),(215,'2025-06-12-14-18-27-add-email-disabled-index.js','5.126','6.10'),(216,'2025-06-12-14-18-57-add-mse-newsletter-created-index.js','5.126','6.10'),(217,'2025-06-18-11-35-41-change-newsletters-link-color-default-to-accent.js','5.126','6.10'),(218,'2025-06-18-11-36-00-update-newsletters-link-color-null-to-accent.js','5.126','6.10'),(219,'2025-06-19-13-41-54-add-web-analytics-setting.js','5.127','6.10'),(220,'2025-06-26-09-36-41-add-social-web-setting.js','5.128','6.10'),(221,'2025-07-11-14-14-54-add-explore-settings.js','5.130','6.10'),(222,'2025-06-20-01-41-54-remove-updated-by-column.js','6.0','6.10'),(223,'2025-06-20-13-41-55-remove-created-by-column.js','6.0','6.10'),(224,'2025-06-23-09-49-25-add-missing-member-uuids.js','6.0','6.10'),(225,'2025-06-23-10-03-26-members-nullable-uuid.js','6.0','6.10'),(226,'2025-06-24-09-19-42-use-object-id-for-hardcoded-user-id.js','6.0','6.10'),(227,'2025-06-25-15-03-29-remove-amp-from-settings.js','6.0','6.10'),(228,'2025-06-30-13-59-10-remove-mail-events-table.js','6.0','6.10'),(229,'2025-06-30-14-00-00-update-feature-image-alt-length.js','6.0','6.10'),(230,'2025-09-11-00-38-13-add-uuid-column-to-tokens.js','6.1','6.10'),(231,'2025-09-11-00-39-08-backfill-tokens-uuid.js','6.1','6.10'),(232,'2025-09-11-00-39-36-tokens-drop-nullable-uuid.js','6.1','6.10'),(233,'2025-09-30-14-28-09-add-utm-fields.js','6.2','6.10'),(234,'2025-10-02-15-13-31-add-members-otc-secret-setting.js','6.3','6.10'),(235,'2025-10-13-10-18-38-add-tokens-otc-used-count-column.js','6.4','6.10'),(236,'2025-11-02-18-29-37-add-outbox-table.js','6.7','6.10'),(237,'2025-11-03-15-17-05-add-csd-email-count.js','6.7','6.10'),(238,'2025-11-03-15-18-04-add-email-batch-fallback-domain.js','6.7','6.10'),(239,'2025-12-01-21-04-36-add-automated-emails-table.js','6.10','6.10'),(240,'2025-12-01-21-04-37-add-automated-email-permissions.js','6.10','6.10');
/*!40000 ALTER TABLE `migrations` ENABLE KEYS */;
UNLOCK TABLES;

--
-- Table structure for table `migrations_lock`
--

DROP TABLE IF EXISTS `migrations_lock`;
/*!40101 SET @saved_cs_client     = @@character_set_client */;
/*!50503 SET character_set_client = utf8mb4 */;
CREATE TABLE `migrations_lock` (
  `lock_key` varchar(191) NOT NULL,
  `locked` tinyint(1) DEFAULT '0',
  `acquired_at` datetime DEFAULT NULL,
  `released_at` datetime DEFAULT NULL,
  PRIMARY KEY (`lock_key`)
) ENGINE=InnoDB DEFAULT CHARSET=utf8mb4 COLLATE=utf8mb4_0900_ai_ci;
/*!40101 SET character_set_client = @saved_cs_client */;

--
-- Dumping data for table `migrations_lock`
--

LOCK TABLES `migrations_lock` WRITE;
/*!40000 ALTER TABLE `migrations_lock` DISABLE KEYS */;
INSERT INTO `migrations_lock` VALUES ('km01',0,'2025-12-27 00:20:50','2025-12-27 00:20:54');
/*!40000 ALTER TABLE `migrations_lock` ENABLE KEYS */;
UNLOCK TABLES;

--
-- Table structure for table `milestones`
--

DROP TABLE IF EXISTS `milestones`;
/*!40101 SET @saved_cs_client     = @@character_set_client */;
/*!50503 SET character_set_client = utf8mb4 */;
CREATE TABLE `milestones` (
  `id` varchar(24) NOT NULL,
  `type` varchar(24) NOT NULL,
  `value` int NOT NULL,
  `currency` varchar(24) DEFAULT NULL,
  `created_at` datetime NOT NULL,
  `email_sent_at` datetime DEFAULT NULL,
  PRIMARY KEY (`id`)
) ENGINE=InnoDB DEFAULT CHARSET=utf8mb4 COLLATE=utf8mb4_0900_ai_ci;
/*!40101 SET character_set_client = @saved_cs_client */;

--
-- Dumping data for table `milestones`
--

LOCK TABLES `milestones` WRITE;
/*!40000 ALTER TABLE `milestones` DISABLE KEYS */;
INSERT INTO `milestones` VALUES ('694f2c047f4d7d685707e3ef','members',0,NULL,'2025-12-27 00:44:52',NULL);
/*!40000 ALTER TABLE `milestones` ENABLE KEYS */;
UNLOCK TABLES;

--
-- Table structure for table `mobiledoc_revisions`
--

DROP TABLE IF EXISTS `mobiledoc_revisions`;
/*!40101 SET @saved_cs_client     = @@character_set_client */;
/*!50503 SET character_set_client = utf8mb4 */;
CREATE TABLE `mobiledoc_revisions` (
  `id` varchar(24) NOT NULL,
  `post_id` varchar(24) NOT NULL,
  `mobiledoc` longtext,
  `created_at_ts` bigint NOT NULL,
  `created_at` datetime NOT NULL,
  PRIMARY KEY (`id`),
  KEY `mobiledoc_revisions_post_id_index` (`post_id`)
) ENGINE=InnoDB DEFAULT CHARSET=utf8mb4 COLLATE=utf8mb4_0900_ai_ci;
/*!40101 SET character_set_client = @saved_cs_client */;

--
-- Dumping data for table `mobiledoc_revisions`
--

LOCK TABLES `mobiledoc_revisions` WRITE;
/*!40000 ALTER TABLE `mobiledoc_revisions` DISABLE KEYS */;
INSERT INTO `mobiledoc_revisions` VALUES ('694f267cc3e44e650f0e60a9','694f2666c3e44e650f0e5e6a','{\"version\":\"0.3.1\",\"atoms\":[],\"cards\":[[\"hr\",{}]],\"markups\":[[\"a\",[\"href\",\"https://ghost.org\"]]],\"sections\":[[1,\"p\",[[0,[],0,\"This is an independent publication. If you subscribe today, you\'ll get full access to the website as well as email newsletters about new content when it\'s available. Your subscription makes this site possible. Thank you!\"]]],[1,\"h3\",[[0,[],0,\"Access all areas\"]]],[1,\"p\",[[0,[],0,\"By signing up, you\'ll get access to the full archive of everything that\'s been published before and everything that\'s still to come. Your very own private library.\"]]],[1,\"h3\",[[0,[],0,\"Fresh content, delivered\"]]],[1,\"p\",[[0,[],0,\"Stay up to date with new content sent straight to your inbox! No more worrying about whether you missed something because of a pesky algorithm or news feed.\"]]],[1,\"h3\",[[0,[],0,\"Meet people like you\"]]],[1,\"p\",[[0,[],0,\"Join a community of other subscribers who share the same interests.\"]]],[10,0],[1,\"h3\",[[0,[],0,\"Start your own thing\"]]],[1,\"p\",[[0,[],0,\"Enjoying the experience? Get started for free and set up your very own subscription business using \"],[0,[0],1,\"Ghost\"],[0,[],0,\", the same platform that powers this website.\"]]]],\"ghostVersion\":\"4.0\"}',1766794876868,'2025-12-27 00:21:16'),('694f267cc3e44e650f0e60aa','694f2666c3e44e650f0e5e6a','{\"version\":\"0.3.1\",\"atoms\":[],\"cards\":[[\"hr\",{}]],\"markups\":[[\"a\",[\"href\",\"https://ghost.org\"]]],\"sections\":[[1,\"p\",[[0,[],0,\"Performance Engineering is an independent publication launched in December 2025 by Manuel A Rodriguez. If you subscribe today, you\'ll get full access to the website as well as email newsletters about new content when it\'s available. Your subscription makes this site possible, and allows Performance Engineering to continue to exist. Thank you!\"]]],[1,\"h3\",[[0,[],0,\"Access all areas\"]]],[1,\"p\",[[0,[],0,\"By signing up, you\'ll get access to the full archive of everything that\'s been published before and everything that\'s still to come. Your very own private library.\"]]],[1,\"h3\",[[0,[],0,\"Fresh content, delivered\"]]],[1,\"p\",[[0,[],0,\"Stay up to date with new content sent straight to your inbox! No more worrying about whether you missed something because of a pesky algorithm or news feed.\"]]],[1,\"h3\",[[0,[],0,\"Meet people like you\"]]],[1,\"p\",[[0,[],0,\"Join a community of other subscribers who share the same interests.\"]]],[10,0],[1,\"h3\",[[0,[],0,\"Start your own thing\"]]],[1,\"p\",[[0,[],0,\"Enjoying the experience? Get started for free and set up your very own subscription business using \"],[0,[0],1,\"Ghost\"],[0,[],0,\", the same platform that powers this website.\"]]]],\"ghostVersion\":\"4.0\"}',1766794876869,'2025-12-27 00:21:16'),('694ff07f19c94fae00e3734d','694ff07e19c94fae00e37345','{\"version\":\"0.3.1\",\"atoms\":[],\"cards\":[[\"markdown\",{\"markdown\":\"# Prefer Fewer Fast CPU Cores Over Many Slow Ones for Latency-Sensitive Workloads\\n\\n## Subtitle\\nUnderstanding the trade-off between CPU core count and single-thread performance is critical for optimizing latency-sensitive applications where sequential execution matters more than raw parallelism.\\n\\n## Executive Summary (TL;DR)\\nChoosing fewer fast CPU cores over many slow cores is a performance principle that prioritizes single-thread performance and instruction-level parallelism (IPC) over core count. This approach delivers 20-40% better single-threaded performance, significantly lower latency for critical operations (often reducing response times from 50-100ms to 20-40ms), and improved throughput for applications with sequential dependencies. However, it sacrifices overall parallel throughput and scales poorly for highly concurrent workloads. Use this approach when latency matters more than throughput, for single-threaded applications, or when Amdahl\'s Law limits parallel speedup.\\n\\n## Problem Context\\nA common misconception in performance optimization is that more CPU cores always translate to better performance. Engineers often scale systems horizontally by adding cores without considering single-thread performance characteristics. This leads to scenarios where applications with sequential bottlenecks or latency-sensitive operations underperform despite high core counts.\\n\\n**Example: API REST Server Performance Issue**\\n\\nConsider a production API server handling authentication and data processing requests. A team migrates from an 8-core fast CPU (Intel Xeon @ 3.8 GHz) to a 32-core server CPU (AMD EPYC @ 2.5 GHz), expecting 4x better throughput. However, they observe the opposite: average response time increases from 35ms to 100ms, and P99 latency jumps from 65ms to 180ms.\\n\\nThe root cause: each API request has 8ms of sequential processing (input validation, authentication token verification, database connection setup, response serialization). With 32 slow cores, this sequential portion takes 15ms. With 8 fast cores, it takes 3ms. Even though the parallelizable database query runs slightly faster on more cores (12ms vs 15ms), the sequential bottleneck dominates: **32 slow cores = 15ms sequential + 12ms parallel = 27ms per request, but context switching overhead pushes it to 100ms. 8 fast cores = 3ms sequential + 15ms parallel = 18ms per request, with minimal overhead.**\\n\\nMany production systems suffer from this because modern CPUs designed for high core counts (such as some server-class processors) prioritize parallelism over single-thread speed. These processors trade higher clock frequencies and aggressive out-of-order execution for more cores, resulting in lower IPC (Instructions Per Cycle) and slower individual core performance.\\n\\nNaive scaling attempts often fail because:\\n- Sequential dependencies prevent effective parallelization (Amdahl\'s Law)\\n- Synchronization overhead increases with more cores\\n- Cache coherency traffic scales non-linearly with core count\\n- Critical paths remain bounded by single-thread performance\\n\\n## How It Works\\nCPU performance is determined by three key factors: clock frequency, IPC (Instructions Per Cycle), and core count. Fast cores achieve higher performance through:\\n\\n**Clock Frequency**: Higher clock rates allow more instructions to execute per second. Modern fast cores (e.g., Intel Core series, AMD Ryzen high-frequency variants) run at 4-6 GHz, while many-core processors often operate at 2-3 GHz.\\n\\n**Real-World Comparison**:\\n- **Many-core CPU**: AMD EPYC 7543 (32 cores @ 2.8 GHz base, IPC ~1.8)\\n- **Fast-core CPU**: Intel Core i9-12900K (8 performance cores @ 5.0 GHz, IPC ~3.2)\\n\\n**Performance Calculation**:\\nA single fast core at 5.0 GHz with IPC 3.2 can execute approximately **16 billion instructions per second** (5.0  10 Hz  3.2 IPC = 16  10 instructions/sec). A single slow core at 2.8 GHz with IPC 1.8 executes approximately **5 billion instructions per second** (2.8  10 Hz  1.8 IPC = 5.04  10 instructions/sec).\\n\\nFor a sequential algorithm (e.g., parsing JSON, computing a hash, traversing a linked list), the fast core completes in **3.1 seconds** what the slow core takes **10 seconds**. Even with 3 slow cores working in parallel, the fast core still wins because the work cannot be effectively parallelized.\\n\\n**Instruction-Level Parallelism (IPC)**: Fast cores employ deeper pipelines, wider execution units, larger instruction windows, and more aggressive branch prediction. This allows them to extract more parallelism from sequential code, executing multiple independent instructions simultaneously within a single thread.\\n\\n**Example**: Consider this sequential code:\\n```javascript\\nlet sum = 0;\\nfor (let i = 0; i < array.length; i++) {\\n    sum += array[i] * 2;\\n}\\n```\\n\\nA fast core with wide execution units can execute `array[i] * 2` and prepare the next iteration\'s memory fetch simultaneously, achieving IPC of 2.5-3.0. A slow core might achieve only 1.2-1.5 IPC on the same code, taking twice as long despite similar clock speeds.\\n\\n**Cache Hierarchy**: Fast cores typically feature larger, faster L1 and L2 caches per core, reducing memory latency for single-threaded workloads. They also have better prefetching logic that predicts memory access patterns.\\n\\n**Out-of-Order Execution**: Advanced out-of-order execution engines can reorder and parallelize independent instructions within a single thread, effectively creating instruction-level parallelism without explicit multi-threading.\\n\\nWhen a workload has sequential dependencies or Amdahl\'s Law limits parallel speedup, a single fast core can outperform multiple slow cores by completing the critical path faster, even if total theoretical throughput is lower.\\n\\n## Why This Becomes a Bottleneck\\nPerformance degrades when core speed is sacrificed for core count because overhead increases faster than useful work. Here\'s why fast cores outperform many slow cores:\\n\\n**1 Sequential Bottlenecks (Amdahl\'s Law)**\\n\\nEvery parallel algorithm has sequential portions that cannot be parallelized (data distribution, result aggregation, synchronization, I/O).\\n\\n**Why this matters**: Amdahl\'s Law shows that even 5% sequential code limits speedup to 20x regardless of core count. The sequential portion becomes the bottleneck.\\n\\n**Slow cores**:\\n- Execute sequential code slowly (e.g., 5ms for validation, logging, serialization)\\n- Sequential work dominates total latency\\n- More cores don\'t helpsequential work can\'t be parallelized\\n\\n**Fast cores**:\\n- Execute sequential code 2-3x faster (e.g., 2ms for the same validation work)\\n- Sequential bottleneck is reduced, improving overall latency\\n\\n**Example**: A web server processing requests has 5ms sequential work (input validation, authentication, response formatting) and 2ms parallelizable work (database query). With 32 slow cores: 5ms sequential + 2ms parallel = **7ms total** (plus 3-5ms overhead = 10-12ms). With 8 fast cores: 2ms sequential + 2ms parallel = **4ms total** (minimal overhead). Fast cores reduce sequential time by 60%, resulting in 50-60% better overall latency.\\n\\n**2 Context Switching Overhead**\\n\\nWhen a thread is paused, the OS must save registers, stack, and CPU state. This work produces no useful output.\\n\\n**Slow cores**:\\n- Fewer instructions per second\\n- Context switches take longer in real time\\n- More time is wasted switching than doing real work\\n- With many threads, context switching overhead dominates (can consume 15-25% of CPU time)\\n\\n**Fast cores**:\\n- Switch faster (same number of cycles, but cycles execute faster)\\n- Return to useful execution sooner\\n- Overhead becomes a smaller percentage of total time (5-10% overhead)\\n\\n**Example**: A server with 64 threads on 32 slow cores spends 20% of CPU time context switching. The same workload on 8 fast cores spends 8% of CPU time context switching, allowing 12% more CPU time for actual work.\\n\\n**3 Lock / Mutex Contention**\\n\\nShared resources require locks. Only one thread can enter the critical section at a time.\\n\\n**Slow cores**:\\n- Hold locks longer (execute critical section code slowly)\\n- Other threads wait idle\\n- Thread queues grow quickly\\n- Lock contention escalates exponentially with more cores\\n\\n**Fast cores**:\\n- Enter, execute, and release locks quickly\\n- Less waiting time for other threads\\n- Higher real throughput despite fewer cores\\n\\n**Example**: A database connection pool with 32 threads competing for 10 connections. On 32 slow cores, threads spend **85% of their time waiting** for locks (average wait time 15ms per operation because slow cores hold locks longer). On 8 fast cores with the same 32 threads, threads spend **15% of their time waiting** (average wait time 2.5ms because fast cores release locks 3x faster).\\n\\n**4 Cache Misses (Critical Factor)**\\n\\nCPU cache is much faster than RAM (L1 cache: ~1ns, RAM: ~100ns). Cache misses occur when needed data is not in cache.\\n\\n**Why cache misses happen**:\\n- Context switching evicts cache lines\\n- Other threads overwrite cache\\n- Multiple cores invalidate each other\'s cache (cache coherence protocol)\\n- Data working set does not fit in cache\\n\\n**Slow cores**:\\n- Execute fewer instructions before being preempted\\n- Lose cache more often (context switches evict cache lines)\\n- Cache misses dominate execution time (each miss costs 100-300ns)\\n- Higher cache miss rates (e.g., 25% miss rate with many cores)\\n\\n**Fast cores**:\\n- Finish work before cache eviction (complete tasks faster)\\n- Better cache locality (fewer threads = less cache contention)\\n- Memory latency is amortized over more useful work\\n- Lower cache miss rates (e.g., 8% miss rate with fewer cores)\\n\\n**Example**: Processing a shared data structure across cores. With 32 cores, cache miss rate is **25%** (cores frequently invalidate each other\'s cache lines via MESI protocol). With 8 cores, cache miss rate drops to **8%** (less contention, better locality). Each cache miss adds 100-300ns latency. The 32-core system spends 25-75ns per operation waiting for memory; the 8-core system spends 8-24ns per operation.\\n\\n**5 Cache Coherence & Core Synchronization**\\n\\nMulti-core CPUs must keep caches consistent (MESI protocol). Cache lines are invalidated across cores when data is modified.\\n\\n**Slow cores**:\\n- React slowly to invalidations (take longer to process coherence messages)\\n- Stall other cores waiting for synchronization\\n- Amplify synchronization delays (slow cores hold cache lines longer)\\n- Cache ping-pong effect: data bounces between cores\' caches\\n\\n**Fast cores**:\\n- Synchronize quickly (process coherence messages faster)\\n- Reduce global latency (faster invalidation propagation)\\n- Less cache ping-pong (fewer cores = fewer invalidations needed)\\n\\n**Example**: A shared counter incremented by multiple threads. With 32 slow cores, each increment triggers cache invalidations across all cores, causing ping-pong. Each increment takes 150ns due to coherence overhead. With 8 fast cores, fewer invalidations are needed, and each increment takes 40ns**3.75x faster**.\\n\\n**6 Blocking I/O (Network, Disk, DB)**\\n\\nMany applications are I/O-bound, not CPU-bound. Threads wait for external resources (network responses, disk reads, database queries).\\n\\n**Slow cores**:\\n- Process I/O results slowly when data arrives\\n- Hold buffers and locks longer while processing\\n- Increase request latency (I/O wait time + slow processing time)\\n\\n**Fast cores**:\\n- Process responses immediately when I/O completes\\n- Free resources quickly (buffers, locks, connections)\\n- Lower end-to-end latency (I/O wait time + fast processing time)\\n\\n**Example**: An e-commerce API processes orders with 99% parallel work (payment processing, inventory checks) and 1% sequential work (final order commit to database). The sequential database write takes 50ms on slow cores vs 20ms on fast cores. Even though 99% of work completes in 10ms on both systems, P99 latency is **60ms on slow cores vs 30ms on fast cores**the sequential I/O bottleneck dominates tail latency.\\n\\n**7 Net Effect**\\n\\n**More cores  more performance when overhead dominates**\\n\\n- Overhead increases with core count (context switching, lock contention, cache coherence)\\n- Slow cores amplify overhead (each overhead event takes longer)\\n- Fast cores minimize overhead (overhead events complete faster)\\n- Sequential work can\'t be parallelizedfast cores are the only solution\\n- Real throughput = Useful work / (Useful work + Overhead). Fast cores reduce both numerator (faster work) and denominator (less overhead)\\n\\n**Example Summary**: A web server handling 10,000 requests/second. With 32 slow cores: 7ms useful work + 5ms overhead = **12ms per request**. With 8 fast cores: 4ms useful work + 1ms overhead = **5ms per request**. Fast cores deliver **2.4x better latency** with 4x fewer cores.\\n\\n## Advantages\\n- **Superior Single-Thread Performance**: 20-40% better performance on single-threaded workloads compared to many-core processors with lower IPC\\n- **Reduced Latency**: Critical operations complete 50-100% faster (e.g., reducing response times from 50-100ms to 20-40ms in observed benchmarks)\\n- **Lower Synchronization Overhead**: Fewer cores mean fewer lock contentions, atomic operations, and cache coherency messages\\n- **Better Cache Locality**: Larger per-core caches and fewer threads reduce cache misses and improve memory access patterns\\n- **Simpler Architecture**: Less need for complex thread pools, work-stealing schedulers, and parallel algorithms\\n- **Predictable Performance**: More deterministic latency characteristics, crucial for real-time systems\\n- **Higher Throughput for Sequential Workloads**: Applications that cannot be parallelized achieve 15-30% better throughput even with fewer total cores\\n\\n## Disadvantages and Trade-offs\\n- **Limited Parallel Throughput**: Cannot match the total compute capacity of many-core systems for embarrassingly parallel workloads\\n- **Higher Per-Core Cost**: Fast cores with advanced microarchitecture features are more expensive to manufacture\\n- **Poor Scalability for Concurrent Workloads**: Request-per-thread server models (like traditional web servers) benefit less when request count exceeds fast core count\\n- **Lower Total Compute Capacity**: Cannot execute as many independent threads simultaneously\\n- **Underutilization Risk**: If workloads can be fully parallelized, fast cores may sit idle while waiting for I/O, wasting resources\\n- **Memory Bandwidth Limits**: Fewer cores mean fewer memory controllers, potentially limiting memory bandwidth for data-parallel workloads\\n\\n## When to Use This Approach\\n- **Latency-Sensitive Applications**: Real-time systems, trading platforms, gaming servers, interactive applications where P99 latency matters more than throughput\\n  \\n  **Example - Trading Platform**: A high-frequency trading system processing order execution. With 32 slow cores, P99 latency is **85ms** (order validation, risk checks, market data processing are sequential). With 8 fast cores, P99 latency drops to **35ms**a **58% improvement**. This directly impacts profitability, as faster execution captures better prices.\\n\\n- **Sequential Workloads**: Applications with Amdahl\'s Law limits, legacy code that cannot be parallelized, algorithms with strong data dependencies\\n\\n- **Single-Threaded Applications**: Node.js event loops, Python GIL-bound code, JavaScript engines, database query execution (single query optimization)\\n  \\n  **Example - Node.js API**: A REST API built with Node.js handling 5,000 requests/second. Node.js runs on a single event loop thread per process. With 16 slow cores running 4 Node.js processes (4 cores each), CPU utilization is **95%** and average latency is 45ms. With 8 fast cores running 4 Node.js processes (2 cores each), CPU utilization is **65%** and average latency is 22ms**51% faster response times** with lower resource usage.\\n\\n- **Cache-Sensitive Workloads**: Applications where cache locality matters more than parallelism (many scientific computing kernels, graph algorithms)\\n\\n- **I/O-Bound Systems with Critical Paths**: Systems where CPU work per request is minimal but latency is critical (API gateways, load balancers with simple routing logic)\\n\\n- **Database Query Execution**: Single complex queries that cannot be parallelized effectively.\\n  \\n  **Example - Database Query**: A complex SQL query with multiple joins and aggregations executes on a single thread. On 32 slow cores, the query takes **450ms** (single-threaded execution doesn\'t benefit from extra cores). On 8 fast cores, the same query completes in **280ms**a **38% improvement** because the single query thread runs faster.\\n\\n- **Development and Testing**: Faster iteration cycles when single-thread performance improves compile times and test execution speed\\n\\n## When Not to Use It\\n- **Highly Parallel Workloads**: Data processing pipelines, batch jobs, scientific simulations with perfect parallelism (no shared state)\\n\\n- **Embarrassingly Parallel Problems**: Image processing, video encoding, Monte Carlo simulations where each unit of work is independent\\n  \\n  **Example - Video Encoding**: Encoding a 4K video (38402160, 60fps, 10 minutes) using H.264 encoding. With 32 slow cores, encoding completes in **15 minutes** (workload perfectly parallelizes across frames). With 8 fast cores, encoding takes **42 minutes****2.8x slower** because the workload benefits from core count, not core speed.\\n\\n  **Example - Batch Image Processing**: Processing 1 million images (resizing, format conversion, thumbnail generation). With 32 slow cores, the batch completes in **2 hours** (31,250 images per hour per core  32 cores). With 8 fast cores, it takes **8 hours** (125,000 images per hour per core  8 cores, but each core processes faster, still net slower than 32 cores).\\n\\n- **Request-Per-Thread Servers**: Traditional threaded web servers handling thousands of concurrent connections (more cores allow more simultaneous request processing)\\n\\n- **Cost-Optimized Deployments**: When total compute capacity per dollar is the primary metric and latency requirements are relaxed\\n\\n- **Cloud Environments with Auto-Scaling**: When horizontal scaling is cheaper than vertical scaling and workload can be distributed\\n\\n- **Containers with Thread Pools**: Applications using thread pools larger than available fast cores, where additional slow cores provide better resource utilization\\n\\n## Performance Impact\\nReal-world observations show measurable improvements across different workload types:\\n\\n**Comparative Performance Table**:\\n\\n| Caso de Uso | Configuracin | Mtrica Clave | Resultado | Mejora |\\n|-------------|---------------|---------------|-----------|--------|\\n| Trading System | 32 cores lentos vs 8 cores rpidos | P99 Latency | 85ms  35ms | 58% |\\n| Node.js REST API | 16 cores lentos vs 8 cores rpidos | Avg Latency | 45ms  22ms | 51% |\\n| Database Query (Single-threaded) | 32 cores vs 8 cores rpidos | Query Time | 450ms  280ms | 38% |\\n| Microservices Gateway | 16 cores vs 8 cores rpidos | P95 Latency | 120ms  55ms | 54% |\\n| JSON Parsing Service | 32 cores vs 8 cores rpidos | Throughput | 8K ops/sec  12K ops/sec | 50% |\\n| WebSocket Server | 24 cores vs 8 cores rpidos | Connection Latency | 25ms  12ms | 52% |\\n\\n**Latency Improvements**: P50 latency reductions of 30-50% and P99 latency improvements of 40-60% in latency-sensitive applications. For example, a trading system reduced order processing latency from 85ms to 35ms by switching from 32 slow cores to 8 fast cores.\\n\\n**Single-Thread Throughput**: 20-40% improvement in single-threaded benchmarks (SPEC CPU benchmarks show this consistently). JavaScript V8 benchmarks show 25-35% improvements on fast-core architectures. A concrete example: parsing a 10MB JSON file takes 280ms on a slow core vs 180ms on a fast core**36% faster**.\\n\\n**Sequential Workload Throughput**: Even with fewer total cores, applications with 10-20% sequential code show 15-30% better overall throughput because the critical path completes faster.\\n\\n**Example - Web Application**: An e-commerce application with 15% sequential code (authentication, session management, order finalization) processes orders. With 32 slow cores: 15ms sequential + 85ms parallel = 100ms per order, throughput of 320 orders/second. With 8 fast cores: 6ms sequential + 85ms parallel = 91ms per order, throughput of 88 orders/second per process. Running 4 processes (total 32 cores equivalent), throughput is **352 orders/second****10% better** despite the same core count, because the sequential bottleneck is reduced.\\n\\n**Resource Utilization**: Lower CPU utilization (50-70% vs 80-95%) but better response time characteristics, indicating that cores are not the bottleneck but rather single-thread speed.\\n\\n**Energy Efficiency**: Better performance per watt for single-threaded workloads, though total system power may be lower with fewer cores.\\n\\n## Common Mistakes\\n- **Assuming More Cores Always Help**: Adding cores to applications with sequential bottlenecks without profiling to identify the actual bottleneck\\n  \\n  **Example**: A Node.js API server handling 3,000 requests/second on 8 cores shows 85% CPU utilization. The team adds 8 more cores (total 16), expecting 50% CPU utilization and better throughput. Result: CPU utilization drops to 60%, but latency **increases by 15%** (from 25ms to 29ms average) due to increased context switching overhead. The bottleneck was single-thread performance in the event loop, not lack of cores. Solution: Switch to 8 faster cores instead of adding more slow cores.\\n\\n- **Ignoring Amdahl\'s Law**: Failing to calculate theoretical speedup limits based on sequential code percentage\\n  \\n  **Example**: An application has 10% sequential code and 90% parallelizable code. The team calculates: \\\"With 32 cores, we should get 32x speedup on the parallel part, so overall speedup should be close to 32x.\\\" Reality: Amdahl\'s Law shows maximum speedup is 1 / (0.1 + 0.9/32) = **7.6x**, not 32x. Adding more cores beyond 8-16 provides diminishing returns. The sequential 10% becomes the bottleneck.\\n\\n- **Over-Parallelization**: Creating excessive threads that contend for fast cores, leading to context switching overhead that negates single-thread advantages\\n  \\n  **Example**: A Java application running on 8 fast cores creates a thread pool with 64 threads (8 threads per core). Each thread competes for CPU time, causing frequent context switches. Result: CPU spends 20% of time context switching instead of executing code. Latency is 40ms vs 18ms with an 8-thread pool (matching core count). The extra threads negate the fast-core advantage.\\n\\n- **Mismatched Architecture Patterns**: Using request-per-thread models (many threads) with fast-core architectures instead of event-driven or async models\\n  \\n  **Example**: A C++ web server uses one thread per request (traditional Apache-style). On 32 slow cores, it handles 32 concurrent requests efficiently. When migrated to 8 fast cores, it can only handle 8 concurrent requests per process, requiring multiple processes and load balancing. The architecture doesn\'t leverage fast cores effectively. Better approach: Use async I/O (epoll/kqueue) to handle thousands of concurrent requests on 8 fast cores.\\n\\n- **Not Profiling Single-Thread Performance**: Optimizing for multi-threaded scenarios without measuring whether single-thread speed is the actual constraint\\n  \\n  **Example**: A team observes high CPU utilization (90%) and assumes they need more cores. They add cores, but latency doesn\'t improve. Profiling reveals: single request processing takes 50ms, but only 5ms is CPU-bound (the rest is I/O wait). The bottleneck is I/O, not CPU cores. Adding fast cores won\'t help; optimizing I/O (async operations, connection pooling) is the solution.\\n\\n- **Cache-Unaware Algorithms**: Implementing algorithms that ignore cache locality, wasting the cache advantages of fast cores\\n  \\n  **Example**: A graph traversal algorithm uses a linked list data structure, causing random memory access patterns. On fast cores with good cache prefetching, cache miss rate is still 18% due to poor locality. Reimplementing with an array-based structure improves cache locality: cache miss rate drops to 4%, and traversal speed improves by 3x. Fast cores\' cache advantages are wasted without cache-aware algorithms.\\n\\n- **Benchmarking Synthetic Loads**: Testing with perfectly parallel synthetic workloads instead of realistic production traffic patterns\\n  \\n  **Example**: A team benchmarks their application with a synthetic workload that processes independent tasks in parallel (100% parallelizable). Results show 32 slow cores outperform 8 fast cores by 4x. They choose 32 slow cores. In production, the real workload has 20% sequential code (authentication, logging, serialization). Result: 8 fast cores actually perform 25% better than 32 slow cores because the sequential bottleneck wasn\'t captured in the synthetic benchmark.\\n\\n## How to Measure and Validate\\n**Profiling Tools**:\\n- Use `perf` (Linux) to measure CPI (Cycles Per Instruction), cache misses, and branch mispredictions\\n- Profile with tools like `vtune` or `perf top` to identify if single-thread performance or parallelism is the bottleneck\\n- Measure IPC metrics: instructions retired per cycle should be higher on fast cores (typically 2-4 IPC vs 1-2 IPC on slow cores)\\n\\n**Example - Using `perf` to Measure CPI**:\\n```bash\\n# Profile a single-threaded application\\nperf stat -e cycles,instructions,cache-references,cache-misses ./my-app\\n\\n# Example output on slow core:\\n# 1,250,000,000 cycles\\n# 2,187,500,000 instructions\\n# CPI = 1,250,000,000 / 2,187,500,000 = 0.57 (this is actually IPC, CPI would be 1.75)\\n# IPC = 2,187,500,000 / 1,250,000,000 = 1.75 instructions per cycle\\n\\n# Example output on fast core (same workload):\\n# 625,000,000 cycles\\n# 2,187,500,000 instructions\\n# IPC = 2,187,500,000 / 625,000,000 = 3.5 instructions per cycle\\n```\\n\\n**Interpreting Results**: If IPC < 2.0 on single-threaded workloads, the CPU is likely a bottleneck. Fast cores typically achieve IPC of 2.5-4.0 on optimized code. If CPI > 2.0 (IPC < 0.5), consider cores with better single-thread performance.\\n\\n**Key Metrics**:\\n- **Latency Percentiles**: Track P50, P95, P99 latency - fast cores should show lower tail latencies\\n  \\n  **Example Benchmark Results**:\\n  - Slow cores: P50 = 45ms, P95 = 120ms, P99 = 180ms\\n  - Fast cores: P50 = 22ms, P95 = 55ms, P99 = 85ms\\n  - The P99 improvement (53% faster) indicates sequential bottlenecks are being resolved.\\n\\n- **Single-Thread Throughput**: Benchmark single-threaded execution time for critical paths\\n  \\n  **Example**: Measure time to process 1,000 database records sequentially:\\n  - Slow core: 12.5 seconds (12.5ms per record)\\n  - Fast core: 7.8 seconds (7.8ms per record)\\n  - **38% faster** indicates the workload benefits from fast cores.\\n\\n- **CPU Utilization**: Lower utilization with better performance indicates single-thread speedup\\n  \\n  **Example**: API server handling 5,000 req/sec:\\n  - 32 slow cores: 95% CPU utilization, 45ms avg latency\\n  - 8 fast cores: 65% CPU utilization, 22ms avg latency\\n  - Lower utilization + better performance = single-thread speed is the constraint, not parallelism.\\n\\n- **Context Switch Rate**: Fewer context switches per request with fast cores\\n  \\n  **Example**: `vmstat 1` shows context switches:\\n  - Slow cores: 15,000 context switches/second, 3.0 switches per request\\n  - Fast cores: 8,000 context switches/second, 1.6 switches per request\\n  - Fewer switches mean less overhead and better cache locality.\\n\\n- **Cache Hit Rates**: Monitor L1/L2/L3 cache hit ratios - fast cores should show better locality\\n  \\n  **Example**: Using `perf` to measure cache performance:\\n  ```bash\\n  perf stat -e L1-dcache-loads,L1-dcache-load-misses,L2-cache-loads,L2-cache-load-misses ./app\\n  ```\\n  - Slow cores: L1 miss rate 12%, L2 miss rate 8%\\n  - Fast cores: L1 miss rate 5%, L2 miss rate 3%\\n  - Lower miss rates indicate better cache locality and prefetching.\\n\\n**Benchmarking Strategy**:\\n1. Run single-threaded benchmarks (SPEC CPU, single-threaded application tests)\\n   \\n   **Example**: SPEC CPU2017 single-threaded benchmark:\\n   - Slow core: 35 points (normalized score)\\n   - Fast core: 52 points (normalized score)\\n   - **49% improvement** in single-thread performance.\\n\\n2. Measure critical path latency under production load\\n3. Compare same workload on many-core vs few-core-fast systems\\n4. Use realistic load patterns, not synthetic parallel workloads\\n5. Measure tail latencies, not just averages\\n\\n**Production Validation**:\\n- A/B test with canary deployments comparing core configurations\\n  \\n  **Example Deployment Strategy**:\\n  1. Deploy 10% of traffic to fast-core servers\\n  2. Monitor for 24-48 hours\\n  3. Compare metrics: latency (P50, P95, P99), error rates, throughput\\n  4. If fast cores show 30%+ latency improvement with stable error rates, gradually migrate traffic\\n\\n- Monitor application-level metrics (request latency, transaction completion time)\\n- Track system metrics (CPU utilization, context switches, cache performance)\\n- Validate that improvements translate to business metrics (user experience, revenue)\\n\\n## Summary and Key Takeaways\\nThe core principle: **Fewer fast CPU cores outperform many slow cores when single-thread performance matters more than total parallel throughput**. This trade-off is fundamental in CPU architecture and should guide hardware selection and system design.\\n\\nThe main trade-off is between latency/sequential performance and total parallel capacity. Fast cores excel at reducing critical path latency and improving single-thread execution, while many-core systems excel at total throughput for parallelizable workloads.\\n\\n**Decision Guideline**: Choose fast cores when (1) latency requirements are strict (P99 < 100ms), (2) workloads have sequential dependencies (Amdahl\'s Law limits apply), (3) single-thread performance is the bottleneck (profiling confirms), or (4) cache locality matters more than parallelism. Choose many cores when (1) workloads are embarrassingly parallel, (2) total throughput is the primary metric, (3) cost per compute unit is critical, or (4) request-per-thread models handle massive concurrency.\\n\\nAlways profile before deciding: measure IPC, latency percentiles, and identify whether the bottleneck is sequential execution or parallel capacity.\"}]],\"markups\":[],\"sections\":[[10,0]]}',1766846591218,'2025-12-27 14:43:11'),('6950348e19c94fae00e37366','6950348e19c94fae00e37362','{\"version\":\"0.3.1\",\"atoms\":[],\"cards\":[[\"markdown\",{\"markdown\":\"# Reduce Context Switching\\n\\n## Subtitle\\nContext switching overhead becomes a measurable performance bottleneck in high-throughput systems when thread or process counts exceed what the workload and CPU cores can efficiently sustain.\\n\\n## Executive Summary (TL;DR)\\nContext switching occurs when the operating system suspends one thread or process and resumes another. Each switch requires saving and restoring CPU state and often disrupts CPU caches. Excessive context switching wastes CPU cycles, reduces cache locality, and increases latency. Reducing unnecessary context switches improves throughput, tail latency, and execution predictability. This optimization is most effective in CPU-bound workloads where thread counts exceed available CPU cores. Typical improvements range from **515% throughput**, with larger gains in poorly tuned systems.\\n\\n---\\n## Problem Context\\nA common misconception is that increasing the number of threads automatically improves performance. This leads to thread-per-request architectures or spawning threads for each concurrent operation. While this can be effective for I/O-bound workloads, it becomes counterproductive for CPU-bound systems or when the number of threads significantly exceeds the number of CPU cores.\\n\\nIn such scenarios, the operating system scheduler must constantly preempt threads to share CPU time. Each preemption incurs overhead and often destroys cache locality. In production systems, this manifests as high CPU usage with low throughput, increased tail latency, and unstable performance under load.\\n\\nTypical mistakes include:\\n- Creating thousands of threads\\n- Blocking threads on I/O\\n- Misconfigured thread pools\\n- Ignoring CPU core limits\\n- Mixing CPU-bound and I/O-bound work in the same execution model\\n\\n---\\n## How It Works\\nContext switching is an OS-level operation in which the kernel stops executing one thread and resumes another. Even on modern hardware, a context switch involves:\\n\\n- **Register save/restore**: CPU registers, program counter, and stack pointer must be saved and restored\\n- **Memory management updates**: Page tables, TLB entries, and protection states may change\\n- **Cache disruption**: The newly scheduled thread typically accesses different memory, causing cache misses\\n- **Scheduler execution**: The kernel must select the next runnable thread and update scheduling metadata\\n\\nA single context switch typically costs **110 microseconds**, but the indirect cost from cache misses can be much higher. Frequent switching prevents caches from warming up, significantly reducing effective CPU throughput.\\n\\nA critical but often overlooked factor is the **runnable queue**. The scheduler maintains a queue of runnable threads. The larger this queue becomes, the more often the scheduler must intervene to enforce fairness, increasing context switching frequency.\\n\\n---\\n## Voluntary vs Involuntary Context Switching\\n\\nContext switches fall into two categories:\\n\\n### Voluntary Context Switching\\nOccurs when a thread explicitly yields the CPU:\\n- Blocking I/O\\n- Waiting on a lock or condition variable\\n- `Thread.Sleep`, `Yield`\\n\\nThese switches happen because the thread *chooses* to stop running.\\n\\n### Involuntary Context Switching\\nOccurs when the scheduler preempts a runnable thread:\\n- Time quantum expiration\\n- Higher-priority thread becomes runnable\\n- Fairness enforcement by the scheduler\\n\\nThese switches happen even if the thread is making progress.\\n\\n**Key insight**  \\nReducing voluntary context switches lowers scheduler churn.  \\nReducing involuntary context switches requires limiting the number of runnable threads.\\n\\n---\\n## Why This Becomes a Bottleneck\\nContext switching becomes a bottleneck when:\\n\\n1. **Thread count exceeds CPU cores**  \\n   With N runnable threads competing for M cores (N >> M), the runnable queue grows. The scheduler must frequently preempt threads to enforce fairness, causing constant context switching.\\n\\n2. **Time quantum expiration**  \\n   Modern schedulers enforce fairness using time slices (quanta). When many threads compete for CPU time, each quantum expiration forces a context switch, even if the thread is making useful progress.\\n\\n3. **Poor thread affinity**  \\n   Threads migrate between CPU cores, losing cache locality and triggering cache coherency traffic. Migration increases scheduler activity and raises the probability of additional preemption.\\n\\n4. **Lock contention**  \\n   Threads block on locks, forcing voluntary context switches. When locks are released, blocked threads must be rescheduled, causing additional involuntary context switches.\\n\\n5. **Inefficient synchronization**  \\n   Blocking I/O and mutex-heavy designs repeatedly move threads between blocked and runnable states, inflating the runnable queue and scheduler workload.\\n\\n6. **Workload imbalance**  \\n   Short-lived tasks interrupt long-running ones, increasing preemption frequency and preventing cache warm-up.\\n\\nThese effects compound into:\\n- Cache thrashing\\n- Increased scheduler overhead\\n- Implicit serialization\\n- Amplified tail latency\\n\\n---\\n## When to Use This Approach\\n- CPU-bound workloads\\n- High-throughput services\\n- Latency-sensitive systems (trading, real-time, gaming)\\n- Systems with excessive runnable thread counts\\n- Profiling shows >510% time in context switching\\n- Cache-sensitive algorithms\\n\\n---\\n## When Not to Use It\\n- I/O-bound workloads dominated by waiting\\n- Interactive user-facing systems\\n- Systems with thread counts near CPU core count\\n- Systems without profiling evidence\\n- Fairness-critical workloads\\n\\n---\\n## Performance Impact\\nObserved improvements:\\n- **515% throughput increase** in CPU-bound systems\\n- **1030% reduction in p95/p99 latency**\\n- **2040% cache hit rate improvement**\\n- Lower CPU usage for the same workload\\n\\nImpact grows as:\\n- Runnable thread count increases relative to CPU cores\\n- Cache sensitivity increases\\n- Context switch frequency increases\\n\\n---\\n## Common Mistakes\\n- Thread-per-request architectures\\n- Ignoring thread pool configuration\\n- Over-parallelizing CPU-bound work\\n- Blocking inside async code\\n- Optimizing without profiling\\n- Ignoring CPU affinity and NUMA\\n- Premature optimization\\n\\n---\\n## How to Reduce Context Switching\\n\\n### 1. Use Thread Pools Instead of Creating Threads\\n\\n**Why this reduces context switching**\\n\\nCreating threads dynamically increases the number of runnable threads, inflating the runnable queue. When the runnable queue grows larger than the number of CPU cores, the scheduler must preempt threads more frequently.\\n\\nThread pools:\\n- Reuse a fixed number of threads\\n- Cap the runnable queue size\\n- Prevent thread explosion under load\\n\\n**Cause  Effect**\\n\\nMore runnable threads  larger runnable queue  more preemption  more context switches  \\nBounded thread pool  smaller runnable queue  fewer context switches\\n\\n---\\n### 2. Limit Concurrency to CPU Cores for CPU-Bound Work\\n\\n**Why this reduces context switching**\\n\\nCPU-bound threads are always runnable. When CPU-bound concurrency exceeds core count, the scheduler enforces fairness via time quanta. Each quantum expiration forces an involuntary context switch.\\n\\nLimiting concurrency allows threads to run longer without preemption.\\n\\n**Cause  Effect**\\n\\nCPU-bound threads > cores  quantum expiration  forced context switches  \\nCPU-bound threads  cores  fewer quantum expirations  fewer context switches\\n\\n---\\n### 3. Use Async / Non-Blocking I/O Instead of Blocking Calls\\n\\n**Why this reduces context switching**\\n\\nBlocking I/O causes voluntary context switches followed by involuntary rescheduling when I/O completes. This repeatedly moves threads in and out of the runnable queue.\\n\\nAsync I/O avoids thread parking entirely and keeps runnable thread count low.\\n\\n**Cause  Effect**\\n\\nBlocking I/O  block/unblock  runnable queue churn  context switches  \\nAsync I/O  fewer runnable threads  fewer context switches\\n\\n---\\n### 4. Minimize Lock Contention\\n\\n**Why this reduces context switching**\\n\\nLock contention causes voluntary blocking and later involuntary rescheduling. Each block/unblock cycle adds context switches and increases runnable queue pressure.\\n\\nReducing contention allows threads to continue executing uninterrupted.\\n\\n---\\n### 5. Reduce Thread Migration (CPU Affinity)\\n\\n**Why this reduces context switching**\\n\\nThread migration increases cache misses and scheduler activity, indirectly increasing preemption frequency. Stable affinity preserves cache locality and reduces scheduler intervention.\\n\\n---\\n### 6. Avoid Thread.Sleep, Yield, and Busy Waiting\\n\\n**Why this reduces context switching**\\n\\nSleep and Yield explicitly request a scheduler handoff, guaranteeing a context switch. Event-driven waiting eliminates unnecessary yield cycles.\\n\\n---\\n### 7. Batch Work Instead of Scheduling Per-Item\\n\\n**Why this reduces context switching**\\n\\nSmall tasks increase scheduling frequency and runnable queue churn. Batching increases useful work per scheduling decision.\\n\\n---\\n### 8. Use Work-Stealing Schedulers\\n\\n**Why this reduces context switching**\\n\\nWork-stealing minimizes idle threads and blocking, keeping runnable threads productive without increasing thread count.\\n\\n---\\n## Summary and Key Takeaways\\nReducing context switching improves performance by minimizing scheduler overhead and preserving cache locality. The optimization is most effective in CPU-bound systems where runnable thread count exceeds CPU cores and profiling confirms switching overhead. Prefer bounded concurrency, async I/O, batching, and careful synchronization. Always measure before optimizing.\\n\\n**Rule of thumb:**  \\nIf your runnable thread count is much higher than your CPU core count and the workload is CPU-bound, reducing context switching will likely improve performance.\\n\\n---\\n## Example in C#\\n\\n```csharp\\n//  Bad: Thread-per-work-item (inflates runnable queue)\\nfor (int i = 0; i < 1000; i++)\\n{\\n    new Thread(() => DoCpuWork()).Start();\\n}\\n\\n//  Good: Bounded parallelism using the ThreadPool\\nParallel.For(0, 1000, i => DoCpuWork());\\n\\n//  Better: Explicitly cap concurrency for CPU-bound work\\nvar options = new ParallelOptions\\n{\\n    MaxDegreeOfParallelism = Environment.ProcessorCount\\n};\\n\\nParallel.For(0, 1000, options, i => DoCpuWork());\\n\\n//  Best for I/O-bound work: async/await (no blocked threads)\\nawait Task.WhenAll(\\n    Enumerable.Range(0, 1000)\\n        .Select(i => DoIoWorkAsync())\\n);\"}]],\"markups\":[],\"sections\":[[10,0]]}',1766864014203,'2025-12-27 19:33:34'),('6951521f19c94fae00e3739f','6951521f19c94fae00e3739b','{\"version\":\"0.3.1\",\"atoms\":[],\"cards\":[[\"markdown\",{\"markdown\":\"# Pin Processes and Threads to Specific CPU Cores for Predictable Performance\\n\\n**Control CPU affinity to improve execution predictability and reduce unnecessary movement between CPU cores, at the cost of scheduler flexibility.**\\n\\n---\\n\\n## Executive Summary (TL;DR)\\n\\nCPU affinity (also called *CPU pinning*) means telling the operating system that a process or thread should run only on specific CPU cores. By default, the operating system is free to move threads between cores to balance load. While this is usually beneficial, it can hurt performance for latency-sensitive or cache-sensitive workloads.\\n\\nPinning execution to specific cores reduces unnecessary movement, improves execution continuity, and makes performance more predictable. Typical improvements range from **520%** for cache-sensitive workloads and can be much higher on multi-socket (NUMA) systems. The trade-off is reduced scheduler flexibility and potential load imbalance.\\n\\nUse CPU pinning when predictability matters more than maximum flexibility.\\n\\n---\\n\\n## A Simple Mental Model (Read This First)\\n\\nThink of a CPU core as a **desk**, and a thread as a **worker**.\\n\\n- When the worker stays at the same desk, their tools are already laid out.\\n- When the worker is moved to another desk, they must set everything up again.\\n- If the worker is moved frequently, more time is spent setting up than working.\\n\\nCPU pinning simply tells the operating system:  \\n**Stop moving this worker between desks.**\\n\\nYou dont need to know what the tools are internally.  \\nLess movement means less wasted time.\\n\\n---\\n\\n## Problem Context\\n\\nModern operating systems use schedulers designed to keep all CPU cores busy. To achieve this, they dynamically move processes and threads between cores to balance load, improve fairness, and optimize power usage.\\n\\nFor most applications, this behavior is ideal. However, for workloads that:\\n- require consistent timing\\n- repeatedly access the same data\\n- are sensitive to latency spikes\\n\\nfrequent core migration becomes expensive.\\n\\nEach migration interrupts execution and forces internal state to be rebuilt. Over time, this creates instability and unpredictable performance.\\n\\n---\\n\\n## What Is CPU Affinity?\\n\\nCPU affinity defines **which CPU cores a process or thread is allowed to run on**.\\n\\n- **Without affinity**: the OS may run your code on any core\\n- **With affinity**: the OS is restricted to a specific set of cores\\n\\nOnce affinity is set, the scheduler respects that constraint and avoids migrating execution outside the allowed cores.\\n\\n---\\n\\n## Default Thread Affinity and Why Threads Move Between Cores\\n\\nBy default, operating systems do **not** strictly bind threads to specific CPU cores.\\n\\nInstead, the scheduler uses a *soft affinity* strategy:\\n- It prefers to run a thread on the same core it ran on last\\n- But it is free to move the thread whenever it decides it is beneficial\\n\\nThis means:\\n- Threads can run on any core\\n- Core assignment is dynamic\\n- There is no guarantee that a thread will stay on the same core\\n\\n### Why Threads Are Migrated\\n\\nThreads move between cores for several reasons:\\n\\n- **Load balancing**  \\n  To prevent some cores from being overloaded while others are idle\\n\\n- **Fairness**  \\n  To ensure all runnable threads get CPU time\\n\\n- **Power and thermal management**  \\n  To spread heat and optimize energy usage\\n\\n- **Oversubscription**  \\n  When more threads are active than available cores\\n\\n- **System activity**  \\n  Interrupt handling, kernel threads, and background services\\n\\nFrom the schedulers perspective, migration is often the right choice.  \\nFrom the applications perspective, migration can introduce overhead and unpredictability.\\n\\nCPU pinning exists to override this behavior when predictability matters more than flexibility.\\n\\n---\\n\\n## Why Uncontrolled Core Migration Hurts Performance\\n\\n### 1. Execution Is Repeatedly Interrupted\\n\\nEach time a thread moves to another core, execution is paused and resumed elsewhere. Even though this is fast, doing it repeatedly adds noticeable overhead.\\n\\n---\\n\\n### 2. Internal Execution State Is Rebuilt\\n\\nWhile running, the CPU prepares internal state to execute code efficiently. When execution moves to another core, that preparation must be rebuilt, slowing progress.\\n\\nYou dont need hardware details to understand this:\\n**movement causes repetition**.\\n\\n---\\n\\n### 3. Latency Becomes Unpredictable\\n\\nSome executions run uninterrupted, others are moved several times. This creates variability and hurts worst-case latency.\\n\\n---\\n\\n### 4. Problems Amplify Under Load\\n\\nAs load increases:\\n- more threads compete for cores\\n- more migration occurs\\n- delays compound\\n\\nThis creates a feedback loop that rapidly degrades performance.\\n\\n---\\n\\n## Why Pinning Improves Many Things at Once\\n\\nCPU pinning improves performance not by optimizing one component, but by removing an entire class of inefficiency.\\n\\n- Less movement  less repeated setup\\n- Longer uninterrupted execution  more useful work\\n- Faster task completion  less system pressure\\n- More stable timing  better tail latency\\n\\nThis is why pinning often improves latency, throughput, and predictability simultaneously.\\n\\n---\\n\\n## How CPU Pinning Works (High-Level)\\n\\n### Hardware Perspective\\n- Each CPU core maintains its own execution context\\n- Staying on the same core preserves continuity\\n- Multi-socket (NUMA) systems amplify the cost of movement\\n\\n### Operating System Perspective\\n- The OS maintains an affinity mask for each process or thread\\n- The scheduler respects this mask\\n- Context switches still happen, but core migration is reduced or eliminated\\n\\n---\\n\\n## Advantages\\n\\n- More predictable performance\\n- Reduced execution interruption\\n- Improved behavior under load\\n- Strong benefits in NUMA systems\\n- Better isolation for critical workloads\\n\\n---\\n\\n## Disadvantages and Trade-offs\\n\\n- Reduced scheduler flexibility\\n- Risk of load imbalance\\n- Requires hardware awareness\\n- Can waste resources if misused\\n- Not suitable for dynamic workloads\\n\\nCPU pinning trades flexibility for predictability.\\n\\n---\\n\\n## When to Use CPU Pinning\\n\\n- Real-time or near-real-time systems\\n- Latency-critical services\\n- Multi-socket (NUMA) servers\\n- Cache-sensitive, predictable workloads\\n- Performance isolation requirements\\n\\n---\\n\\n## When Not to Use It\\n\\n- General-purpose web applications\\n- Highly dynamic or bursty workloads\\n- Cloud environments with abstracted CPUs\\n- Batch processing jobs\\n- Development and testing environments\\n\\n---\\n\\n## Common Mistakes\\n\\n- Pinning without profiling\\n- Pinning everything instead of critical parts\\n- Ignoring NUMA topology\\n- Confusing hyperthreads with physical cores\\n- Over-pinning and starving system processes\\n\\n---\\n\\n## How to Measure and Validate\\n\\nBefore pinning:\\n- Measure latency percentiles\\n- Measure throughput\\n- Observe CPU migrations\\n\\nAfter pinning:\\n- Verify migrations are reduced\\n- Check for load imbalance\\n- Validate latency improvement\\n- Monitor over time\\n\\nIf you cant measure the improvement, the pinning is not justified.\\n\\n---\\n\\n## C# Examples\\n\\n### Process Affinity (Most Common)\\n\\n```csharp\\nusing System;\\nusing System.Diagnostics;\\n\\nclass Program\\n{\\n    static void Main()\\n    {\\n        var process = Process.GetCurrentProcess();\\n\\n        // Pin process to CPU 0 and 1 (binary mask: 00000011)\\n        process.ProcessorAffinity = new IntPtr(0b00000011);\\n\\n        Console.WriteLine(\\\"Process pinned to CPU 0 and 1\\\");\\n    }\\n}\"}]],\"markups\":[],\"sections\":[[10,0]]}',1766937119207,'2025-12-28 15:51:59'),('69532d9aebcfaae683eec9a3','69532d9aebcfaae683eec996','{\"version\":\"0.3.1\",\"atoms\":[],\"cards\":[[\"markdown\",{\"markdown\":\"# Avoid False Sharing and Cache Line Contention\\n*(Clear, Practical, From Zero)*\\n\\nDesign memory layouts so that data written by different threads does **not** share cache lines.  \\nThis avoids unnecessary cache invalidations and allows real parallel execution.\\n\\n---\\n\\n## Executive Summary (TL;DR)\\n\\nFalse sharing happens when **multiple threads write to different variables that live inside the same CPU cache line** (usually 64 bytes).\\n\\nEven though the variables are logically independent, the CPU cache works with **cache lines**, not variables.  \\nWhen one thread writes, other cores must invalidate their copy of the entire cache line.  \\nThe cache line then **bounces between cores**, creating hidden serialization.\\n\\nThis causes:\\n- Poor scalability\\n- High CPU usage with low throughput\\n- Performance degradation (1050% or worse)\\n\\nFalse sharing **does NOT break correctness**, only performance  which makes it hard to notice.\\n\\n**Solution:** ensure that data frequently written by different threads lives in **different cache lines**, using:\\n- Per-thread data (preferred)\\n- Padding and alignment (when necessary)\\n\\nOnly apply after **profiling confirms cache contention**.\\n\\n---\\n\\n## 1. What Is a Cache Line? (From Zero)\\n\\nThe CPU does **not** load individual variables into cache.\\n\\nInstead, it loads memory in **fixed-size blocks** called **cache lines**.\\n\\n- Typical size: **64 bytes** (x86-64)\\n- Some ARM CPUs: **128 bytes**\\n- Minimum unit moved between:\\n  - RAM  cache\\n  - Core  core\\n\\n> You cannot partially load or partially invalidate a cache line.  \\n> It is always **all or nothing**.\\n\\n---\\n\\n## 2. Simple Analogy (Very Important)\\n\\nThink of memory like this:\\n\\n- **RAM** = a large filing cabinet\\n- **CPU cache** = your desk\\n- **Cache line** = a folder\\n\\nEven if you only need **one page**, you must bring **the entire folder** to your desk.\\n\\nThe CPU works exactly the same way.\\n\\n---\\n\\n## 3. How Variables End Up Sharing Cache Lines\\n\\n```csharp\\nclass Counters {\\n    public long A; // 8 bytes\\n    public long B; // 8 bytes\\n}\\n```\\n\\n**Memory layout (simplified):**\\n\\n```\\n| A (8B) | B (8B) | other data ... |\\n|<---------- 64 bytes (one cache line) ---------->|\\n```\\n\\nEven though A and B are:\\n- Different variables\\n- Used by different threads\\n\\nThey live in the same cache line.\\n\\n---\\n\\n## 4. The Golden Rule (Explains Everything)\\n\\n**If two threads write to anything inside the same cache line, they compete.**\\n\\nIt does **NOT** matter:\\n- That variables are different\\n- That there are no locks\\n- That the code is correct\\n\\nThis is false sharing.\\n\\n---\\n\\n## 5. Why the CPU Forces This (Cache Coherency)\\n\\nEach CPU core has its own cache.\\n\\nThe CPU must guarantee:\\n\\n> \\\"All cores see a consistent view of memory.\\\"\\n\\nTo do this, it uses cache coherency protocols (e.g., MESI).\\n\\n**Core rule:**\\n- Only one core can modify a cache line at a time\\n- Other cores must invalidate their copy before writing\\n\\nCorrect behavior  but expensive.\\n\\n---\\n\\n## 6. False Sharing Explained as a Timeline (Cronograma)\\n\\n### Scenario Setup\\n\\n- **Core 0**  Thread 0  writes to variable `A`\\n- **Core 1**  Thread 1  writes to variable `B`\\n- `A` and `B` share the same cache line\\n\\n### Why Different Cores Have the Same Cache Line\\n\\nWhen Thread 0 on Core 0 first reads `A`, the CPU loads the entire 64-byte cache line containing `A` (and `B`) into Core 0\'s L1 cache.\\n\\nLater, when Thread 1 on Core 1 reads `B`, the CPU loads the same cache line (now containing both `A` and `B`) into Core 1\'s L1 cache.\\n\\n**Both cores now have identical copies of the same cache line in their local caches.**\\n\\n### Detailed Timeline\\n\\n```\\nTime    Core 0 (Thread 0)              Core 1 (Thread 1)              Cache Line State\\n\\nT0      Reads A                        -                               Core 0: Exclusive (E)\\n                                                                        Cache line loaded from RAM\\n                                                                        Cost: ~100-300 cycles\\n\\nT1      -                              Reads B                         Core 0: Shared (S)\\n                                                                        Core 1: Shared (S)\\n                                                                        Both have read-only copies\\n                                                                        Cost: ~40-100 cycles (transfer from Core 0)\\n\\nT2      Writes to A                    -                               Core 0: Modified (M)\\n                                                                        Core 1: Invalid (I)  INVALIDATION!\\n                                                                        \\n                                                                        Core 0 sends \\\"invalidate\\\" message to Core 1\\n                                                                        Cost: ~10-20 cycles (inter-core communication)\\n\\nT3      (continues working)            Wants to write B                Core 1 detects cache line is Invalid\\n                                                                        Core 1 must request cache line from Core 0\\n                                                                        \\n                                                                        Request sent to Core 0: \\\"I need this cache line\\\"\\n                                                                        Cost: ~10-20 cycles (request)\\n\\nT4      Receives request               (waiting...)                    Core 0 must write back to memory (if Modified)\\n                                                                        Core 0 sends cache line to Core 1\\n                                                                        Cost: ~40-100 cycles (write-back + transfer)\\n\\nT5      -                              Receives cache line             Core 0: Shared (S)\\n                                                                        Core 1: Modified (M)\\n                                                                        Now Core 1 has exclusive ownership\\n                                                                        Cost: ~10 cycles (state update)\\n\\nT6      (wants A again)                Writes to B                     Core 1: Modified (M)\\n                                                                        Core 0: Invalid (I)  INVALIDATION AGAIN!\\n                                                                        \\n                                                                        Process repeats...\\n                                                                        Cost: ~40-100 cycles per cycle\\n```\\n\\n### Who Controls the Cache Line Updates?\\n\\n**The CPU\'s cache coherency protocol (MESI) controls everything automatically:**\\n\\n1. **Hardware-level**: No software involvement requiredit happens in CPU hardware\\n2. **Cache controller**: Each core has a cache controller that manages MESI states\\n3. **Interconnect**: Cores communicate through the CPU interconnect (bus or mesh)\\n4. **Snooping**: Cores \\\"snoop\\\" on each other\'s cache transactions to maintain coherency\\n\\n### Cost Breakdown\\n\\n**Per false sharing cycle:**\\n- Invalidation message: ~10-20 cycles\\n- Cache line request: ~10-20 cycles  \\n- Write-back to memory (if needed): ~10-30 cycles\\n- Cache line transfer: ~40-100 cycles\\n- State updates: ~5-10 cycles\\n\\n**Total per cycle: ~75-180 CPU cycles**\\n\\nIf Thread 0 writes to `A` 1 million times per second, and Thread 1 writes to `B` 1 million times per second:\\n- **Potential cache line transfers: 2 million per second**\\n- **Wasted cycles: ~150-360 million cycles per second**\\n- **On a 3GHz CPU: 5-12% of total CPU time wasted on false sharing overhead**\\n\\n### Why This Creates Serialization\\n\\nEven though Thread 0 and Thread 1 are running on different cores (true parallelism), they **cannot write simultaneously** because:\\n\\n1. Only one core can have the cache line in Modified state\\n2. The other core must wait for the transfer to complete\\n3. This creates **implicit serialization** at the hardware level\\n\\n**Result**: What looks like parallel execution is actually serial execution with expensive synchronization.\\n\\n### Visual Representation\\n\\n```\\nNormal Parallel Execution (no false sharing):\\n\\nCore 0: [Write A][Write A][Write A][Write A]...   Continuous\\nCore 1: [Write B][Write B][Write B][Write B]...   Continuous\\n         Both working simultaneously\\n\\nWith False Sharing:\\n\\nCore 0: [Write A][----WAIT----][Write A][----WAIT----]...\\nCore 1: [----WAIT----][Write B][----WAIT----][Write B]...\\n         Taking turns (serialized!)\\n         Wasted cycles during WAIT\\n```\\n\\nThis hidden serialization is why performance degrades even though your code looks perfectly parallel.\\n\\n---\\n\\n### Common Misconceptions\\n\\n**\\\"Separate variables mean separate memory locations\\\"**\\n- The CPU caches data in 64-byte chunks called cache lines. Two variables declared separately can end up on the same cache line if they\'re close in memory. Think of it like apartment buildings: even though you live in apartment 101 and your neighbor in 102, you share the same building (cache line).\\n\\n**\\\"Lock-free code is automatically fast\\\"**\\n- Lock-free data structures avoid blocking but can suffer from false sharing when multiple threads update adjacent fields. Atomic operations still trigger cache invalidations.\\n\\n**\\\"High CPU usage means good parallelization\\\"**\\n- False sharing can cause high CPU usage while destroying actual parallelism. CPUs spend time waiting for cache lines to transfer between cores, not doing useful work.\\n\\n**\\\"The compiler/runtime will optimize this away\\\"**\\n- Compilers don\'t automatically pad structures to prevent false sharing. They optimize for single-threaded performance, not multi-threaded cache behavior.\\n\\n### Why Naive Solutions Fail\\n\\n**Adding locks**: Makes it worse by serializing execution completely. The problem is cache contention, not lack of synchronization.\\n\\n**Increasing thread count**: More threads mean more cache invalidations (O(n) growth). The problem compounds.\\n\\n**Logical separation**: Different variables can still share cache lines if they\'re physically close in memory. CPU sees physical layout, not code structure.\\n\\n**Algorithm optimization alone**: Doesn\'t help if threads are fighting over cache lines at the hardware level.\\n\\n## Why This Becomes a Bottleneck\\n\\n**Cache Line Ping-Pong**: Cache lines constantly bounce between cores. Each transfer consumes bandwidth and creates latency. Impact: 10-50% performance degradation.\\n\\n**Serialization Despite Parallelism**: Threads appear to run in parallel but wait for each other at the cache level. Impact: Applications that should scale linearly instead plateau or degrade.\\n\\n**Memory Bus Saturation**: Cache line transfers consume bandwidth, competing with actual data access. Impact: System-wide performance degradation.\\n\\n**NUMA Amplification**: In NUMA systems, remote cache transfers are 2-3x slower. Impact: 50-100% additional latency.\\n\\n**Scalability Collapse**: Performance degrades with more threads instead of improving. Impact: Applications fail to utilize available CPU cores.\\n\\n## When to Use This Approach\\n\\n**High-performance multi-threaded applications**: Many threads, high throughput requirements, processing large volumes of data.\\n\\n**Frequently-updated shared state**: Per-thread counters, statistics structures, lock-free data structures with adjacent fields.\\n\\n**Profiling indicates cache contention**: High cache miss rates, poor scalability, cache line transfers detected by tools.\\n\\n**High-thread-count systems**: 8+ cores where false sharing effects are amplified. Especially important in NUMA systems.\\n\\n**Latency-sensitive parallel workloads**: Real-time systems, financial trading, game engines, media processing.\\n\\n**Lock-free algorithms**: Lock-free queues, stacks, hash tables. These are particularly susceptible to false sharing.\\n\\n## When Not to Use It\\n\\n**Single-threaded applications**: No parallelism means no false sharing.\\n\\n**Read-only shared data**: False sharing only occurs with writes. Multiple threads reading is fine (Shared state).\\n\\n**Infrequently accessed data**: Padding overhead isn\'t justified by occasional access.\\n\\n**Small data structures**: Structures that naturally span multiple cache lines might not need explicit padding.\\n\\n**Cloud/containerized environments**: Hardware topology is abstracted, cache line sizes may vary.\\n\\n**Development/prototyping**: Premature optimization distracts from correctness. Profile first.\\n\\n**Memory-constrained systems**: Embedded systems, mobile devices. Padding overhead might be unacceptable.\\n\\n**When profiling shows no issue**: Don\'t optimize what isn\'t broken. Use tools to confirm before adding padding.\\n\\n---\\n\\n## How to Avoid False Sharing (General Principles)\\n\\n### Strategy 1: Per-Thread Data (Preferred)\\n\\n**Best approach**: Give each thread its own copy of data. No sharing = no false sharing.\\n\\n**When to use**: \\n- Per-thread counters, statistics, or accumulators\\n- Thread-local state that\'s aggregated later\\n\\n**Benefits**:\\n- No padding overhead\\n- No false sharing (each thread has separate memory)\\n- Cleaner, simpler code\\n\\n**Trade-off**: Must aggregate results when needed (but this is usually infrequent).\\n\\n**Example pattern**:\\n- Each thread maintains its own counter/state\\n- Periodically (or at end), aggregate across all threads\\n- Much cheaper than constant cache line contention\\n\\n### Strategy 2: Padding and Alignment\\n\\n**When per-thread data isn\'t feasible**: Use padding to separate shared data into different cache lines.\\n\\n**Key principles**:\\n- Ensure frequently-written variables start at cache line boundaries\\n- Pad each variable to at least cache line size (64 or 128 bytes)\\n- Use compiler directives to enforce alignment\\n\\n**Benefits**:\\n- Works when data must be shared\\n- Predictable memory layout\\n\\n**Trade-offs**:\\n- Increased memory usage (4-8x)\\n- More complex code\\n- Platform-specific (cache line sizes vary)\\n\\n### Strategy 3: Separate Data Structures\\n\\n**Design approach**: Design data structures so hot fields written by different threads are naturally separated.\\n\\n**Principles**:\\n- Place head/tail pointers in separate cache lines\\n- Separate producer/consumer fields\\n- Group data by access pattern (hot vs. cold)\\n\\n**Benefits**:\\n- Natural separation, less artificial padding\\n- Better overall data structure design\\n\\n### Strategy 4: Reduce Write Frequency\\n\\n**Optimization**: Reduce how often threads write to shared data.\\n\\n**Techniques**:\\n- Batch updates (write every N operations instead of every operation)\\n- Use local accumulators, then periodically update shared state\\n- Prefer read-heavy patterns\\n\\n**Benefits**:\\n- Less false sharing even if data shares cache lines\\n- Better cache efficiency overall\\n\\n**When to use**: When you can\'t avoid sharing but can reduce write frequency.\\n\\n### Strategy 5: Cache Line Size Awareness\\n\\n**Know your platform**:\\n- x86-64: 64 bytes\\n- Some ARM: 128 bytes\\n- Test on target hardware\\n\\n**Implementation**:\\n- Use constants for cache line size\\n- Consider padding to 128 bytes for cross-platform safety\\n- Document assumptions\\n\\n### General Checklist\\n\\n1. **Profile first**: Use `perf c2c` or VTune to identify false sharing\\n2. **Choose strategy**: Prefer per-thread data when possible\\n3. **Measure impact**: Verify improvements after changes\\n4. **Document decisions**: Explain why padding/alignment exists\\n5. **Test on target**: Different platforms have different cache line sizes\\n\\n---\\n\\n## How to Avoid False Sharing in C#\\n\\n### Method 1: ThreadLocal<T> (Best for Per-Thread Data)\\n\\n**Use when**: Each thread needs its own accumulator, counter, or state.\\n\\n```csharp\\nusing System.Threading;\\n\\npublic class RequestCounter {\\n    // Each thread gets its own counter - no sharing!\\n    private readonly ThreadLocal<long> _counter = new ThreadLocal<long>(() => 0);\\n    \\n    public void Increment() {\\n        _counter.Value++;  // Thread-local, no false sharing\\n    }\\n    \\n    public long GetTotal() {\\n        // Aggregate across all threads when needed\\n        long total = 0;\\n        // Note: ThreadLocal doesn\'t provide easy enumeration\\n        // You might need to track threads manually or use a different approach\\n        return total;\\n    }\\n}\\n\\n// Better: Use ThreadLocal with explicit thread tracking\\npublic class ThreadSafeCounter {\\n    private readonly ThreadLocal<long> _counter = new ThreadLocal<long>(() => 0);\\n    private readonly ConcurrentDictionary<int, long> _threadCounters = new();\\n    \\n    public void Increment() {\\n        _counter.Value++;\\n        _threadCounters[Thread.CurrentThread.ManagedThreadId] = _counter.Value;\\n    }\\n    \\n    public long GetTotal() {\\n        return _threadCounters.Values.Sum();\\n    }\\n}\\n```\\n\\n**Why it works**: Each thread accesses completely separate memory locations. No cache line sharing possible.\\n\\n**When to use**: Counters, statistics, accumulators that need per-thread isolation.\\n\\n### Method 2: StructLayout with Padding (For Shared Data)\\n\\n**Use when**: Data must be shared but needs cache line separation.\\n\\n```csharp\\nusing System.Runtime.InteropServices;\\n\\n// Option 1: Explicit size with padding\\n[StructLayout(LayoutKind.Explicit, Size = 128)]  // Pad to 128 bytes (safe for 64 and 128-byte cache lines)\\npublic struct PaddedCounter {\\n    [FieldOffset(0)]\\n    public long Value;\\n    // Rest is automatic padding to 128 bytes\\n}\\n\\npublic class ThreadSafeCounters {\\n    private readonly PaddedCounter[] _counters;\\n    \\n    public ThreadSafeCounters(int threadCount) {\\n        _counters = new PaddedCounter[threadCount];\\n    }\\n    \\n    public void Increment(int threadId) {\\n        Interlocked.Increment(ref _counters[threadId].Value);\\n    }\\n}\\n\\n// Option 2: Manual padding fields\\npublic class ManualPaddedCounter {\\n    private long _counter;\\n    \\n    // Pad to ensure next instance starts at new cache line\\n    // Cache line is 64 bytes, long is 8 bytes\\n    // Need 7 more longs (56 bytes) to reach 64 bytes total\\n    private long _padding1, _padding2, _padding3, _padding4,\\n                 _padding5, _padding6, _padding7;\\n    \\n    public long Value {\\n        get => _counter;\\n        set => _counter = value;\\n    }\\n}\\n```\\n\\n**Why it works**: Forces each `PaddedCounter` to occupy a full cache line (128 bytes), ensuring separate cache lines.\\n\\n**When to use**: Arrays of per-thread data that must be indexed by thread ID.\\n\\n### Method 3: Separate Cache Lines for Lock-Free Structures\\n\\n**Use when**: Building lock-free queues, stacks, or other concurrent structures.\\n\\n```csharp\\nusing System.Runtime.InteropServices;\\n\\n[StructLayout(LayoutKind.Explicit, Size = 128)]\\npublic class LockFreeQueue<T> where T : class {\\n    private readonly T[] _buffer;\\n    private readonly int _capacity;\\n    \\n    [FieldOffset(0)]\\n    private volatile int _head;  // Consumer writes here - first cache line\\n    \\n    // Padding to next cache line (64 bytes)\\n    [FieldOffset(64)]\\n    private volatile int _tail;  // Producer writes here - second cache line\\n    \\n    public LockFreeQueue(int capacity) {\\n        _capacity = capacity;\\n        _buffer = new T[capacity];\\n        _head = 0;\\n        _tail = 0;\\n    }\\n    \\n    public bool TryEnqueue(T item) {\\n        int currentTail = _tail;\\n        int nextTail = (currentTail + 1) % _capacity;\\n        \\n        if (nextTail == _head) {\\n            return false; // Queue full\\n        }\\n        \\n        _buffer[currentTail] = item;\\n        _tail = nextTail;  // Producer writes to separate cache line\\n        return true;\\n    }\\n    \\n    public bool TryDequeue(out T item) {\\n        int currentHead = _head;\\n        \\n        if (currentHead == _tail) {\\n            item = default(T);\\n            return false; // Queue empty\\n        }\\n        \\n        item = _buffer[currentHead];\\n        _buffer[currentHead] = null;\\n        _head = (currentHead + 1) % _capacity;  // Consumer writes to separate cache line\\n        return true;\\n    }\\n}\\n```\\n\\n**Why it works**: Producer (`_tail`) and consumer (`_head`) write to different cache lines, eliminating contention.\\n\\n### Method 4: Separate Arrays for Different Threads\\n\\n**Use when**: You need indexed access but can separate by thread.\\n\\n```csharp\\n//  Bad: All counters in one array - false sharing\\npublic class BadCounters {\\n    private readonly long[] _counters = new long[Environment.ProcessorCount];\\n    \\n    public void Increment(int threadId) {\\n        _counters[threadId]++;  // False sharing if elements share cache lines\\n    }\\n}\\n\\n//  Good: Use padded structures\\npublic class GoodCounters {\\n    private readonly PaddedCounter[] _counters;\\n    \\n    public GoodCounters() {\\n        int threadCount = Environment.ProcessorCount;\\n        _counters = new PaddedCounter[threadCount];\\n    }\\n    \\n    public void Increment(int threadId) {\\n        Interlocked.Increment(ref _counters[threadId].Value);\\n    }\\n}\\n\\n// Using the PaddedCounter struct from Method 2\\n[StructLayout(LayoutKind.Explicit, Size = 128)]\\npublic struct PaddedCounter {\\n    [FieldOffset(0)]\\n    public long Value;\\n}\\n```\\n\\n### Method 5: Reduce Write Frequency (Batching)\\n\\n**Use when**: You can\'t avoid sharing but can reduce write frequency.\\n\\n```csharp\\npublic class BatchedCounter {\\n    // Thread-local accumulator\\n    private readonly ThreadLocal<long> _localCounter = new ThreadLocal<long>(() => 0);\\n    \\n    // Shared counter, updated less frequently\\n    private long _sharedCounter;\\n    private readonly object _lock = new object();\\n    private const int BATCH_SIZE = 1000;\\n    \\n    public void Increment() {\\n        _localCounter.Value++;\\n        \\n        // Only update shared counter every BATCH_SIZE increments\\n        if (_localCounter.Value % BATCH_SIZE == 0) {\\n            lock (_lock) {\\n                _sharedCounter += BATCH_SIZE;\\n            }\\n        }\\n    }\\n    \\n    public long GetTotal() {\\n        long total = _sharedCounter;\\n        // Add any remaining in thread-local counters\\n        // (simplified - in practice, you\'d need to track all threads)\\n        return total;\\n    }\\n}\\n```\\n\\n**Why it works**: Reduces writes to shared data by 1000x (from every increment to every 1000 increments).\\n\\n### Method 6: Using Memory-Mapped or Aligned Allocation (Advanced)\\n\\n**Use when**: You need precise control over memory layout.\\n\\n```csharp\\nusing System.Runtime.InteropServices;\\n\\n// Note: This requires unsafe code and platform-specific implementation\\npublic unsafe class AlignedCounter {\\n    private long* _counter;\\n    \\n    public AlignedCounter() {\\n        // Allocate aligned to cache line boundary (64 bytes)\\n        // This is platform-specific and requires P/Invoke or native allocation\\n        _counter = (long*)AlignedAlloc(64, sizeof(long));\\n    }\\n    \\n    private void* AlignedAlloc(ulong alignment, ulong size) {\\n        // Platform-specific implementation needed:\\n        // - Windows: _aligned_malloc\\n        // - Linux: posix_memalign or aligned_alloc\\n        // - Use DllImport or NativeMemory.AlignedAlloc (modern .NET)\\n        throw new NotImplementedException(\\\"Platform-specific implementation\\\");\\n    }\\n    \\n    // Modern .NET alternative (if available)\\n    public void ModernApproach() {\\n        // .NET 6+ has NativeMemory.AlignedAlloc\\n        // IntPtr ptr = NativeMemory.AlignedAlloc((nuint)sizeof(long), 64);\\n    }\\n}\\n```\\n\\n**Why it works**: Ensures memory starts exactly at a cache line boundary.\\n\\n**When to use**: When you need guaranteed alignment and can\'t use `StructLayout`.\\n\\n### C# Best Practices Summary\\n\\n1. **Prefer ThreadLocal<T>**: Simplest and most effective for per-thread data\\n2. **Use StructLayout for arrays**: When you need indexed access to per-thread data\\n3. **Separate producer/consumer fields**: For lock-free structures, ensure 64+ bytes separation\\n4. **Batch updates**: Reduce write frequency when you can\'t avoid sharing\\n5. **Use constants**: Define `CACHE_LINE_SIZE = 64` or `128` as a constant\\n6. **Verify with tools**: Use profiling tools to confirm false sharing is fixed\\n7. **Document**: Add comments explaining why padding exists\\n\\n### Common C# Pitfalls\\n\\n**Pitfall 1: Assuming array elements are separate**\\n```csharp\\n//  Bad: Array of longs - elements might share cache lines\\nlong[] counters = new long[8];  // 8 * 8 = 64 bytes - all in one cache line!\\n\\n//  Good: Use padded structures\\nPaddedCounter[] counters = new PaddedCounter[8];  // Each is 128 bytes - separate cache lines\\n```\\n\\n**Pitfall 2: Not using StructLayout**\\n```csharp\\n//  Bad: Compiler might reorder fields\\npublic struct Counter {\\n    public long Value;\\n    public long Padding1, Padding2, ...;  // Might not work!\\n}\\n\\n//  Good: Explicit layout\\n[StructLayout(LayoutKind.Explicit, Size = 128)]\\npublic struct Counter {\\n    [FieldOffset(0)]\\n    public long Value;\\n}\\n```\\n\\n**Pitfall 3: Forgetting about object headers**\\n```csharp\\n// In C#, objects have headers (overhead)\\n// PaddedCounter struct is fine, but arrays of objects might have additional overhead\\n// Prefer structs over classes for per-thread data\\n```\\n\\n### Performance Impact in C#\\n\\nTypical improvements when fixing false sharing in C#:\\n- **Per-thread counters**: 30-50% throughput improvement\\n- **Lock-free queues**: 40-60% latency reduction\\n- **Thread pool statistics**: 25-40% overhead reduction\\n- **Scalability**: Can often restore linear scaling up to 16-32 threads\\n\\n---\"}]],\"markups\":[],\"sections\":[[10,0]]}',1767058842867,'2025-12-30 01:40:42'),('6954500debcfaae683eec9bd','6954500debcfaae683eec9b2','{\"version\":\"0.3.1\",\"atoms\":[],\"cards\":[[\"markdown\",{\"markdown\":\"# Optimize Branch Prediction for Better CPU Pipeline Utilization\\n\\n**Write code with predictable control flow to minimize branch mispredictions and maximize CPU pipeline efficiency.**\\n\\n---\\n\\n## Executive Summary (TL;DR)\\n\\nBranch prediction is a CPU optimization technique that guesses which path code will take at conditional statements (if/else, loops, switches) before the condition is actually evaluated. When predictions are correct, the CPU pipeline continues smoothly. When predictions are wrong, the CPU must discard speculative work and restart, costing 10-20 CPU cycles per misprediction. To optimize for branch prediction, write code with predictable patterns: move common cases first, use branchless operations where possible, and separate unpredictable branches. This typically improves performance by 5-30% in code-heavy with conditionals. The trade-off is that it may reduce code readability and requires profiling to identify problematic branches. Apply this optimization primarily to hot paths in performance-critical code after profiling confirms branch mispredictions.\\n\\n---\\n\\n## Problem Context\\n\\nModern CPUs can execute multiple instructions simultaneously through **pipelining**think of it like an assembly line where different stages process different instructions at the same time. However, conditional branches (if/else, loops, switches) create a problem: the CPU doesn\'t know which path to take until it evaluates the condition, but it needs to know *now* to keep the pipeline full.\\n\\n**What is pipelining?** Imagine a factory assembly line. Instead of building one car completely before starting the next, you have stages: frame, engine, wheels, paint. While one car is being painted, the next is getting wheels, and the one after that is getting an engine. Similarly, a CPU pipeline has stages like: fetch instruction, decode, execute, write result. Modern CPUs have 10-20 pipeline stages, and when full, they can process multiple instructions simultaneously.\\n\\n**What is a branch?** Any point in code where execution can take different paths: if/else statements, loops (should we continue?), switch statements, function calls, etc.\\n\\n**The problem**: When the CPU encounters a branch, it must wait for the condition to be evaluated before knowing which instructions to load next. This **stalls the pipeline**, wasting cycles. To avoid this, CPUs use **branch prediction**they guess which path will be taken based on historical patterns.\\n\\n### Common Misconceptions\\n\\n**\\\"Branch prediction is something I control\\\"**\\n- Branch prediction happens automatically in CPU hardware. You don\'t explicitly control it, but your code patterns influence how well it works.\\n\\n**\\\"All branches are equally expensive\\\"**\\n- Branches that are predictable (always true, always false, or follow patterns) have near-zero cost. Unpredictable branches cost 10-20 cycles per misprediction.\\n\\n**\\\"Modern CPUs are so fast, branches don\'t matter\\\"**\\n- Modern CPUs are fast *because* of optimizations like branch prediction. When prediction fails, you still pay the penalty. In tight loops with branches, this adds up quickly.\\n\\n**\\\"I should eliminate all branches\\\"**\\n- Not necessary. Eliminate or optimize *unpredictable* branches in hot paths. Predictable branches have minimal cost.\\n\\n### Why Naive Solutions Fail\\n\\n**Adding more branches to \\\"clarify\\\" logic**\\n- More branches mean more prediction opportunities. If they\'re unpredictable, performance gets worse.\\n\\n**Complex nested conditionals**\\n- Nested branches compound the problem. If the outer branch is mispredicted, inner branches may be evaluated speculatively (wrong path), wasting more cycles.\\n\\n**Assuming the compiler optimizes everything**\\n- Compilers do optimize, but they can\'t fix fundamentally unpredictable control flow patterns. The CPU\'s hardware predictor works with patterns, not logic.\\n\\n**Ignoring profiling data**\\n- Without profiling, you might optimize the wrong branches or miss the ones causing real performance problems.\\n\\n---\\n\\n## How It Works\\n\\n### Understanding CPU Pipelines\\n\\n**What is a CPU pipeline?** Modern CPUs break instruction execution into stages. While one instruction is being executed, the next is being decoded, and the one after that is being fetched from memory. This parallelism allows CPUs to process multiple instructions simultaneously.\\n\\n**Pipeline stages** (simplified):\\n1. **Fetch**: Load instruction from memory\\n2. **Decode**: Determine what the instruction does\\n3. **Execute**: Perform the operation\\n4. **Memory**: Access data memory (if needed)\\n5. **Write-back**: Store result\\n\\nWith a 5-stage pipeline, theoretically 5 instructions can be in flight at once. Modern CPUs have 10-20 stages.\\n\\n**The pipeline problem**: To keep the pipeline full, the CPU must fetch the next instruction *before* the current one finishes. But with branches, the CPU doesn\'t know which instruction comes next until the branch condition is evaluated. This creates a **hazard**a situation that prevents the pipeline from proceeding.\\n\\n**What is a hazard?** A situation where the pipeline cannot continue because it lacks necessary information. Branch hazards occur because we don\'t know which instruction to fetch next until the branch is resolved.\\n\\n### Branch Prediction Explained\\n\\n**What is branch prediction?** The CPU guesses which path a branch will take based on:\\n- **Static prediction**: Simple heuristics (e.g., forward branches are usually not taken, backward branches usually are)\\n- **Dynamic prediction**: Historical patterns (e.g., this branch was taken 90% of the time recently, so predict \\\"taken\\\")\\n- **Branch target prediction**: Predicting the target address for indirect branches\\n\\n**Modern branch predictors** use sophisticated algorithms:\\n- **Pattern history tables**: Track taken/not-taken patterns\\n- **Branch target buffers**: Cache target addresses\\n- **Global vs. local history**: Consider recent branches globally or per-branch\\n\\n**What happens when prediction is correct?**\\n- Pipeline continues smoothly\\n- Next instructions are already being processed\\n- Near-zero penalty (maybe 1 cycle for prediction logic)\\n\\n**What happens when prediction is wrong?**\\n- CPU must **flush the pipeline**discard all speculatively executed instructions\\n- Restart from the correct path\\n- **Branch misprediction penalty**: 10-20 cycles on modern CPUs\\n- All the work done speculatively is wasted\\n\\n**Why is the penalty so high?** The pipeline is deep (10-20 stages). When a misprediction is discovered, all instructions in the wrong path must be discarded, and the pipeline must restart. The deeper the pipeline, the higher the cost.\\n\\n### Real-World Example: Loop with Condition\\n\\n```csharp\\n// Loop that processes items, conditionally incrementing a counter\\nint count = 0;\\nforeach (var item in items) {\\n    if (item.Value > threshold) {  // Branch inside hot loop\\n        count++;\\n    }\\n}\\n```\\n\\n**What the CPU sees**:\\n- For each iteration, must predict: will `item.Value > threshold` be true or false?\\n- If items are mostly above threshold, predictor learns \\\"taken\\\"\\n- If items are mostly below, predictor learns \\\"not taken\\\"\\n- If it\'s random, predictor fails frequently  many mispredictions\\n\\n**Cost calculation**:\\n- 1,000,000 items\\n- If branch is 50/50 unpredictable: ~500,000 mispredictions\\n- 500,000  15 cycles = 7,500,000 wasted cycles\\n- On a 3GHz CPU: 2.5ms wasted (significant in a tight loop!)\\n\\n---\\n\\n## Why This Becomes a Bottleneck\\n\\n### Pipeline Stalls\\n\\n**What happens**: When a branch is mispredicted, the pipeline must flush and restart. During this time, no useful work is done.\\n\\n**Impact**: In tight loops with unpredictable branches, you might spend 10-20% of CPU time on misprediction penalties instead of actual computation.\\n\\n**Example**: A loop that processes 1 million items with a 50/50 unpredictable branch might waste 2-3ms just on branch mispredictions. In a function that should complete in 10ms, this is a 20-30% overhead.\\n\\n### Speculative Execution Waste\\n\\n**What is speculative execution?** When the CPU predicts a branch will be taken, it speculatively executes instructions from that path before confirming the prediction.\\n\\n**The waste**: If prediction is wrong, all speculatively executed instructions are discarded:\\n- Decoded instructions: wasted\\n- Executed operations: wasted (unless side-effect free)\\n- Cache loads: might still help (prefetched data)\\n- Memory bandwidth: partially wasted\\n\\n**Impact**: Not just the misprediction penalty, but also wasted work and resources.\\n\\n### Compounding Effects\\n\\n**Multiple branches in sequence**:\\n- If Branch A is mispredicted, Branch B might be evaluated on the wrong path\\n- When the pipeline corrects, Branch B must be re-evaluated\\n- Cascading waste from multiple mispredictions\\n\\n**Nested branches**:\\n- Outer branch misprediction causes inner branches to be evaluated speculatively on wrong path\\n- More instructions wasted, larger penalty\\n\\n**Impact**: Complex control flow with unpredictable branches can amplify the problem.\\n\\n### Cache and Memory Effects\\n\\n**Instruction cache misses**: When a branch is mispredicted, the CPU might load instructions from the wrong path into the instruction cache. When it corrects, it must load the correct instructions, potentially causing cache misses.\\n\\n**Data prefetching**: Speculative execution might prefetch data from the wrong path, wasting memory bandwidth.\\n\\n**Impact**: Additional penalties beyond the direct misprediction cost.\\n\\n---\\n\\n## When to Use This Approach\\n\\n**Hot paths in performance-critical code**: Code that executes frequently (millions of times per second) and is on the critical path for latency or throughput.\\n\\n**Loops with conditions**: Especially tight loops with branches inside. The branch is evaluated many times, so mispredictions compound.\\n\\n**After profiling confirms branch mispredictions**: Use profiling tools (perf, VTune) to identify branches with high misprediction rates. Optimize those, not all branches.\\n\\n**Code with predictable patterns**: When you can make branches more predictable (e.g., process common cases first, sort data to make comparisons predictable).\\n\\n**Latency-sensitive applications**: Where consistent, low latency matters (game engines, trading systems, real-time systems).\\n\\n**High-throughput processing**: Where processing speed directly impacts business metrics (data processing, request handling).\\n\\n**Why these scenarios**: Branch optimization only matters when branches are frequently executed. In cold code, the optimization cost (readability, maintenance) isn\'t worth it.\\n\\n---\\n\\n## When Not to Use It\\n\\n**Cold code**: Code that executes rarely. The optimization cost (readability) isn\'t worth the negligible performance benefit.\\n\\n**Already predictable branches**: If profiling shows branches are already well-predicted (low misprediction rate), optimization won\'t help.\\n\\n**One-time initialization**: Code that runs once at startup. Branch mispredictions here don\'t matter.\\n\\n**Readability is more important**: When code clarity and maintainability are priorities over micro-optimizations.\\n\\n**Compiler already optimizes it**: Modern compilers do branch optimization. Manual optimization might be redundant or conflict with compiler decisions.\\n\\n**No profiling data**: Don\'t optimize branches without data showing they\'re a problem. You might optimize the wrong thing.\\n\\n**Complex optimization for minimal gain**: If the optimization makes code much more complex for a 1-2% gain, it\'s probably not worth it.\\n\\n**Why avoid these**: Branch optimization has costs (readability, maintenance). Only apply when benefits clearly outweigh costs.\\n\\n---\\n\\n## How to Measure and Validate\\n\\n### Profiling Tools\\n\\n**perf (Linux)**:\\n```bash\\n# Measure branch mispredictions\\nperf stat -e branches,branch-misses ./your_application\\n\\n# Detailed branch analysis\\nperf record -e branch-misses ./your_application\\nperf report\\n```\\n\\n**Intel VTune**: \\n- \\\"Microarchitecture Exploration\\\" analysis\\n- Shows branch misprediction hotspots\\n- Visual representation of branch efficiency\\n\\n**Visual Studio Profiler (Windows)**:\\n- \\\"CPU Usage\\\" profiling\\n- Can show branch misprediction events\\n- Timeline view of performance issues\\n\\n### Key Metrics\\n\\n**Branch misprediction rate**: \\n- Formula: `(branch-misses / branches)  100%`\\n- Target: < 5% for hot code\\n- Action: If > 10%, investigate and optimize\\n\\n**Cycles lost to mispredictions**:\\n- Calculate: `branch-misses  misprediction-penalty`\\n- Compare to total cycles to see impact\\n\\n**Instruction-per-cycle (IPC)**:\\n- Higher is better (more work per cycle)\\n- Branch mispredictions reduce IPC\\n- Monitor IPC before/after optimization\\n\\n### Detection Strategies\\n\\n1. **Profile your hot paths**: Use profiling tools on code that executes frequently\\n2. **Look for high misprediction rates**: Branches with >10% misprediction rate are candidates\\n3. **Identify unpredictable patterns**: Look for branches that alternate unpredictably\\n4. **Measure before/after**: Profile, optimize, profile again to verify improvement\\n\\n---\\n\\n## Optimization Techniques\\n\\n### Technique 1: Common Case First\\n\\n**Principle**: Put the most likely path first in if/else statements.\\n\\n**Why it works**: CPU predictors often favor the first path (static prediction) or learn that the first path is more common.\\n\\n```csharp\\n//  Bad: Rare case first\\nif (errorOccurred) {  // Rare case\\n    HandleError();\\n} else {  // Common case\\n    ProcessNormal();\\n}\\n\\n//  Good: Common case first\\nif (!errorOccurred) {  // Common case\\n    ProcessNormal();\\n} else {  // Rare case\\n    HandleError();\\n}\\n```\\n\\n**Benefit**: Predictor learns the common path faster, fewer mispredictions.\\n\\n### Technique 2: Separate Unpredictable Branches\\n\\n**Principle**: If you have a loop with an unpredictable branch, separate the filtering from the processing.\\n\\n```csharp\\n//  Bad: Unpredictable branch in hot loop\\nint count = 0;\\nforeach (var item in items) {\\n    if (item.IsValid && item.Value > threshold) {  // Unpredictable\\n        Process(item);\\n        count++;\\n    }\\n}\\n\\n//  Good: Separate filtering (branch once per item)\\nvar validItems = items\\n    .Where(i => i.IsValid && i.Value > threshold)\\n    .ToList();  // Branch here, but only once per item\\n\\nforeach (var item in validItems) {  // No branches in hot loop!\\n    Process(item);\\n}\\n```\\n\\n**Benefit**: Branch happens during filtering (once), not in the hot processing loop (many times).\\n\\n### Technique 3: Branchless Operations\\n\\n**Principle**: Use arithmetic operations instead of branches when possible.\\n\\n```csharp\\n//  Bad: Branch in hot loop\\nint count = 0;\\nforeach (var value in values) {\\n    if (value > threshold) {\\n        count++;\\n    }\\n}\\n\\n//  Good: Branchless\\nint count = 0;\\nforeach (var value in values) {\\n    count += (value > threshold) ? 1 : 0;  // No branch, just arithmetic\\n}\\n\\n// Or using LINQ (compiler may optimize)\\nint count = values.Count(v => v > threshold);\\n```\\n\\n**Why it works**: Conditional expressions can be compiled to branchless code (conditional moves, bitwise operations). No branch = no misprediction.\\n\\n**Trade-off**: Might be slightly less readable. Use when profiling shows the branch is a problem.\\n\\n### Technique 4: Sort Data for Predictable Comparisons\\n\\n**Principle**: When comparing values in a loop, sorted data makes branches predictable.\\n\\n```csharp\\n// Unsorted data: comparisons are unpredictable\\nforeach (var item in unsortedItems) {\\n    if (item.Value > threshold) {  // 50/50 unpredictable\\n        Process(item);\\n    }\\n}\\n\\n// Sorted data: comparisons are predictable\\nArray.Sort(items, (a, b) => a.Value.CompareTo(b.Value));\\nforeach (var item in items) {\\n    if (item.Value > threshold) {  // First all false, then all true\\n        Process(item);  // Predictable pattern!\\n    }\\n}\\n```\\n\\n**Why it works**: With sorted data, comparisons follow a pattern (all false, then all true). Predictors learn this quickly.\\n\\n**Trade-off**: Sorting has a cost. Only worth it if you process the data multiple times or if sorting is cheap.\\n\\n### Technique 5: Use Lookup Tables Instead of Switches\\n\\n**Principle**: For small, dense switch statements, lookup tables can avoid branches.\\n\\n```csharp\\n//  Many branches (switch)\\nint result;\\nswitch (value) {\\n    case 0: result = Function0(); break;\\n    case 1: result = Function1(); break;\\n    case 2: result = Function2(); break;\\n    // ... many cases\\n}\\n\\n//  Lookup table (no branches if predictable)\\nvar functions = new Func<int>[] { Function0, Function1, Function2, ... };\\nint result = functions[value]();  // Direct jump, no branches\\n```\\n\\n**Why it works**: Direct array access and function call, no conditional branches to predict.\\n\\n**Trade-off**: Only works for dense, small ranges. Sparse switches might be better as switches.\\n\\n### Technique 6: Loop Unrolling for Predictable Patterns\\n\\n**Principle**: Reduce the number of loop branches by processing multiple items per iteration.\\n\\n```csharp\\n// Standard loop: branch every iteration\\nfor (int i = 0; i < array.Length; i++) {\\n    Process(array[i]);\\n}\\n\\n// Unrolled: branch every 4 iterations\\nfor (int i = 0; i < array.Length - 3; i += 4) {\\n    Process(array[i]);\\n    Process(array[i + 1]);\\n    Process(array[i + 2]);\\n    Process(array[i + 3]);\\n}\\n// Handle remainder...\\n\\n// Or let the compiler do it (modern compilers auto-unroll when beneficial)\\n```\\n\\n**Why it works**: Fewer loop branches = fewer misprediction opportunities.\\n\\n**Trade-off**: More code, might hurt instruction cache. Modern compilers often do this automatically.\\n\\n---\\n\\n## Example Scenarios\\n\\n### Scenario 1: Processing Items with Filters\\n\\n**Problem**: Loop with unpredictable filtering condition.\\n\\n**Solution**: Separate filtering from processing.\\n\\n```csharp\\n// Before: Unpredictable branch in hot loop\\npublic int ProcessValidItems(List<Item> items, int threshold) {\\n    int count = 0;\\n    foreach (var item in items) {\\n        if (item.IsValid && item.Value > threshold) {  // Unpredictable\\n            ProcessItem(item);\\n            count++;\\n        }\\n    }\\n    return count;\\n}\\n\\n// After: Predictable (or no branches in hot loop)\\npublic int ProcessValidItems(List<Item> items, int threshold) {\\n    var validItems = items\\n        .Where(i => i.IsValid && i.Value > threshold)\\n        .ToList();\\n    \\n    foreach (var item in validItems) {  // No branches!\\n        ProcessItem(item);\\n    }\\n    return validItems.Count;\\n}\\n```\\n\\n**Performance**: 10-20% improvement when the branch was unpredictable.\\n\\n### Scenario 2: Error Handling\\n\\n**Problem**: Error checks in hot path, but errors are rare.\\n\\n**Solution**: Common case first, or extract error handling.\\n\\n```csharp\\n// Before: Rare case first\\npublic void ProcessRequest(Request req) {\\n    if (req == null || !req.IsValid) {  // Rare\\n        throw new ArgumentException();\\n    }\\n    // Common case...\\n}\\n\\n// After: Common case first, or extract\\npublic void ProcessRequest(Request req) {\\n    if (req != null && req.IsValid) {  // Common case first\\n        // Process...\\n    } else {\\n        throw new ArgumentException();\\n    }\\n}\\n```\\n\\n**Performance**: 5-10% improvement in hot request processing path.\\n\\n### Scenario 3: Counting with Conditions\\n\\n**Problem**: Branch in counting loop.\\n\\n**Solution**: Branchless counting.\\n\\n```csharp\\n// Before: Branch\\nint count = 0;\\nforeach (var value in values) {\\n    if (value > threshold) {\\n        count++;\\n    }\\n}\\n\\n// After: Branchless\\nint count = 0;\\nforeach (var value in values) {\\n    count += (value > threshold) ? 1 : 0;\\n}\\n\\n// Or LINQ (compiler optimizes)\\nint count = values.Count(v => v > threshold);\\n```\\n\\n**Performance**: 15-25% improvement in tight counting loops.\"}]],\"markups\":[],\"sections\":[[10,0]]}',1767133197943,'2025-12-30 22:19:57'),('6956d503ebcfaae683eec9d5','6956d503ebcfaae683eec9c9','{\"version\":\"0.3.1\",\"atoms\":[],\"cards\":[[\"markdown\",{\"markdown\":\"# Avoid Busy-Wait Loops: Use Proper Synchronization Primitives\\n\\n**Replace CPU-wasting busy-wait loops with appropriate synchronization mechanisms to free CPU cycles for useful work and improve system efficiency.**\\n\\n---\\n\\n## Executive Summary (TL;DR)\\n\\nBusy-wait loops continuously consume CPU cycles while waiting for a condition to become true, without yielding control to the operating system scheduler. This wastes CPU resources, increases power consumption, prevents other threads from running, and can degrade overall system performance. The solution is to use proper synchronization primitives (events, semaphores, condition variables) that allow the thread to sleep until the condition is met, or use async/await for non-blocking waits. For very short waits (microseconds), `SpinWait` provides an optimized alternative. Eliminating busy-wait loops can free 10-100% of CPU depending on the situation. Always prefer blocking synchronization or async patterns over busy-waiting, except in extremely latency-critical scenarios with guaranteed short wait times.\\n\\n---\\n\\n## Problem Context\\n\\n**What is a busy-wait loop?** A loop that repeatedly checks a condition in a tight cycle, consuming CPU continuously while waiting. The thread never gives up the CPU, so it burns cycles doing nothing useful.\\n\\n**Example of busy-wait**:\\n```csharp\\nwhile (!condition) {\\n    // Empty loop - just checking condition over and over\\n    // CPU is at 100% doing nothing useful!\\n}\\n```\\n\\n**The problem**: While the thread is busy-waiting, it:\\n- Consumes CPU cycles that could be used by other threads\\n- Wastes power (especially important in mobile/embedded systems)\\n- Prevents the operating system from scheduling other work\\n- Can cause thermal issues (CPU heating up from constant work)\\n- May starve other threads of CPU time\\n\\n### Key Terms Explained\\n**Context Switch**: When the OS scheduler stops one thread and starts another. This has overhead (saving/restoring thread state), but allows the CPU to do useful work instead of busy-waiting.\\n\\n**Yield**: When a thread voluntarily gives up the CPU, allowing the scheduler to run other threads. Threads can yield explicitly or by calling blocking operations.\\n\\n**Blocking**: When a thread waits for something (I/O, synchronization event) and is suspended by the OS. The thread doesn\'t consume CPU while blocked.\\n\\n### Why Naive Solutions Fail\\n\\n**Adding `Thread.Sleep()` to busy-wait loops**\\n```csharp\\nwhile (!condition) {\\n    Thread.Sleep(1);  // Sleep 1ms\\n}\\n```\\n- **Why it\'s better but still wrong**: Reduces CPU usage but introduces unnecessary latency. The thread wakes up every millisecond to check, even if the condition changed immediately after sleeping. Still not ideal.\\n\\n**Using very short sleeps**\\n```csharp\\nwhile (!condition) {\\n    Thread.Sleep(0);  // Yield to other threads\\n}\\n```\\n- **Why it\'s better**: Yields to other threads, but still has overhead. Better than pure busy-wait, but proper synchronization is cleaner and more efficient.\\n\\n**Assuming the problem will fix itself**\\n- **Why it fails**: Busy-wait doesn\'t fix itself. It continues wasting resources until you fix the code.\\n\\n---\\n\\n## How It Works\\n\\n### Understanding Thread Scheduling\\n\\n**What happens when a thread runs**: The OS scheduler assigns a CPU core to the thread. The thread executes instructions until:\\n- It completes its work\\n- It yields the CPU (explicitly or by blocking)\\n- The scheduler preempts it (time slice expires, higher priority thread needs CPU)\\n\\n**What happens during busy-wait**: The thread never yields. It executes the loop instructions continuously:\\n1. Check condition (read memory/register)\\n2. Compare with expected value\\n3. Branch back to step 1 if false\\n4. Repeat millions of times per second\\n\\n**CPU perspective**: The CPU executes these instructions at full speed (billions per second). Even though the thread isn\'t doing useful work, the CPU is working hard, consuming power and generating heat.\\n\\n### Operating System Behavior\\n\\n**When a thread blocks** (waits on a synchronization primitive):\\n1. Thread calls a blocking operation (e.g., `Wait()`, `Sleep()`)\\n2. OS scheduler removes thread from \\\"runnable\\\" queue\\n3. Thread is marked as \\\"waiting\\\" and doesn\'t consume CPU\\n4. OS schedules other threads to use the CPU\\n5. When the condition is met, OS moves thread back to \\\"runnable\\\" queue\\n6. Thread eventually gets scheduled and continues execution\\n\\n**Context switch overhead**: Switching threads takes ~1-10 microseconds on modern systems. This is minimal compared to wasting thousands of cycles in busy-wait.\\n\\n**Wake-up latency**: Modern OS kernels can wake waiting threads very quickly (often microseconds). Proper synchronization primitives use efficient kernel mechanisms (events, futexes) for fast wake-up.\\n\\n### Synchronization Primitives Explained\\n\\n**Event/Semaphore/Mutex**: These are OS-provided synchronization mechanisms that:\\n- Allow threads to wait without consuming CPU\\n- Wake threads efficiently when conditions change\\n- Use kernel mechanisms for fast signaling\\n\\n**How they work**:\\n1. Thread calls `Wait()`  OS suspends thread, doesn\'t consume CPU\\n2. Another thread calls `Set()`/`Signal()`  OS wakes waiting thread(s)\\n3. Woken thread resumes execution\\n\\n**Performance**: Wake-up is typically microseconds. The overhead is negligible compared to busy-wait waste.\\n\\n---\\n\\n## Why This Becomes a Bottleneck\\n\\n### CPU Starvation\\n\\n**What happens**: Busy-wait threads consume CPU cores, preventing other threads from running. If you have N CPU cores and M threads busy-waiting (where M > N), some threads can\'t run at all.\\n\\n**Impact**: System throughput decreases. Actual work threads compete for fewer available cores, causing slowdowns.\\n\\n**Example**: 8-core server, 10 threads doing real work, 5 threads busy-waiting. The 5 busy-wait threads consume 5 cores continuously. Only 3 cores available for the 10 work threads  severe contention and poor performance.\\n\\n### Power Consumption\\n\\n**What happens**: CPUs consume more power when executing instructions. Busy-wait loops execute instructions continuously, keeping the CPU active and consuming power.\\n\\n**Impact**: \\n- Higher electricity costs in data centers\\n- Reduced battery life in mobile devices\\n- Thermal issues (CPU heating up)\\n- Need for better cooling systems\\n\\n**Why it\'s significant**: In a data center with thousands of servers, busy-wait loops can significantly increase power consumption and cooling costs.\\n\\n### Thermal Throttling\\n\\n**What happens**: When CPUs get too hot, they reduce clock frequency to cool down (thermal throttling). Busy-wait loops generate heat, potentially triggering throttling.\\n\\n**Impact**: Reduced CPU performance system-wide. Even threads doing useful work slow down because the CPU is throttled.\\n\\n### Scheduler Inefficiency\\n\\n**What happens**: The OS scheduler can\'t make optimal scheduling decisions when threads don\'t yield. It can\'t balance load effectively or prioritize important work.\\n\\n**Impact**: Suboptimal resource utilization, longer response times for important work, poor system responsiveness.\\n\\n### False Parallelism\\n\\n**What happens**: Multiple threads busy-waiting appear to be \\\"running\\\" (they consume CPU), but they\'re not doing useful work. This creates an illusion of parallelism without actual progress.\\n\\n**Impact**: System appears busy but throughput is low. Monitoring tools show high CPU usage but low actual work completion.\\n\\n---\\n\\n## Optimization Techniques\\n\\n### Technique 1: Use Events/Semaphores for Coordination\\n\\n**When**: Threads need to wait for conditions or events from other threads.\\n\\n```csharp\\n//  Bad: Busy-wait\\npublic class BadWait {\\n    private bool _flag = false;\\n    \\n    public void WaitForFlag() {\\n        while (!_flag) {\\n            // Wasting CPU!\\n        }\\n    }\\n    \\n    public void SetFlag() {\\n        _flag = true;\\n    }\\n}\\n\\n//  Good: Use ManualResetEventSlim\\npublic class GoodWait {\\n    private readonly ManualResetEventSlim _event = new ManualResetEventSlim(false);\\n    \\n    public void WaitForFlag() {\\n        _event.Wait();  // Blocks, doesn\'t consume CPU\\n    }\\n    \\n    public void SetFlag() {\\n        _event.Set();  // Wakes waiting thread\\n    }\\n}\\n```\\n\\n**Why it works**: `ManualResetEventSlim` uses efficient OS mechanisms. Thread blocks (doesn\'t consume CPU) until `Set()` is called, then wakes quickly (microseconds).\\n\\n**Performance**: Eliminates CPU waste. Wake-up latency is microseconds, negligible for most scenarios.\\n\\n### Technique 2: Use async/await for I/O\\n\\n**When**: Waiting for I/O operations (file, network, database).\\n\\n```csharp\\n//  Bad: Busy-wait for I/O\\npublic void ProcessFile(string path) {\\n    while (!File.Exists(path)) {\\n        // Wasting CPU waiting for file!\\n    }\\n    // Process file...\\n}\\n\\n//  Good: Use async/await\\npublic async Task ProcessFileAsync(string path) {\\n    // Wait for file without blocking thread\\n    while (!File.Exists(path)) {\\n        await Task.Delay(100);  // Yield to other work\\n    }\\n    // Process file...\\n}\\n\\n//  Better: Use FileSystemWatcher or proper async I/O\\npublic async Task ProcessFileAsync(string path) {\\n    using var fileStream = File.OpenRead(path);\\n    // Use async I/O methods\\n    var buffer = new byte[4096];\\n    await fileStream.ReadAsync(buffer, 0, buffer.Length);\\n}\\n```\\n\\n**Why it works**: `async/await` doesn\'t block threads. The thread can do other work while waiting for I/O. When I/O completes, execution resumes.\\n\\n**Performance**: Doesn\'t waste threads on I/O waits. Allows more concurrent operations.\\n\\n### Technique 3: Use SpinWait for Very Short Waits\\n\\n**When**: Wait time is guaranteed to be extremely short (nanoseconds to microseconds).\\n\\n```csharp\\n//  For very short waits, use SpinWait\\npublic class OptimizedWait {\\n    private volatile bool _flag = false;\\n    \\n    public void WaitForFlag() {\\n        var spinWait = new SpinWait();\\n        while (!_flag) {\\n            spinWait.SpinOnce();  // Optimized for short waits\\n            // After some spins, yields to other threads\\n        }\\n    }\\n}\\n```\\n\\n**Why it works**: `SpinWait` uses CPU hints and gradually increases back-off. For very short waits, it avoids context switch overhead. For longer waits, it yields to prevent waste.\\n\\n**When to use**: Only when you\'re absolutely certain the wait will be nanoseconds/microseconds. For longer waits, use blocking primitives.\\n\\n**Trade-off**: Still consumes some CPU, but optimized. Use sparingly.\\n\\n### Technique 4: Use TaskCompletionSource for Async Coordination\\n\\n**When**: Coordinating async operations without blocking threads.\\n\\n```csharp\\n//  Good: TaskCompletionSource for async coordination\\npublic class AsyncWait {\\n    private readonly TaskCompletionSource<bool> _tcs = new TaskCompletionSource<bool>();\\n    \\n    public async Task WaitForFlagAsync() {\\n        await _tcs.Task;  // Non-blocking wait\\n    }\\n    \\n    public void SetFlag() {\\n        _tcs.SetResult(true);  // Completes the task\\n    }\\n}\\n```\\n\\n**Why it works**: `TaskCompletionSource` creates a task that completes when `SetResult()` is called. `await` doesn\'t block threadsit schedules continuation when the task completes.\\n\\n**Performance**: No thread blocking, efficient scheduling, allows high concurrency.\\n\\n### Technique 5: Use Producer-Consumer Patterns\\n\\n**When**: Threads need to wait for work items.\\n\\n```csharp\\n//  Good: Use BlockingCollection for producer-consumer\\npublic class WorkQueue {\\n    private readonly BlockingCollection<WorkItem> _queue = new BlockingCollection<WorkItem>();\\n    \\n    // Producer\\n    public void AddWork(WorkItem item) {\\n        _queue.Add(item);  // Wakes waiting consumers\\n    }\\n    \\n    // Consumer\\n    public WorkItem GetWork() {\\n        return _queue.Take();  // Blocks until work available\\n    }\\n}\\n```\\n\\n**Why it works**: `BlockingCollection` uses efficient blocking. Consumers block (no CPU waste) until producers add work, then wake immediately.\\n\\n**Performance**: Efficient coordination without busy-wait.\\n\\n---\\n\\n## Example Scenarios\\n\\n### Scenario 1: Waiting for a Flag\\n\\n**Problem**: Thread needs to wait for a boolean flag to become true.\\n\\n**Bad solution**: Busy-wait loop.\\n\\n**Good solution**: Use `ManualResetEventSlim` or `TaskCompletionSource`.\\n\\n```csharp\\n//  Recommended\\nprivate readonly ManualResetEventSlim _ready = new ManualResetEventSlim(false);\\n\\npublic void WaitForReady() {\\n    _ready.Wait();  // Blocks efficiently\\n}\\n\\npublic void SetReady() {\\n    _ready.Set();  // Wakes waiting threads\\n}\\n```\\n\\n**Performance impact**: Eliminates CPU waste. Thread blocks until ready, then wakes in microseconds.\\n\\n### Scenario 2: Waiting for I/O\\n\\n**Problem**: Waiting for file to be created or network response.\\n\\n**Bad solution**: Busy-wait checking file existence or response.\\n\\n**Good solution**: Use async/await with proper async I/O.\\n\\n```csharp\\n//  Recommended\\npublic async Task<string> ReadFileAsync(string path) {\\n    using var reader = new StreamReader(path);\\n    return await reader.ReadToEndAsync();  // Non-blocking I/O\\n}\\n```\\n\\n**Performance impact**: Doesn\'t block threads. Allows thousands of concurrent I/O operations.\\n\\n### Scenario 3: Producer-Consumer Pattern\\n\\n**Problem**: Consumer threads need to wait for work items.\\n\\n**Bad solution**: Busy-wait checking if queue has items.\\n\\n**Good solution**: Use `BlockingCollection` or channels.\\n\\n```csharp\\n//  Recommended\\nprivate readonly BlockingCollection<WorkItem> _workQueue = new BlockingCollection<WorkItem>();\\n\\npublic void ProcessWork() {\\n    foreach (var item in _workQueue.GetConsumingEnumerable()) {\\n        Process(item);  // Blocks until work available\\n    }\\n}\\n```\\n\\n**Performance impact**: Efficient coordination. Consumers block until work available, no CPU waste.\\n\\n---\\n\\n## Summary and Key Takeaways\\n\\nBusy-wait loops continuously consume CPU cycles while waiting for conditions, wasting resources and preventing other threads from running. Replace them with proper synchronization primitives (events, semaphores, async/await) that allow threads to block efficiently until conditions are met.\\n\\n**Core Principle**: Never busy-wait unless you\'re absolutely certain the wait will be nanoseconds. Use blocking synchronization or async patterns for all other cases.\\n\\n**Main Trade-off**: Tiny wake-up latency (microseconds) for blocking vs. massive CPU waste for busy-wait. The trade-off almost always favors blocking.\"}]],\"markups\":[],\"sections\":[[10,0]]}',1767298307119,'2026-01-01 20:11:47');
/*!40000 ALTER TABLE `mobiledoc_revisions` ENABLE KEYS */;
UNLOCK TABLES;

--
-- Table structure for table `newsletters`
--

DROP TABLE IF EXISTS `newsletters`;
/*!40101 SET @saved_cs_client     = @@character_set_client */;
/*!50503 SET character_set_client = utf8mb4 */;
CREATE TABLE `newsletters` (
  `id` varchar(24) NOT NULL,
  `uuid` varchar(36) NOT NULL,
  `name` varchar(191) NOT NULL,
  `description` varchar(2000) DEFAULT NULL,
  `feedback_enabled` tinyint(1) NOT NULL DEFAULT '0',
  `slug` varchar(191) NOT NULL,
  `sender_name` varchar(191) DEFAULT NULL,
  `sender_email` varchar(191) DEFAULT NULL,
  `sender_reply_to` varchar(191) NOT NULL DEFAULT 'newsletter',
  `status` varchar(50) NOT NULL DEFAULT 'active',
  `visibility` varchar(50) NOT NULL DEFAULT 'members',
  `subscribe_on_signup` tinyint(1) NOT NULL DEFAULT '1',
  `sort_order` int unsigned NOT NULL DEFAULT '0',
  `header_image` varchar(2000) DEFAULT NULL,
  `show_header_icon` tinyint(1) NOT NULL DEFAULT '1',
  `show_header_title` tinyint(1) NOT NULL DEFAULT '1',
  `show_excerpt` tinyint(1) NOT NULL DEFAULT '0',
  `title_font_category` varchar(191) NOT NULL DEFAULT 'sans_serif',
  `title_alignment` varchar(191) NOT NULL DEFAULT 'center',
  `show_feature_image` tinyint(1) NOT NULL DEFAULT '1',
  `body_font_category` varchar(191) NOT NULL DEFAULT 'sans_serif',
  `footer_content` text,
  `show_badge` tinyint(1) NOT NULL DEFAULT '1',
  `show_header_name` tinyint(1) NOT NULL DEFAULT '1',
  `show_post_title_section` tinyint(1) NOT NULL DEFAULT '1',
  `show_comment_cta` tinyint(1) NOT NULL DEFAULT '1',
  `show_subscription_details` tinyint(1) NOT NULL DEFAULT '0',
  `show_latest_posts` tinyint(1) NOT NULL DEFAULT '0',
  `background_color` varchar(50) NOT NULL DEFAULT 'light',
  `post_title_color` varchar(50) DEFAULT NULL,
  `created_at` datetime NOT NULL,
  `updated_at` datetime DEFAULT NULL,
  `button_corners` varchar(50) NOT NULL DEFAULT 'rounded',
  `button_style` varchar(50) NOT NULL DEFAULT 'fill',
  `title_font_weight` varchar(50) NOT NULL DEFAULT 'bold',
  `link_style` varchar(50) NOT NULL DEFAULT 'underline',
  `image_corners` varchar(50) NOT NULL DEFAULT 'square',
  `header_background_color` varchar(50) NOT NULL DEFAULT 'transparent',
  `section_title_color` varchar(50) DEFAULT NULL,
  `divider_color` varchar(50) DEFAULT NULL,
  `button_color` varchar(50) DEFAULT 'accent',
  `link_color` varchar(50) DEFAULT 'accent',
  PRIMARY KEY (`id`),
  UNIQUE KEY `newsletters_uuid_unique` (`uuid`),
  UNIQUE KEY `newsletters_name_unique` (`name`),
  UNIQUE KEY `newsletters_slug_unique` (`slug`)
) ENGINE=InnoDB DEFAULT CHARSET=utf8mb4 COLLATE=utf8mb4_0900_ai_ci;
/*!40101 SET character_set_client = @saved_cs_client */;

--
-- Dumping data for table `newsletters`
--

LOCK TABLES `newsletters` WRITE;
/*!40000 ALTER TABLE `newsletters` DISABLE KEYS */;
INSERT INTO `newsletters` VALUES ('694f2665c3e44e650f0e5de9','f77efd8b-3657-49c5-a382-350404caa68b','Performance Engineering',NULL,0,'default-newsletter',NULL,NULL,'newsletter','active','members',1,0,NULL,1,1,0,'sans_serif','center',1,'sans_serif',NULL,1,0,1,1,0,0,'light',NULL,'2025-12-27 00:20:53','2025-12-27 00:21:16','rounded','fill','bold','underline','square','transparent',NULL,NULL,'accent','accent');
/*!40000 ALTER TABLE `newsletters` ENABLE KEYS */;
UNLOCK TABLES;

--
-- Table structure for table `offer_redemptions`
--

DROP TABLE IF EXISTS `offer_redemptions`;
/*!40101 SET @saved_cs_client     = @@character_set_client */;
/*!50503 SET character_set_client = utf8mb4 */;
CREATE TABLE `offer_redemptions` (
  `id` varchar(24) NOT NULL,
  `offer_id` varchar(24) NOT NULL,
  `member_id` varchar(24) NOT NULL,
  `subscription_id` varchar(24) NOT NULL,
  `created_at` datetime NOT NULL,
  PRIMARY KEY (`id`),
  KEY `offer_redemptions_offer_id_foreign` (`offer_id`),
  KEY `offer_redemptions_member_id_foreign` (`member_id`),
  KEY `offer_redemptions_subscription_id_foreign` (`subscription_id`),
  CONSTRAINT `offer_redemptions_member_id_foreign` FOREIGN KEY (`member_id`) REFERENCES `members` (`id`) ON DELETE CASCADE,
  CONSTRAINT `offer_redemptions_offer_id_foreign` FOREIGN KEY (`offer_id`) REFERENCES `offers` (`id`) ON DELETE CASCADE,
  CONSTRAINT `offer_redemptions_subscription_id_foreign` FOREIGN KEY (`subscription_id`) REFERENCES `members_stripe_customers_subscriptions` (`id`) ON DELETE CASCADE
) ENGINE=InnoDB DEFAULT CHARSET=utf8mb4 COLLATE=utf8mb4_0900_ai_ci;
/*!40101 SET character_set_client = @saved_cs_client */;

--
-- Dumping data for table `offer_redemptions`
--

LOCK TABLES `offer_redemptions` WRITE;
/*!40000 ALTER TABLE `offer_redemptions` DISABLE KEYS */;
/*!40000 ALTER TABLE `offer_redemptions` ENABLE KEYS */;
UNLOCK TABLES;

--
-- Table structure for table `offers`
--

DROP TABLE IF EXISTS `offers`;
/*!40101 SET @saved_cs_client     = @@character_set_client */;
/*!50503 SET character_set_client = utf8mb4 */;
CREATE TABLE `offers` (
  `id` varchar(24) NOT NULL,
  `active` tinyint(1) NOT NULL DEFAULT '1',
  `name` varchar(191) NOT NULL,
  `code` varchar(191) NOT NULL,
  `product_id` varchar(24) NOT NULL,
  `stripe_coupon_id` varchar(255) DEFAULT NULL,
  `interval` varchar(50) NOT NULL,
  `currency` varchar(50) DEFAULT NULL,
  `discount_type` varchar(50) NOT NULL,
  `discount_amount` int NOT NULL,
  `duration` varchar(50) NOT NULL,
  `duration_in_months` int DEFAULT NULL,
  `portal_title` varchar(191) DEFAULT NULL,
  `portal_description` varchar(2000) DEFAULT NULL,
  `created_at` datetime NOT NULL,
  `updated_at` datetime DEFAULT NULL,
  PRIMARY KEY (`id`),
  UNIQUE KEY `offers_name_unique` (`name`),
  UNIQUE KEY `offers_code_unique` (`code`),
  UNIQUE KEY `offers_stripe_coupon_id_unique` (`stripe_coupon_id`),
  KEY `offers_product_id_foreign` (`product_id`),
  CONSTRAINT `offers_product_id_foreign` FOREIGN KEY (`product_id`) REFERENCES `products` (`id`)
) ENGINE=InnoDB DEFAULT CHARSET=utf8mb4 COLLATE=utf8mb4_0900_ai_ci;
/*!40101 SET character_set_client = @saved_cs_client */;

--
-- Dumping data for table `offers`
--

LOCK TABLES `offers` WRITE;
/*!40000 ALTER TABLE `offers` DISABLE KEYS */;
/*!40000 ALTER TABLE `offers` ENABLE KEYS */;
UNLOCK TABLES;

--
-- Table structure for table `outbox`
--

DROP TABLE IF EXISTS `outbox`;
/*!40101 SET @saved_cs_client     = @@character_set_client */;
/*!50503 SET character_set_client = utf8mb4 */;
CREATE TABLE `outbox` (
  `id` varchar(24) NOT NULL,
  `event_type` varchar(50) NOT NULL,
  `status` varchar(50) NOT NULL DEFAULT 'pending',
  `payload` text NOT NULL,
  `created_at` datetime NOT NULL,
  `updated_at` datetime DEFAULT NULL,
  `retry_count` int unsigned NOT NULL DEFAULT '0',
  `last_retry_at` datetime DEFAULT NULL,
  `message` varchar(2000) DEFAULT NULL,
  PRIMARY KEY (`id`),
  KEY `outbox_event_type_status_created_at_index` (`event_type`,`status`,`created_at`)
) ENGINE=InnoDB DEFAULT CHARSET=utf8mb4 COLLATE=utf8mb4_0900_ai_ci;
/*!40101 SET character_set_client = @saved_cs_client */;

--
-- Dumping data for table `outbox`
--

LOCK TABLES `outbox` WRITE;
/*!40000 ALTER TABLE `outbox` DISABLE KEYS */;
/*!40000 ALTER TABLE `outbox` ENABLE KEYS */;
UNLOCK TABLES;

--
-- Table structure for table `permissions`
--

DROP TABLE IF EXISTS `permissions`;
/*!40101 SET @saved_cs_client     = @@character_set_client */;
/*!50503 SET character_set_client = utf8mb4 */;
CREATE TABLE `permissions` (
  `id` varchar(24) NOT NULL,
  `name` varchar(50) NOT NULL,
  `object_type` varchar(50) NOT NULL,
  `action_type` varchar(50) NOT NULL,
  `object_id` varchar(24) DEFAULT NULL,
  `created_at` datetime NOT NULL,
  `updated_at` datetime DEFAULT NULL,
  PRIMARY KEY (`id`),
  UNIQUE KEY `permissions_name_unique` (`name`)
) ENGINE=InnoDB DEFAULT CHARSET=utf8mb4 COLLATE=utf8mb4_0900_ai_ci;
/*!40101 SET character_set_client = @saved_cs_client */;

--
-- Dumping data for table `permissions`
--

LOCK TABLES `permissions` WRITE;
/*!40000 ALTER TABLE `permissions` DISABLE KEYS */;
INSERT INTO `permissions` VALUES ('694f2665c3e44e650f0e5deb','Read explore data','explore','read',NULL,'2025-12-27 00:20:53','2025-12-27 00:20:53'),('694f2665c3e44e650f0e5dec','Export database','db','exportContent',NULL,'2025-12-27 00:20:53','2025-12-27 00:20:53'),('694f2665c3e44e650f0e5ded','Import database','db','importContent',NULL,'2025-12-27 00:20:53','2025-12-27 00:20:53'),('694f2665c3e44e650f0e5dee','Delete all content','db','deleteAllContent',NULL,'2025-12-27 00:20:53','2025-12-27 00:20:53'),('694f2665c3e44e650f0e5def','Send mail','mail','send',NULL,'2025-12-27 00:20:53','2025-12-27 00:20:53'),('694f2665c3e44e650f0e5df0','Browse notifications','notification','browse',NULL,'2025-12-27 00:20:53','2025-12-27 00:20:53'),('694f2665c3e44e650f0e5df1','Add notifications','notification','add',NULL,'2025-12-27 00:20:53','2025-12-27 00:20:53'),('694f2665c3e44e650f0e5df2','Delete notifications','notification','destroy',NULL,'2025-12-27 00:20:53','2025-12-27 00:20:53'),('694f2665c3e44e650f0e5df3','Browse posts','post','browse',NULL,'2025-12-27 00:20:53','2025-12-27 00:20:53'),('694f2665c3e44e650f0e5df4','Read posts','post','read',NULL,'2025-12-27 00:20:53','2025-12-27 00:20:53'),('694f2665c3e44e650f0e5df5','Edit posts','post','edit',NULL,'2025-12-27 00:20:53','2025-12-27 00:20:53'),('694f2665c3e44e650f0e5df6','Add posts','post','add',NULL,'2025-12-27 00:20:53','2025-12-27 00:20:53'),('694f2665c3e44e650f0e5df7','Delete posts','post','destroy',NULL,'2025-12-27 00:20:53','2025-12-27 00:20:53'),('694f2665c3e44e650f0e5df8','Browse settings','setting','browse',NULL,'2025-12-27 00:20:53','2025-12-27 00:20:53'),('694f2665c3e44e650f0e5df9','Read settings','setting','read',NULL,'2025-12-27 00:20:53','2025-12-27 00:20:53'),('694f2665c3e44e650f0e5dfa','Edit settings','setting','edit',NULL,'2025-12-27 00:20:53','2025-12-27 00:20:53'),('694f2665c3e44e650f0e5dfb','Generate slugs','slug','generate',NULL,'2025-12-27 00:20:53','2025-12-27 00:20:53'),('694f2665c3e44e650f0e5dfc','Browse tags','tag','browse',NULL,'2025-12-27 00:20:53','2025-12-27 00:20:53'),('694f2665c3e44e650f0e5dfd','Read tags','tag','read',NULL,'2025-12-27 00:20:53','2025-12-27 00:20:53'),('694f2665c3e44e650f0e5dfe','Edit tags','tag','edit',NULL,'2025-12-27 00:20:53','2025-12-27 00:20:53'),('694f2665c3e44e650f0e5dff','Add tags','tag','add',NULL,'2025-12-27 00:20:53','2025-12-27 00:20:53'),('694f2665c3e44e650f0e5e00','Delete tags','tag','destroy',NULL,'2025-12-27 00:20:53','2025-12-27 00:20:53'),('694f2665c3e44e650f0e5e01','Browse themes','theme','browse',NULL,'2025-12-27 00:20:53','2025-12-27 00:20:53'),('694f2665c3e44e650f0e5e02','Edit themes','theme','edit',NULL,'2025-12-27 00:20:53','2025-12-27 00:20:53'),('694f2665c3e44e650f0e5e03','Activate themes','theme','activate',NULL,'2025-12-27 00:20:53','2025-12-27 00:20:53'),('694f2665c3e44e650f0e5e04','View active theme details','theme','readActive',NULL,'2025-12-27 00:20:53','2025-12-27 00:20:53'),('694f2665c3e44e650f0e5e05','Upload themes','theme','add',NULL,'2025-12-27 00:20:53','2025-12-27 00:20:53'),('694f2665c3e44e650f0e5e06','Download themes','theme','read',NULL,'2025-12-27 00:20:53','2025-12-27 00:20:53'),('694f2665c3e44e650f0e5e07','Delete themes','theme','destroy',NULL,'2025-12-27 00:20:53','2025-12-27 00:20:53'),('694f2665c3e44e650f0e5e08','Browse users','user','browse',NULL,'2025-12-27 00:20:53','2025-12-27 00:20:53'),('694f2665c3e44e650f0e5e09','Read users','user','read',NULL,'2025-12-27 00:20:53','2025-12-27 00:20:53'),('694f2665c3e44e650f0e5e0a','Edit users','user','edit',NULL,'2025-12-27 00:20:53','2025-12-27 00:20:53'),('694f2665c3e44e650f0e5e0b','Add users','user','add',NULL,'2025-12-27 00:20:53','2025-12-27 00:20:53'),('694f2665c3e44e650f0e5e0c','Delete users','user','destroy',NULL,'2025-12-27 00:20:53','2025-12-27 00:20:53'),('694f2665c3e44e650f0e5e0d','Assign a role','role','assign',NULL,'2025-12-27 00:20:53','2025-12-27 00:20:53'),('694f2665c3e44e650f0e5e0e','Browse roles','role','browse',NULL,'2025-12-27 00:20:53','2025-12-27 00:20:53'),('694f2665c3e44e650f0e5e0f','Browse invites','invite','browse',NULL,'2025-12-27 00:20:53','2025-12-27 00:20:53'),('694f2665c3e44e650f0e5e10','Read invites','invite','read',NULL,'2025-12-27 00:20:53','2025-12-27 00:20:53'),('694f2665c3e44e650f0e5e11','Edit invites','invite','edit',NULL,'2025-12-27 00:20:53','2025-12-27 00:20:53'),('694f2665c3e44e650f0e5e12','Add invites','invite','add',NULL,'2025-12-27 00:20:53','2025-12-27 00:20:53'),('694f2665c3e44e650f0e5e13','Delete invites','invite','destroy',NULL,'2025-12-27 00:20:53','2025-12-27 00:20:53'),('694f2665c3e44e650f0e5e14','Download redirects','redirect','download',NULL,'2025-12-27 00:20:53','2025-12-27 00:20:53'),('694f2665c3e44e650f0e5e15','Upload redirects','redirect','upload',NULL,'2025-12-27 00:20:53','2025-12-27 00:20:53'),('694f2665c3e44e650f0e5e16','Add webhooks','webhook','add',NULL,'2025-12-27 00:20:53','2025-12-27 00:20:53'),('694f2665c3e44e650f0e5e17','Edit webhooks','webhook','edit',NULL,'2025-12-27 00:20:53','2025-12-27 00:20:53'),('694f2665c3e44e650f0e5e18','Delete webhooks','webhook','destroy',NULL,'2025-12-27 00:20:53','2025-12-27 00:20:53'),('694f2665c3e44e650f0e5e19','Browse integrations','integration','browse',NULL,'2025-12-27 00:20:53','2025-12-27 00:20:53'),('694f2665c3e44e650f0e5e1a','Read integrations','integration','read',NULL,'2025-12-27 00:20:53','2025-12-27 00:20:53'),('694f2665c3e44e650f0e5e1b','Edit integrations','integration','edit',NULL,'2025-12-27 00:20:53','2025-12-27 00:20:53'),('694f2665c3e44e650f0e5e1c','Add integrations','integration','add',NULL,'2025-12-27 00:20:53','2025-12-27 00:20:53'),('694f2665c3e44e650f0e5e1d','Delete integrations','integration','destroy',NULL,'2025-12-27 00:20:53','2025-12-27 00:20:53'),('694f2665c3e44e650f0e5e1e','Browse API keys','api_key','browse',NULL,'2025-12-27 00:20:53','2025-12-27 00:20:53'),('694f2665c3e44e650f0e5e1f','Read API keys','api_key','read',NULL,'2025-12-27 00:20:53','2025-12-27 00:20:53'),('694f2665c3e44e650f0e5e20','Edit API keys','api_key','edit',NULL,'2025-12-27 00:20:53','2025-12-27 00:20:53'),('694f2665c3e44e650f0e5e21','Add API keys','api_key','add',NULL,'2025-12-27 00:20:53','2025-12-27 00:20:53'),('694f2665c3e44e650f0e5e22','Delete API keys','api_key','destroy',NULL,'2025-12-27 00:20:53','2025-12-27 00:20:53'),('694f2665c3e44e650f0e5e23','Browse Actions','action','browse',NULL,'2025-12-27 00:20:53','2025-12-27 00:20:53'),('694f2665c3e44e650f0e5e24','Browse Members','member','browse',NULL,'2025-12-27 00:20:53','2025-12-27 00:20:53'),('694f2665c3e44e650f0e5e25','Read Members','member','read',NULL,'2025-12-27 00:20:53','2025-12-27 00:20:53'),('694f2665c3e44e650f0e5e26','Edit Members','member','edit',NULL,'2025-12-27 00:20:53','2025-12-27 00:20:53'),('694f2665c3e44e650f0e5e27','Add Members','member','add',NULL,'2025-12-27 00:20:53','2025-12-27 00:20:53'),('694f2665c3e44e650f0e5e28','Delete Members','member','destroy',NULL,'2025-12-27 00:20:53','2025-12-27 00:20:53'),('694f2665c3e44e650f0e5e29','Browse Products','product','browse',NULL,'2025-12-27 00:20:53','2025-12-27 00:20:53'),('694f2665c3e44e650f0e5e2a','Read Products','product','read',NULL,'2025-12-27 00:20:53','2025-12-27 00:20:53'),('694f2665c3e44e650f0e5e2b','Edit Products','product','edit',NULL,'2025-12-27 00:20:53','2025-12-27 00:20:53'),('694f2665c3e44e650f0e5e2c','Add Products','product','add',NULL,'2025-12-27 00:20:53','2025-12-27 00:20:53'),('694f2665c3e44e650f0e5e2d','Delete Products','product','destroy',NULL,'2025-12-27 00:20:53','2025-12-27 00:20:53'),('694f2665c3e44e650f0e5e2e','Publish posts','post','publish',NULL,'2025-12-27 00:20:53','2025-12-27 00:20:53'),('694f2665c3e44e650f0e5e2f','Backup database','db','backupContent',NULL,'2025-12-27 00:20:53','2025-12-27 00:20:53'),('694f2665c3e44e650f0e5e30','Email preview','email_preview','read',NULL,'2025-12-27 00:20:53','2025-12-27 00:20:53'),('694f2665c3e44e650f0e5e31','Send test email','email_preview','sendTestEmail',NULL,'2025-12-27 00:20:53','2025-12-27 00:20:53'),('694f2665c3e44e650f0e5e32','Browse emails','email','browse',NULL,'2025-12-27 00:20:53','2025-12-27 00:20:53'),('694f2665c3e44e650f0e5e33','Read emails','email','read',NULL,'2025-12-27 00:20:53','2025-12-27 00:20:53'),('694f2665c3e44e650f0e5e34','Retry emails','email','retry',NULL,'2025-12-27 00:20:53','2025-12-27 00:20:53'),('694f2665c3e44e650f0e5e35','Browse labels','label','browse',NULL,'2025-12-27 00:20:53','2025-12-27 00:20:53'),('694f2665c3e44e650f0e5e36','Read labels','label','read',NULL,'2025-12-27 00:20:53','2025-12-27 00:20:53'),('694f2665c3e44e650f0e5e37','Edit labels','label','edit',NULL,'2025-12-27 00:20:53','2025-12-27 00:20:53'),('694f2665c3e44e650f0e5e38','Add labels','label','add',NULL,'2025-12-27 00:20:53','2025-12-27 00:20:53'),('694f2665c3e44e650f0e5e39','Delete labels','label','destroy',NULL,'2025-12-27 00:20:53','2025-12-27 00:20:53'),('694f2665c3e44e650f0e5e3a','Browse automated emails','automated_email','browse',NULL,'2025-12-27 00:20:53','2025-12-27 00:20:53'),('694f2665c3e44e650f0e5e3b','Read automated emails','automated_email','read',NULL,'2025-12-27 00:20:53','2025-12-27 00:20:53'),('694f2665c3e44e650f0e5e3c','Edit automated emails','automated_email','edit',NULL,'2025-12-27 00:20:53','2025-12-27 00:20:53'),('694f2665c3e44e650f0e5e3d','Add automated emails','automated_email','add',NULL,'2025-12-27 00:20:53','2025-12-27 00:20:53'),('694f2665c3e44e650f0e5e3e','Delete automated emails','automated_email','destroy',NULL,'2025-12-27 00:20:53','2025-12-27 00:20:53'),('694f2665c3e44e650f0e5e3f','Read member signin urls','member_signin_url','read',NULL,'2025-12-27 00:20:53','2025-12-27 00:20:53'),('694f2665c3e44e650f0e5e40','Read identities','identity','read',NULL,'2025-12-27 00:20:53','2025-12-27 00:20:53'),('694f2665c3e44e650f0e5e41','Auth Stripe Connect for Members','members_stripe_connect','auth',NULL,'2025-12-27 00:20:53','2025-12-27 00:20:53'),('694f2665c3e44e650f0e5e42','Browse snippets','snippet','browse',NULL,'2025-12-27 00:20:53','2025-12-27 00:20:53'),('694f2665c3e44e650f0e5e43','Read snippets','snippet','read',NULL,'2025-12-27 00:20:53','2025-12-27 00:20:53'),('694f2665c3e44e650f0e5e44','Edit snippets','snippet','edit',NULL,'2025-12-27 00:20:53','2025-12-27 00:20:53'),('694f2665c3e44e650f0e5e45','Add snippets','snippet','add',NULL,'2025-12-27 00:20:53','2025-12-27 00:20:53'),('694f2665c3e44e650f0e5e46','Delete snippets','snippet','destroy',NULL,'2025-12-27 00:20:53','2025-12-27 00:20:53'),('694f2665c3e44e650f0e5e47','Browse offers','offer','browse',NULL,'2025-12-27 00:20:53','2025-12-27 00:20:53'),('694f2665c3e44e650f0e5e48','Read offers','offer','read',NULL,'2025-12-27 00:20:53','2025-12-27 00:20:53'),('694f2665c3e44e650f0e5e49','Edit offers','offer','edit',NULL,'2025-12-27 00:20:53','2025-12-27 00:20:53'),('694f2665c3e44e650f0e5e4a','Add offers','offer','add',NULL,'2025-12-27 00:20:53','2025-12-27 00:20:53'),('694f2665c3e44e650f0e5e4b','Reset all passwords','authentication','resetAllPasswords',NULL,'2025-12-27 00:20:53','2025-12-27 00:20:53'),('694f2665c3e44e650f0e5e4c','Browse custom theme settings','custom_theme_setting','browse',NULL,'2025-12-27 00:20:53','2025-12-27 00:20:53'),('694f2665c3e44e650f0e5e4d','Edit custom theme settings','custom_theme_setting','edit',NULL,'2025-12-27 00:20:53','2025-12-27 00:20:53'),('694f2665c3e44e650f0e5e4e','Browse newsletters','newsletter','browse',NULL,'2025-12-27 00:20:53','2025-12-27 00:20:53'),('694f2665c3e44e650f0e5e4f','Read newsletters','newsletter','read',NULL,'2025-12-27 00:20:53','2025-12-27 00:20:53'),('694f2665c3e44e650f0e5e50','Add newsletters','newsletter','add',NULL,'2025-12-27 00:20:53','2025-12-27 00:20:53'),('694f2665c3e44e650f0e5e51','Edit newsletters','newsletter','edit',NULL,'2025-12-27 00:20:53','2025-12-27 00:20:53'),('694f2665c3e44e650f0e5e52','Browse comments','comment','browse',NULL,'2025-12-27 00:20:53','2025-12-27 00:20:53'),('694f2665c3e44e650f0e5e53','Read comments','comment','read',NULL,'2025-12-27 00:20:53','2025-12-27 00:20:53'),('694f2665c3e44e650f0e5e54','Edit comments','comment','edit',NULL,'2025-12-27 00:20:53','2025-12-27 00:20:53'),('694f2665c3e44e650f0e5e55','Add comments','comment','add',NULL,'2025-12-27 00:20:53','2025-12-27 00:20:53'),('694f2665c3e44e650f0e5e56','Delete comments','comment','destroy',NULL,'2025-12-27 00:20:53','2025-12-27 00:20:53'),('694f2665c3e44e650f0e5e57','Moderate comments','comment','moderate',NULL,'2025-12-27 00:20:53','2025-12-27 00:20:53'),('694f2665c3e44e650f0e5e58','Like comments','comment','like',NULL,'2025-12-27 00:20:53','2025-12-27 00:20:53'),('694f2665c3e44e650f0e5e59','Unlike comments','comment','unlike',NULL,'2025-12-27 00:20:53','2025-12-27 00:20:53'),('694f2665c3e44e650f0e5e5a','Report comments','comment','report',NULL,'2025-12-27 00:20:53','2025-12-27 00:20:53'),('694f2665c3e44e650f0e5e5b','Browse links','link','browse',NULL,'2025-12-27 00:20:53','2025-12-27 00:20:53'),('694f2665c3e44e650f0e5e5c','Edit links','link','edit',NULL,'2025-12-27 00:20:53','2025-12-27 00:20:53'),('694f2665c3e44e650f0e5e5d','Browse mentions','mention','browse',NULL,'2025-12-27 00:20:53','2025-12-27 00:20:53'),('694f2665c3e44e650f0e5e5e','Browse collections','collection','browse',NULL,'2025-12-27 00:20:53','2025-12-27 00:20:53'),('694f2665c3e44e650f0e5e5f','Read collections','collection','read',NULL,'2025-12-27 00:20:53','2025-12-27 00:20:53'),('694f2665c3e44e650f0e5e60','Edit collections','collection','edit',NULL,'2025-12-27 00:20:53','2025-12-27 00:20:53'),('694f2665c3e44e650f0e5e61','Add collections','collection','add',NULL,'2025-12-27 00:20:53','2025-12-27 00:20:53'),('694f2665c3e44e650f0e5e62','Delete collections','collection','destroy',NULL,'2025-12-27 00:20:53','2025-12-27 00:20:53'),('694f2665c3e44e650f0e5e63','Browse recommendations','recommendation','browse',NULL,'2025-12-27 00:20:53','2025-12-27 00:20:53'),('694f2665c3e44e650f0e5e64','Read recommendations','recommendation','read',NULL,'2025-12-27 00:20:53','2025-12-27 00:20:53'),('694f2665c3e44e650f0e5e65','Edit recommendations','recommendation','edit',NULL,'2025-12-27 00:20:53','2025-12-27 00:20:53'),('694f2665c3e44e650f0e5e66','Add recommendations','recommendation','add',NULL,'2025-12-27 00:20:53','2025-12-27 00:20:53'),('694f2665c3e44e650f0e5e67','Delete recommendations','recommendation','destroy',NULL,'2025-12-27 00:20:53','2025-12-27 00:20:53');
/*!40000 ALTER TABLE `permissions` ENABLE KEYS */;
UNLOCK TABLES;

--
-- Table structure for table `permissions_roles`
--

DROP TABLE IF EXISTS `permissions_roles`;
/*!40101 SET @saved_cs_client     = @@character_set_client */;
/*!50503 SET character_set_client = utf8mb4 */;
CREATE TABLE `permissions_roles` (
  `id` varchar(24) NOT NULL,
  `role_id` varchar(24) NOT NULL,
  `permission_id` varchar(24) NOT NULL,
  PRIMARY KEY (`id`)
) ENGINE=InnoDB DEFAULT CHARSET=utf8mb4 COLLATE=utf8mb4_0900_ai_ci;
/*!40101 SET character_set_client = @saved_cs_client */;

--
-- Dumping data for table `permissions_roles`
--

LOCK TABLES `permissions_roles` WRITE;
/*!40000 ALTER TABLE `permissions_roles` DISABLE KEYS */;
INSERT INTO `permissions_roles` VALUES ('694f2666c3e44e650f0e5e7b','694f2665c3e44e650f0e5dd9','694f2665c3e44e650f0e5dec'),('694f2666c3e44e650f0e5e7c','694f2665c3e44e650f0e5dd9','694f2665c3e44e650f0e5ded'),('694f2666c3e44e650f0e5e7d','694f2665c3e44e650f0e5dd9','694f2665c3e44e650f0e5dee'),('694f2666c3e44e650f0e5e7e','694f2665c3e44e650f0e5dd9','694f2665c3e44e650f0e5e2f'),('694f2666c3e44e650f0e5e7f','694f2665c3e44e650f0e5dd9','694f2665c3e44e650f0e5def'),('694f2666c3e44e650f0e5e80','694f2665c3e44e650f0e5dd9','694f2665c3e44e650f0e5df0'),('694f2666c3e44e650f0e5e81','694f2665c3e44e650f0e5dd9','694f2665c3e44e650f0e5df1'),('694f2666c3e44e650f0e5e82','694f2665c3e44e650f0e5dd9','694f2665c3e44e650f0e5df2'),('694f2666c3e44e650f0e5e83','694f2665c3e44e650f0e5dd9','694f2665c3e44e650f0e5df3'),('694f2666c3e44e650f0e5e84','694f2665c3e44e650f0e5dd9','694f2665c3e44e650f0e5df4'),('694f2666c3e44e650f0e5e85','694f2665c3e44e650f0e5dd9','694f2665c3e44e650f0e5df5'),('694f2666c3e44e650f0e5e86','694f2665c3e44e650f0e5dd9','694f2665c3e44e650f0e5df6'),('694f2666c3e44e650f0e5e87','694f2665c3e44e650f0e5dd9','694f2665c3e44e650f0e5df7'),('694f2666c3e44e650f0e5e88','694f2665c3e44e650f0e5dd9','694f2665c3e44e650f0e5e2e'),('694f2666c3e44e650f0e5e89','694f2665c3e44e650f0e5dd9','694f2665c3e44e650f0e5df8'),('694f2666c3e44e650f0e5e8a','694f2665c3e44e650f0e5dd9','694f2665c3e44e650f0e5df9'),('694f2666c3e44e650f0e5e8b','694f2665c3e44e650f0e5dd9','694f2665c3e44e650f0e5dfa'),('694f2666c3e44e650f0e5e8c','694f2665c3e44e650f0e5dd9','694f2665c3e44e650f0e5dfb'),('694f2666c3e44e650f0e5e8d','694f2665c3e44e650f0e5dd9','694f2665c3e44e650f0e5dfc'),('694f2666c3e44e650f0e5e8e','694f2665c3e44e650f0e5dd9','694f2665c3e44e650f0e5dfd'),('694f2666c3e44e650f0e5e8f','694f2665c3e44e650f0e5dd9','694f2665c3e44e650f0e5dfe'),('694f2666c3e44e650f0e5e90','694f2665c3e44e650f0e5dd9','694f2665c3e44e650f0e5dff'),('694f2666c3e44e650f0e5e91','694f2665c3e44e650f0e5dd9','694f2665c3e44e650f0e5e00'),('694f2666c3e44e650f0e5e92','694f2665c3e44e650f0e5dd9','694f2665c3e44e650f0e5e01'),('694f2666c3e44e650f0e5e93','694f2665c3e44e650f0e5dd9','694f2665c3e44e650f0e5e02'),('694f2666c3e44e650f0e5e94','694f2665c3e44e650f0e5dd9','694f2665c3e44e650f0e5e03'),('694f2666c3e44e650f0e5e95','694f2665c3e44e650f0e5dd9','694f2665c3e44e650f0e5e04'),('694f2666c3e44e650f0e5e96','694f2665c3e44e650f0e5dd9','694f2665c3e44e650f0e5e05'),('694f2666c3e44e650f0e5e97','694f2665c3e44e650f0e5dd9','694f2665c3e44e650f0e5e06'),('694f2666c3e44e650f0e5e98','694f2665c3e44e650f0e5dd9','694f2665c3e44e650f0e5e07'),('694f2666c3e44e650f0e5e99','694f2665c3e44e650f0e5dd9','694f2665c3e44e650f0e5e08'),('694f2666c3e44e650f0e5e9a','694f2665c3e44e650f0e5dd9','694f2665c3e44e650f0e5e09'),('694f2666c3e44e650f0e5e9b','694f2665c3e44e650f0e5dd9','694f2665c3e44e650f0e5e0a'),('694f2666c3e44e650f0e5e9c','694f2665c3e44e650f0e5dd9','694f2665c3e44e650f0e5e0b'),('694f2666c3e44e650f0e5e9d','694f2665c3e44e650f0e5dd9','694f2665c3e44e650f0e5e0c'),('694f2666c3e44e650f0e5e9e','694f2665c3e44e650f0e5dd9','694f2665c3e44e650f0e5e0d'),('694f2666c3e44e650f0e5e9f','694f2665c3e44e650f0e5dd9','694f2665c3e44e650f0e5e0e'),('694f2666c3e44e650f0e5ea0','694f2665c3e44e650f0e5dd9','694f2665c3e44e650f0e5e0f'),('694f2666c3e44e650f0e5ea1','694f2665c3e44e650f0e5dd9','694f2665c3e44e650f0e5e10'),('694f2666c3e44e650f0e5ea2','694f2665c3e44e650f0e5dd9','694f2665c3e44e650f0e5e11'),('694f2666c3e44e650f0e5ea3','694f2665c3e44e650f0e5dd9','694f2665c3e44e650f0e5e12'),('694f2666c3e44e650f0e5ea4','694f2665c3e44e650f0e5dd9','694f2665c3e44e650f0e5e13'),('694f2666c3e44e650f0e5ea5','694f2665c3e44e650f0e5dd9','694f2665c3e44e650f0e5e14'),('694f2666c3e44e650f0e5ea6','694f2665c3e44e650f0e5dd9','694f2665c3e44e650f0e5e15'),('694f2666c3e44e650f0e5ea7','694f2665c3e44e650f0e5dd9','694f2665c3e44e650f0e5e16'),('694f2666c3e44e650f0e5ea8','694f2665c3e44e650f0e5dd9','694f2665c3e44e650f0e5e17'),('694f2666c3e44e650f0e5ea9','694f2665c3e44e650f0e5dd9','694f2665c3e44e650f0e5e18'),('694f2666c3e44e650f0e5eaa','694f2665c3e44e650f0e5dd9','694f2665c3e44e650f0e5e19'),('694f2666c3e44e650f0e5eab','694f2665c3e44e650f0e5dd9','694f2665c3e44e650f0e5e1a'),('694f2666c3e44e650f0e5eac','694f2665c3e44e650f0e5dd9','694f2665c3e44e650f0e5e1b'),('694f2666c3e44e650f0e5ead','694f2665c3e44e650f0e5dd9','694f2665c3e44e650f0e5e1c'),('694f2666c3e44e650f0e5eae','694f2665c3e44e650f0e5dd9','694f2665c3e44e650f0e5e1d'),('694f2666c3e44e650f0e5eaf','694f2665c3e44e650f0e5dd9','694f2665c3e44e650f0e5e1e'),('694f2666c3e44e650f0e5eb0','694f2665c3e44e650f0e5dd9','694f2665c3e44e650f0e5e1f'),('694f2666c3e44e650f0e5eb1','694f2665c3e44e650f0e5dd9','694f2665c3e44e650f0e5e20'),('694f2666c3e44e650f0e5eb2','694f2665c3e44e650f0e5dd9','694f2665c3e44e650f0e5e21'),('694f2666c3e44e650f0e5eb3','694f2665c3e44e650f0e5dd9','694f2665c3e44e650f0e5e22'),('694f2666c3e44e650f0e5eb4','694f2665c3e44e650f0e5dd9','694f2665c3e44e650f0e5e23'),('694f2666c3e44e650f0e5eb5','694f2665c3e44e650f0e5dd9','694f2665c3e44e650f0e5e24'),('694f2666c3e44e650f0e5eb6','694f2665c3e44e650f0e5dd9','694f2665c3e44e650f0e5e25'),('694f2666c3e44e650f0e5eb7','694f2665c3e44e650f0e5dd9','694f2665c3e44e650f0e5e26'),('694f2666c3e44e650f0e5eb8','694f2665c3e44e650f0e5dd9','694f2665c3e44e650f0e5e27'),('694f2666c3e44e650f0e5eb9','694f2665c3e44e650f0e5dd9','694f2665c3e44e650f0e5e28'),('694f2666c3e44e650f0e5eba','694f2665c3e44e650f0e5dd9','694f2665c3e44e650f0e5e29'),('694f2666c3e44e650f0e5ebb','694f2665c3e44e650f0e5dd9','694f2665c3e44e650f0e5e2a'),('694f2666c3e44e650f0e5ebc','694f2665c3e44e650f0e5dd9','694f2665c3e44e650f0e5e2b'),('694f2666c3e44e650f0e5ebd','694f2665c3e44e650f0e5dd9','694f2665c3e44e650f0e5e2c'),('694f2666c3e44e650f0e5ebe','694f2665c3e44e650f0e5dd9','694f2665c3e44e650f0e5e2d'),('694f2666c3e44e650f0e5ebf','694f2665c3e44e650f0e5dd9','694f2665c3e44e650f0e5e35'),('694f2666c3e44e650f0e5ec0','694f2665c3e44e650f0e5dd9','694f2665c3e44e650f0e5e36'),('694f2666c3e44e650f0e5ec1','694f2665c3e44e650f0e5dd9','694f2665c3e44e650f0e5e37'),('694f2666c3e44e650f0e5ec2','694f2665c3e44e650f0e5dd9','694f2665c3e44e650f0e5e38'),('694f2666c3e44e650f0e5ec3','694f2665c3e44e650f0e5dd9','694f2665c3e44e650f0e5e39'),('694f2666c3e44e650f0e5ec4','694f2665c3e44e650f0e5dd9','694f2665c3e44e650f0e5e3a'),('694f2666c3e44e650f0e5ec5','694f2665c3e44e650f0e5dd9','694f2665c3e44e650f0e5e3b'),('694f2666c3e44e650f0e5ec6','694f2665c3e44e650f0e5dd9','694f2665c3e44e650f0e5e3c'),('694f2666c3e44e650f0e5ec7','694f2665c3e44e650f0e5dd9','694f2665c3e44e650f0e5e3d'),('694f2666c3e44e650f0e5ec8','694f2665c3e44e650f0e5dd9','694f2665c3e44e650f0e5e3e'),('694f2666c3e44e650f0e5ec9','694f2665c3e44e650f0e5dd9','694f2665c3e44e650f0e5e30'),('694f2666c3e44e650f0e5eca','694f2665c3e44e650f0e5dd9','694f2665c3e44e650f0e5e31'),('694f2666c3e44e650f0e5ecb','694f2665c3e44e650f0e5dd9','694f2665c3e44e650f0e5e32'),('694f2666c3e44e650f0e5ecc','694f2665c3e44e650f0e5dd9','694f2665c3e44e650f0e5e33'),('694f2666c3e44e650f0e5ecd','694f2665c3e44e650f0e5dd9','694f2665c3e44e650f0e5e34'),('694f2666c3e44e650f0e5ece','694f2665c3e44e650f0e5dd9','694f2665c3e44e650f0e5e3f'),('694f2666c3e44e650f0e5ecf','694f2665c3e44e650f0e5dd9','694f2665c3e44e650f0e5e42'),('694f2666c3e44e650f0e5ed0','694f2665c3e44e650f0e5dd9','694f2665c3e44e650f0e5e43'),('694f2666c3e44e650f0e5ed1','694f2665c3e44e650f0e5dd9','694f2665c3e44e650f0e5e44'),('694f2666c3e44e650f0e5ed2','694f2665c3e44e650f0e5dd9','694f2665c3e44e650f0e5e45'),('694f2666c3e44e650f0e5ed3','694f2665c3e44e650f0e5dd9','694f2665c3e44e650f0e5e46'),('694f2666c3e44e650f0e5ed4','694f2665c3e44e650f0e5dd9','694f2665c3e44e650f0e5e4c'),('694f2666c3e44e650f0e5ed5','694f2665c3e44e650f0e5dd9','694f2665c3e44e650f0e5e4d'),('694f2666c3e44e650f0e5ed6','694f2665c3e44e650f0e5dd9','694f2665c3e44e650f0e5e47'),('694f2666c3e44e650f0e5ed7','694f2665c3e44e650f0e5dd9','694f2665c3e44e650f0e5e48'),('694f2666c3e44e650f0e5ed8','694f2665c3e44e650f0e5dd9','694f2665c3e44e650f0e5e49'),('694f2666c3e44e650f0e5ed9','694f2665c3e44e650f0e5dd9','694f2665c3e44e650f0e5e4a'),('694f2666c3e44e650f0e5eda','694f2665c3e44e650f0e5dd9','694f2665c3e44e650f0e5e4b'),('694f2666c3e44e650f0e5edb','694f2665c3e44e650f0e5dd9','694f2665c3e44e650f0e5e41'),('694f2666c3e44e650f0e5edc','694f2665c3e44e650f0e5dd9','694f2665c3e44e650f0e5e4e'),('694f2666c3e44e650f0e5edd','694f2665c3e44e650f0e5dd9','694f2665c3e44e650f0e5e4f'),('694f2666c3e44e650f0e5ede','694f2665c3e44e650f0e5dd9','694f2665c3e44e650f0e5e50'),('694f2666c3e44e650f0e5edf','694f2665c3e44e650f0e5dd9','694f2665c3e44e650f0e5e51'),('694f2666c3e44e650f0e5ee0','694f2665c3e44e650f0e5dd9','694f2665c3e44e650f0e5deb'),('694f2666c3e44e650f0e5ee1','694f2665c3e44e650f0e5dd9','694f2665c3e44e650f0e5e52'),('694f2666c3e44e650f0e5ee2','694f2665c3e44e650f0e5dd9','694f2665c3e44e650f0e5e53'),('694f2666c3e44e650f0e5ee3','694f2665c3e44e650f0e5dd9','694f2665c3e44e650f0e5e54'),('694f2666c3e44e650f0e5ee4','694f2665c3e44e650f0e5dd9','694f2665c3e44e650f0e5e55'),('694f2666c3e44e650f0e5ee5','694f2665c3e44e650f0e5dd9','694f2665c3e44e650f0e5e56'),('694f2666c3e44e650f0e5ee6','694f2665c3e44e650f0e5dd9','694f2665c3e44e650f0e5e57'),('694f2666c3e44e650f0e5ee7','694f2665c3e44e650f0e5dd9','694f2665c3e44e650f0e5e58'),('694f2666c3e44e650f0e5ee8','694f2665c3e44e650f0e5dd9','694f2665c3e44e650f0e5e59'),('694f2666c3e44e650f0e5ee9','694f2665c3e44e650f0e5dd9','694f2665c3e44e650f0e5e5a'),('694f2666c3e44e650f0e5eea','694f2665c3e44e650f0e5dd9','694f2665c3e44e650f0e5e5b'),('694f2666c3e44e650f0e5eeb','694f2665c3e44e650f0e5dd9','694f2665c3e44e650f0e5e5c'),('694f2666c3e44e650f0e5eec','694f2665c3e44e650f0e5dd9','694f2665c3e44e650f0e5e5d'),('694f2666c3e44e650f0e5eed','694f2665c3e44e650f0e5dd9','694f2665c3e44e650f0e5e5e'),('694f2666c3e44e650f0e5eee','694f2665c3e44e650f0e5dd9','694f2665c3e44e650f0e5e5f'),('694f2666c3e44e650f0e5eef','694f2665c3e44e650f0e5dd9','694f2665c3e44e650f0e5e60'),('694f2666c3e44e650f0e5ef0','694f2665c3e44e650f0e5dd9','694f2665c3e44e650f0e5e61'),('694f2666c3e44e650f0e5ef1','694f2665c3e44e650f0e5dd9','694f2665c3e44e650f0e5e62'),('694f2666c3e44e650f0e5ef2','694f2665c3e44e650f0e5dd9','694f2665c3e44e650f0e5e63'),('694f2666c3e44e650f0e5ef3','694f2665c3e44e650f0e5dd9','694f2665c3e44e650f0e5e64'),('694f2666c3e44e650f0e5ef4','694f2665c3e44e650f0e5dd9','694f2665c3e44e650f0e5e65'),('694f2666c3e44e650f0e5ef5','694f2665c3e44e650f0e5dd9','694f2665c3e44e650f0e5e66'),('694f2666c3e44e650f0e5ef6','694f2665c3e44e650f0e5dd9','694f2665c3e44e650f0e5e67'),('694f2666c3e44e650f0e5ef7','694f2665c3e44e650f0e5dd9','694f2665c3e44e650f0e5e40'),('694f2666c3e44e650f0e5ef8','694f2665c3e44e650f0e5de1','694f2665c3e44e650f0e5dec'),('694f2666c3e44e650f0e5ef9','694f2665c3e44e650f0e5de1','694f2665c3e44e650f0e5ded'),('694f2666c3e44e650f0e5efa','694f2665c3e44e650f0e5de1','694f2665c3e44e650f0e5dee'),('694f2666c3e44e650f0e5efb','694f2665c3e44e650f0e5de1','694f2665c3e44e650f0e5e2f'),('694f2666c3e44e650f0e5efc','694f2665c3e44e650f0e5de1','694f2665c3e44e650f0e5e24'),('694f2666c3e44e650f0e5efd','694f2665c3e44e650f0e5de2','694f2665c3e44e650f0e5e2e'),('694f2666c3e44e650f0e5efe','694f2665c3e44e650f0e5ddf','694f2665c3e44e650f0e5deb'),('694f2666c3e44e650f0e5eff','694f2665c3e44e650f0e5de0','694f2665c3e44e650f0e5ded'),('694f2666c3e44e650f0e5f00','694f2665c3e44e650f0e5de0','694f2665c3e44e650f0e5e27'),('694f2666c3e44e650f0e5f01','694f2665c3e44e650f0e5de0','694f2665c3e44e650f0e5dfd'),('694f2666c3e44e650f0e5f02','694f2665c3e44e650f0e5dde','694f2665c3e44e650f0e5def'),('694f2666c3e44e650f0e5f03','694f2665c3e44e650f0e5dde','694f2665c3e44e650f0e5df0'),('694f2666c3e44e650f0e5f04','694f2665c3e44e650f0e5dde','694f2665c3e44e650f0e5df1'),('694f2666c3e44e650f0e5f05','694f2665c3e44e650f0e5dde','694f2665c3e44e650f0e5df2'),('694f2666c3e44e650f0e5f06','694f2665c3e44e650f0e5dde','694f2665c3e44e650f0e5df3'),('694f2666c3e44e650f0e5f07','694f2665c3e44e650f0e5dde','694f2665c3e44e650f0e5df4'),('694f2666c3e44e650f0e5f08','694f2665c3e44e650f0e5dde','694f2665c3e44e650f0e5df5'),('694f2666c3e44e650f0e5f09','694f2665c3e44e650f0e5dde','694f2665c3e44e650f0e5df6'),('694f2666c3e44e650f0e5f0a','694f2665c3e44e650f0e5dde','694f2665c3e44e650f0e5df7'),('694f2666c3e44e650f0e5f0b','694f2665c3e44e650f0e5dde','694f2665c3e44e650f0e5e2e'),('694f2666c3e44e650f0e5f0c','694f2665c3e44e650f0e5dde','694f2665c3e44e650f0e5df8'),('694f2666c3e44e650f0e5f0d','694f2665c3e44e650f0e5dde','694f2665c3e44e650f0e5df9'),('694f2666c3e44e650f0e5f0e','694f2665c3e44e650f0e5dde','694f2665c3e44e650f0e5dfa'),('694f2666c3e44e650f0e5f0f','694f2665c3e44e650f0e5dde','694f2665c3e44e650f0e5dfb'),('694f2666c3e44e650f0e5f10','694f2665c3e44e650f0e5dde','694f2665c3e44e650f0e5dfc'),('694f2666c3e44e650f0e5f11','694f2665c3e44e650f0e5dde','694f2665c3e44e650f0e5dfd'),('694f2666c3e44e650f0e5f12','694f2665c3e44e650f0e5dde','694f2665c3e44e650f0e5dfe'),('694f2666c3e44e650f0e5f13','694f2665c3e44e650f0e5dde','694f2665c3e44e650f0e5dff'),('694f2666c3e44e650f0e5f14','694f2665c3e44e650f0e5dde','694f2665c3e44e650f0e5e00'),('694f2666c3e44e650f0e5f15','694f2665c3e44e650f0e5dde','694f2665c3e44e650f0e5e01'),('694f2666c3e44e650f0e5f16','694f2665c3e44e650f0e5dde','694f2665c3e44e650f0e5e02'),('694f2666c3e44e650f0e5f17','694f2665c3e44e650f0e5dde','694f2665c3e44e650f0e5e03'),('694f2666c3e44e650f0e5f18','694f2665c3e44e650f0e5dde','694f2665c3e44e650f0e5e04'),('694f2666c3e44e650f0e5f19','694f2665c3e44e650f0e5dde','694f2665c3e44e650f0e5e05'),('694f2666c3e44e650f0e5f1a','694f2665c3e44e650f0e5dde','694f2665c3e44e650f0e5e06'),('694f2666c3e44e650f0e5f1b','694f2665c3e44e650f0e5dde','694f2665c3e44e650f0e5e07'),('694f2666c3e44e650f0e5f1c','694f2665c3e44e650f0e5dde','694f2665c3e44e650f0e5e08'),('694f2666c3e44e650f0e5f1d','694f2665c3e44e650f0e5dde','694f2665c3e44e650f0e5e09'),('694f2666c3e44e650f0e5f1e','694f2665c3e44e650f0e5dde','694f2665c3e44e650f0e5e0a'),('694f2666c3e44e650f0e5f1f','694f2665c3e44e650f0e5dde','694f2665c3e44e650f0e5e0b'),('694f2666c3e44e650f0e5f20','694f2665c3e44e650f0e5dde','694f2665c3e44e650f0e5e0c'),('694f2666c3e44e650f0e5f21','694f2665c3e44e650f0e5dde','694f2665c3e44e650f0e5e0d'),('694f2666c3e44e650f0e5f22','694f2665c3e44e650f0e5dde','694f2665c3e44e650f0e5e0e'),('694f2666c3e44e650f0e5f23','694f2665c3e44e650f0e5dde','694f2665c3e44e650f0e5e0f'),('694f2666c3e44e650f0e5f24','694f2665c3e44e650f0e5dde','694f2665c3e44e650f0e5e10'),('694f2666c3e44e650f0e5f25','694f2665c3e44e650f0e5dde','694f2665c3e44e650f0e5e11'),('694f2666c3e44e650f0e5f26','694f2665c3e44e650f0e5dde','694f2665c3e44e650f0e5e12'),('694f2666c3e44e650f0e5f27','694f2665c3e44e650f0e5dde','694f2665c3e44e650f0e5e13'),('694f2666c3e44e650f0e5f28','694f2665c3e44e650f0e5dde','694f2665c3e44e650f0e5e14'),('694f2666c3e44e650f0e5f29','694f2665c3e44e650f0e5dde','694f2665c3e44e650f0e5e15'),('694f2666c3e44e650f0e5f2a','694f2665c3e44e650f0e5dde','694f2665c3e44e650f0e5e16'),('694f2666c3e44e650f0e5f2b','694f2665c3e44e650f0e5dde','694f2665c3e44e650f0e5e17'),('694f2666c3e44e650f0e5f2c','694f2665c3e44e650f0e5dde','694f2665c3e44e650f0e5e18'),('694f2666c3e44e650f0e5f2d','694f2665c3e44e650f0e5dde','694f2665c3e44e650f0e5e23'),('694f2666c3e44e650f0e5f2e','694f2665c3e44e650f0e5dde','694f2665c3e44e650f0e5e24'),('694f2666c3e44e650f0e5f2f','694f2665c3e44e650f0e5dde','694f2665c3e44e650f0e5e25'),('694f2666c3e44e650f0e5f30','694f2665c3e44e650f0e5dde','694f2665c3e44e650f0e5e26'),('694f2666c3e44e650f0e5f31','694f2665c3e44e650f0e5dde','694f2665c3e44e650f0e5e27'),('694f2666c3e44e650f0e5f32','694f2665c3e44e650f0e5dde','694f2665c3e44e650f0e5e28'),('694f2666c3e44e650f0e5f33','694f2665c3e44e650f0e5dde','694f2665c3e44e650f0e5e35'),('694f2666c3e44e650f0e5f34','694f2665c3e44e650f0e5dde','694f2665c3e44e650f0e5e36'),('694f2666c3e44e650f0e5f35','694f2665c3e44e650f0e5dde','694f2665c3e44e650f0e5e37'),('694f2666c3e44e650f0e5f36','694f2665c3e44e650f0e5dde','694f2665c3e44e650f0e5e38'),('694f2666c3e44e650f0e5f37','694f2665c3e44e650f0e5dde','694f2665c3e44e650f0e5e39'),('694f2666c3e44e650f0e5f38','694f2665c3e44e650f0e5dde','694f2665c3e44e650f0e5e3a'),('694f2666c3e44e650f0e5f39','694f2665c3e44e650f0e5dde','694f2665c3e44e650f0e5e3b'),('694f2666c3e44e650f0e5f3a','694f2665c3e44e650f0e5dde','694f2665c3e44e650f0e5e3c'),('694f2666c3e44e650f0e5f3b','694f2665c3e44e650f0e5dde','694f2665c3e44e650f0e5e3d'),('694f2666c3e44e650f0e5f3c','694f2665c3e44e650f0e5dde','694f2665c3e44e650f0e5e3e'),('694f2666c3e44e650f0e5f3d','694f2665c3e44e650f0e5dde','694f2665c3e44e650f0e5e30'),('694f2666c3e44e650f0e5f3e','694f2665c3e44e650f0e5dde','694f2665c3e44e650f0e5e31'),('694f2666c3e44e650f0e5f3f','694f2665c3e44e650f0e5dde','694f2665c3e44e650f0e5e32'),('694f2666c3e44e650f0e5f40','694f2665c3e44e650f0e5dde','694f2665c3e44e650f0e5e33'),('694f2666c3e44e650f0e5f41','694f2665c3e44e650f0e5dde','694f2665c3e44e650f0e5e34'),('694f2666c3e44e650f0e5f42','694f2665c3e44e650f0e5dde','694f2665c3e44e650f0e5e42'),('694f2666c3e44e650f0e5f43','694f2665c3e44e650f0e5dde','694f2665c3e44e650f0e5e43'),('694f2666c3e44e650f0e5f44','694f2665c3e44e650f0e5dde','694f2665c3e44e650f0e5e44'),('694f2666c3e44e650f0e5f45','694f2665c3e44e650f0e5dde','694f2665c3e44e650f0e5e45'),('694f2666c3e44e650f0e5f46','694f2665c3e44e650f0e5dde','694f2665c3e44e650f0e5e46'),('694f2666c3e44e650f0e5f47','694f2665c3e44e650f0e5dde','694f2665c3e44e650f0e5e29'),('694f2666c3e44e650f0e5f48','694f2665c3e44e650f0e5dde','694f2665c3e44e650f0e5e2a'),('694f2666c3e44e650f0e5f49','694f2665c3e44e650f0e5dde','694f2665c3e44e650f0e5e2b'),('694f2666c3e44e650f0e5f4a','694f2665c3e44e650f0e5dde','694f2665c3e44e650f0e5e2c'),('694f2666c3e44e650f0e5f4b','694f2665c3e44e650f0e5dde','694f2665c3e44e650f0e5e47'),('694f2666c3e44e650f0e5f4c','694f2665c3e44e650f0e5dde','694f2665c3e44e650f0e5e48'),('694f2666c3e44e650f0e5f4d','694f2665c3e44e650f0e5dde','694f2665c3e44e650f0e5e49'),('694f2666c3e44e650f0e5f4e','694f2665c3e44e650f0e5dde','694f2665c3e44e650f0e5e4a'),('694f2666c3e44e650f0e5f4f','694f2665c3e44e650f0e5dde','694f2665c3e44e650f0e5e4e'),('694f2666c3e44e650f0e5f50','694f2665c3e44e650f0e5dde','694f2665c3e44e650f0e5e4f'),('694f2666c3e44e650f0e5f51','694f2665c3e44e650f0e5dde','694f2665c3e44e650f0e5e50'),('694f2666c3e44e650f0e5f52','694f2665c3e44e650f0e5dde','694f2665c3e44e650f0e5e51'),('694f2666c3e44e650f0e5f53','694f2665c3e44e650f0e5dde','694f2665c3e44e650f0e5deb'),('694f2666c3e44e650f0e5f54','694f2665c3e44e650f0e5dde','694f2665c3e44e650f0e5e52'),('694f2666c3e44e650f0e5f55','694f2665c3e44e650f0e5dde','694f2665c3e44e650f0e5e53'),('694f2666c3e44e650f0e5f56','694f2665c3e44e650f0e5dde','694f2665c3e44e650f0e5e54'),('694f2666c3e44e650f0e5f57','694f2665c3e44e650f0e5dde','694f2665c3e44e650f0e5e55'),('694f2666c3e44e650f0e5f58','694f2665c3e44e650f0e5dde','694f2665c3e44e650f0e5e56'),('694f2666c3e44e650f0e5f59','694f2665c3e44e650f0e5dde','694f2665c3e44e650f0e5e57'),('694f2666c3e44e650f0e5f5a','694f2665c3e44e650f0e5dde','694f2665c3e44e650f0e5e58'),('694f2666c3e44e650f0e5f5b','694f2665c3e44e650f0e5dde','694f2665c3e44e650f0e5e59'),('694f2666c3e44e650f0e5f5c','694f2665c3e44e650f0e5dde','694f2665c3e44e650f0e5e5a'),('694f2666c3e44e650f0e5f5d','694f2665c3e44e650f0e5dde','694f2665c3e44e650f0e5e5b'),('694f2666c3e44e650f0e5f5e','694f2665c3e44e650f0e5dde','694f2665c3e44e650f0e5e5c'),('694f2666c3e44e650f0e5f5f','694f2665c3e44e650f0e5dde','694f2665c3e44e650f0e5e5d'),('694f2666c3e44e650f0e5f60','694f2665c3e44e650f0e5dde','694f2665c3e44e650f0e5e5e'),('694f2666c3e44e650f0e5f61','694f2665c3e44e650f0e5dde','694f2665c3e44e650f0e5e5f'),('694f2666c3e44e650f0e5f62','694f2665c3e44e650f0e5dde','694f2665c3e44e650f0e5e60'),('694f2666c3e44e650f0e5f63','694f2665c3e44e650f0e5dde','694f2665c3e44e650f0e5e61'),('694f2666c3e44e650f0e5f64','694f2665c3e44e650f0e5dde','694f2665c3e44e650f0e5e62'),('694f2666c3e44e650f0e5f65','694f2665c3e44e650f0e5dde','694f2665c3e44e650f0e5e63'),('694f2666c3e44e650f0e5f66','694f2665c3e44e650f0e5dde','694f2665c3e44e650f0e5e64'),('694f2666c3e44e650f0e5f67','694f2665c3e44e650f0e5dde','694f2665c3e44e650f0e5e65'),('694f2666c3e44e650f0e5f68','694f2665c3e44e650f0e5dde','694f2665c3e44e650f0e5e66'),('694f2666c3e44e650f0e5f69','694f2665c3e44e650f0e5dde','694f2665c3e44e650f0e5e67'),('694f2666c3e44e650f0e5f6a','694f2665c3e44e650f0e5dde','694f2665c3e44e650f0e5e3f'),('694f2666c3e44e650f0e5f6b','694f2665c3e44e650f0e5de3','694f2665c3e44e650f0e5df0'),('694f2666c3e44e650f0e5f6c','694f2665c3e44e650f0e5de3','694f2665c3e44e650f0e5df1'),('694f2666c3e44e650f0e5f6d','694f2665c3e44e650f0e5de3','694f2665c3e44e650f0e5df2'),('694f2666c3e44e650f0e5f6e','694f2665c3e44e650f0e5de3','694f2665c3e44e650f0e5df3'),('694f2666c3e44e650f0e5f6f','694f2665c3e44e650f0e5de3','694f2665c3e44e650f0e5df4'),('694f2666c3e44e650f0e5f70','694f2665c3e44e650f0e5de3','694f2665c3e44e650f0e5df5'),('694f2666c3e44e650f0e5f71','694f2665c3e44e650f0e5de3','694f2665c3e44e650f0e5df6'),('694f2666c3e44e650f0e5f72','694f2665c3e44e650f0e5de3','694f2665c3e44e650f0e5df7'),('694f2666c3e44e650f0e5f73','694f2665c3e44e650f0e5de3','694f2665c3e44e650f0e5e2e'),('694f2666c3e44e650f0e5f74','694f2665c3e44e650f0e5de3','694f2665c3e44e650f0e5df8'),('694f2666c3e44e650f0e5f75','694f2665c3e44e650f0e5de3','694f2665c3e44e650f0e5df9'),('694f2666c3e44e650f0e5f76','694f2665c3e44e650f0e5de3','694f2665c3e44e650f0e5dfb'),('694f2666c3e44e650f0e5f77','694f2665c3e44e650f0e5de3','694f2665c3e44e650f0e5dfc'),('694f2666c3e44e650f0e5f78','694f2665c3e44e650f0e5de3','694f2665c3e44e650f0e5dfd'),('694f2666c3e44e650f0e5f79','694f2665c3e44e650f0e5de3','694f2665c3e44e650f0e5dfe'),('694f2666c3e44e650f0e5f7a','694f2665c3e44e650f0e5de3','694f2665c3e44e650f0e5dff'),('694f2666c3e44e650f0e5f7b','694f2665c3e44e650f0e5de3','694f2665c3e44e650f0e5e00'),('694f2666c3e44e650f0e5f7c','694f2665c3e44e650f0e5de3','694f2665c3e44e650f0e5e08'),('694f2666c3e44e650f0e5f7d','694f2665c3e44e650f0e5de3','694f2665c3e44e650f0e5e09'),('694f2666c3e44e650f0e5f7e','694f2665c3e44e650f0e5de3','694f2665c3e44e650f0e5e0a'),('694f2666c3e44e650f0e5f7f','694f2665c3e44e650f0e5de3','694f2665c3e44e650f0e5e0b'),('694f2666c3e44e650f0e5f80','694f2665c3e44e650f0e5de3','694f2665c3e44e650f0e5e0c'),('694f2666c3e44e650f0e5f81','694f2665c3e44e650f0e5de3','694f2665c3e44e650f0e5e0d'),('694f2666c3e44e650f0e5f82','694f2665c3e44e650f0e5de3','694f2665c3e44e650f0e5e0e'),('694f2666c3e44e650f0e5f83','694f2665c3e44e650f0e5de3','694f2665c3e44e650f0e5e0f'),('694f2666c3e44e650f0e5f84','694f2665c3e44e650f0e5de3','694f2665c3e44e650f0e5e10'),('694f2666c3e44e650f0e5f85','694f2665c3e44e650f0e5de3','694f2665c3e44e650f0e5e11'),('694f2666c3e44e650f0e5f86','694f2665c3e44e650f0e5de3','694f2665c3e44e650f0e5e12'),('694f2666c3e44e650f0e5f87','694f2665c3e44e650f0e5de3','694f2665c3e44e650f0e5e13'),('694f2666c3e44e650f0e5f88','694f2665c3e44e650f0e5de3','694f2665c3e44e650f0e5e01'),('694f2666c3e44e650f0e5f89','694f2665c3e44e650f0e5de3','694f2665c3e44e650f0e5e04'),('694f2666c3e44e650f0e5f8a','694f2665c3e44e650f0e5de3','694f2665c3e44e650f0e5e30'),('694f2666c3e44e650f0e5f8b','694f2665c3e44e650f0e5de3','694f2665c3e44e650f0e5e31'),('694f2666c3e44e650f0e5f8c','694f2665c3e44e650f0e5de3','694f2665c3e44e650f0e5e32'),('694f2666c3e44e650f0e5f8d','694f2665c3e44e650f0e5de3','694f2665c3e44e650f0e5e33'),('694f2666c3e44e650f0e5f8e','694f2665c3e44e650f0e5de3','694f2665c3e44e650f0e5e34'),('694f2666c3e44e650f0e5f8f','694f2665c3e44e650f0e5de3','694f2665c3e44e650f0e5e42'),('694f2666c3e44e650f0e5f90','694f2665c3e44e650f0e5de3','694f2665c3e44e650f0e5e43'),('694f2666c3e44e650f0e5f91','694f2665c3e44e650f0e5de3','694f2665c3e44e650f0e5e44'),('694f2666c3e44e650f0e5f92','694f2665c3e44e650f0e5de3','694f2665c3e44e650f0e5e45'),('694f2666c3e44e650f0e5f93','694f2665c3e44e650f0e5de3','694f2665c3e44e650f0e5e46'),('694f2666c3e44e650f0e5f94','694f2665c3e44e650f0e5de3','694f2665c3e44e650f0e5e35'),('694f2666c3e44e650f0e5f95','694f2665c3e44e650f0e5de3','694f2665c3e44e650f0e5e36'),('694f2666c3e44e650f0e5f96','694f2665c3e44e650f0e5de3','694f2665c3e44e650f0e5e37'),('694f2666c3e44e650f0e5f97','694f2665c3e44e650f0e5de3','694f2665c3e44e650f0e5e38'),('694f2666c3e44e650f0e5f98','694f2665c3e44e650f0e5de3','694f2665c3e44e650f0e5e39'),('694f2666c3e44e650f0e5f99','694f2665c3e44e650f0e5de3','694f2665c3e44e650f0e5e29'),('694f2666c3e44e650f0e5f9a','694f2665c3e44e650f0e5de3','694f2665c3e44e650f0e5e2a'),('694f2666c3e44e650f0e5f9b','694f2665c3e44e650f0e5de3','694f2665c3e44e650f0e5e4e'),('694f2666c3e44e650f0e5f9c','694f2665c3e44e650f0e5de3','694f2665c3e44e650f0e5e4f'),('694f2666c3e44e650f0e5f9d','694f2665c3e44e650f0e5de3','694f2665c3e44e650f0e5e5e'),('694f2666c3e44e650f0e5f9e','694f2665c3e44e650f0e5de3','694f2665c3e44e650f0e5e5f'),('694f2666c3e44e650f0e5f9f','694f2665c3e44e650f0e5de3','694f2665c3e44e650f0e5e60'),('694f2666c3e44e650f0e5fa0','694f2665c3e44e650f0e5de3','694f2665c3e44e650f0e5e61'),('694f2666c3e44e650f0e5fa1','694f2665c3e44e650f0e5de3','694f2665c3e44e650f0e5e62'),('694f2666c3e44e650f0e5fa2','694f2665c3e44e650f0e5de3','694f2665c3e44e650f0e5e63'),('694f2666c3e44e650f0e5fa3','694f2665c3e44e650f0e5de3','694f2665c3e44e650f0e5e64'),('694f2666c3e44e650f0e5fa4','694f2665c3e44e650f0e5de3','694f2665c3e44e650f0e5e24'),('694f2666c3e44e650f0e5fa5','694f2665c3e44e650f0e5de3','694f2665c3e44e650f0e5e25'),('694f2666c3e44e650f0e5fa6','694f2665c3e44e650f0e5de3','694f2665c3e44e650f0e5e26'),('694f2666c3e44e650f0e5fa7','694f2665c3e44e650f0e5de3','694f2665c3e44e650f0e5e27'),('694f2666c3e44e650f0e5fa8','694f2665c3e44e650f0e5de3','694f2665c3e44e650f0e5e28'),('694f2666c3e44e650f0e5fa9','694f2665c3e44e650f0e5de3','694f2665c3e44e650f0e5e3f'),('694f2666c3e44e650f0e5faa','694f2665c3e44e650f0e5de3','694f2665c3e44e650f0e5e47'),('694f2666c3e44e650f0e5fab','694f2665c3e44e650f0e5de3','694f2665c3e44e650f0e5e48'),('694f2666c3e44e650f0e5fac','694f2665c3e44e650f0e5de3','694f2665c3e44e650f0e5e52'),('694f2666c3e44e650f0e5fad','694f2665c3e44e650f0e5de3','694f2665c3e44e650f0e5e53'),('694f2666c3e44e650f0e5fae','694f2665c3e44e650f0e5de3','694f2665c3e44e650f0e5e54'),('694f2666c3e44e650f0e5faf','694f2665c3e44e650f0e5de3','694f2665c3e44e650f0e5e55'),('694f2666c3e44e650f0e5fb0','694f2665c3e44e650f0e5de3','694f2665c3e44e650f0e5e56'),('694f2666c3e44e650f0e5fb1','694f2665c3e44e650f0e5de3','694f2665c3e44e650f0e5e57'),('694f2666c3e44e650f0e5fb2','694f2665c3e44e650f0e5de3','694f2665c3e44e650f0e5e58'),('694f2666c3e44e650f0e5fb3','694f2665c3e44e650f0e5de3','694f2665c3e44e650f0e5e59'),('694f2666c3e44e650f0e5fb4','694f2665c3e44e650f0e5de3','694f2665c3e44e650f0e5e5a'),('694f2666c3e44e650f0e5fb5','694f2665c3e44e650f0e5dda','694f2665c3e44e650f0e5df0'),('694f2666c3e44e650f0e5fb6','694f2665c3e44e650f0e5dda','694f2665c3e44e650f0e5df1'),('694f2666c3e44e650f0e5fb7','694f2665c3e44e650f0e5dda','694f2665c3e44e650f0e5df2'),('694f2666c3e44e650f0e5fb8','694f2665c3e44e650f0e5dda','694f2665c3e44e650f0e5df3'),('694f2666c3e44e650f0e5fb9','694f2665c3e44e650f0e5dda','694f2665c3e44e650f0e5df4'),('694f2666c3e44e650f0e5fba','694f2665c3e44e650f0e5dda','694f2665c3e44e650f0e5df5'),('694f2666c3e44e650f0e5fbb','694f2665c3e44e650f0e5dda','694f2665c3e44e650f0e5df6'),('694f2666c3e44e650f0e5fbc','694f2665c3e44e650f0e5dda','694f2665c3e44e650f0e5df7'),('694f2666c3e44e650f0e5fbd','694f2665c3e44e650f0e5dda','694f2665c3e44e650f0e5e2e'),('694f2666c3e44e650f0e5fbe','694f2665c3e44e650f0e5dda','694f2665c3e44e650f0e5df8'),('694f2666c3e44e650f0e5fbf','694f2665c3e44e650f0e5dda','694f2665c3e44e650f0e5df9'),('694f2666c3e44e650f0e5fc0','694f2665c3e44e650f0e5dda','694f2665c3e44e650f0e5dfb'),('694f2666c3e44e650f0e5fc1','694f2665c3e44e650f0e5dda','694f2665c3e44e650f0e5dfc'),('694f2666c3e44e650f0e5fc2','694f2665c3e44e650f0e5dda','694f2665c3e44e650f0e5dfd'),('694f2666c3e44e650f0e5fc3','694f2665c3e44e650f0e5dda','694f2665c3e44e650f0e5dfe'),('694f2666c3e44e650f0e5fc4','694f2665c3e44e650f0e5dda','694f2665c3e44e650f0e5dff'),('694f2666c3e44e650f0e5fc5','694f2665c3e44e650f0e5dda','694f2665c3e44e650f0e5e00'),('694f2666c3e44e650f0e5fc6','694f2665c3e44e650f0e5dda','694f2665c3e44e650f0e5e08'),('694f2666c3e44e650f0e5fc7','694f2665c3e44e650f0e5dda','694f2665c3e44e650f0e5e09'),('694f2666c3e44e650f0e5fc8','694f2665c3e44e650f0e5dda','694f2665c3e44e650f0e5e0a'),('694f2666c3e44e650f0e5fc9','694f2665c3e44e650f0e5dda','694f2665c3e44e650f0e5e0b'),('694f2666c3e44e650f0e5fca','694f2665c3e44e650f0e5dda','694f2665c3e44e650f0e5e0c'),('694f2666c3e44e650f0e5fcb','694f2665c3e44e650f0e5dda','694f2665c3e44e650f0e5e0d'),('694f2666c3e44e650f0e5fcc','694f2665c3e44e650f0e5dda','694f2665c3e44e650f0e5e0e'),('694f2666c3e44e650f0e5fcd','694f2665c3e44e650f0e5dda','694f2665c3e44e650f0e5e0f'),('694f2666c3e44e650f0e5fce','694f2665c3e44e650f0e5dda','694f2665c3e44e650f0e5e10'),('694f2666c3e44e650f0e5fcf','694f2665c3e44e650f0e5dda','694f2665c3e44e650f0e5e11'),('694f2666c3e44e650f0e5fd0','694f2665c3e44e650f0e5dda','694f2665c3e44e650f0e5e12'),('694f2666c3e44e650f0e5fd1','694f2665c3e44e650f0e5dda','694f2665c3e44e650f0e5e13'),('694f2666c3e44e650f0e5fd2','694f2665c3e44e650f0e5dda','694f2665c3e44e650f0e5e01'),('694f2666c3e44e650f0e5fd3','694f2665c3e44e650f0e5dda','694f2665c3e44e650f0e5e04'),('694f2666c3e44e650f0e5fd4','694f2665c3e44e650f0e5dda','694f2665c3e44e650f0e5e30'),('694f2666c3e44e650f0e5fd5','694f2665c3e44e650f0e5dda','694f2665c3e44e650f0e5e31'),('694f2666c3e44e650f0e5fd6','694f2665c3e44e650f0e5dda','694f2665c3e44e650f0e5e32'),('694f2666c3e44e650f0e5fd7','694f2665c3e44e650f0e5dda','694f2665c3e44e650f0e5e33'),('694f2666c3e44e650f0e5fd8','694f2665c3e44e650f0e5dda','694f2665c3e44e650f0e5e34'),('694f2666c3e44e650f0e5fd9','694f2665c3e44e650f0e5dda','694f2665c3e44e650f0e5e42'),('694f2666c3e44e650f0e5fda','694f2665c3e44e650f0e5dda','694f2665c3e44e650f0e5e43'),('694f2666c3e44e650f0e5fdb','694f2665c3e44e650f0e5dda','694f2665c3e44e650f0e5e44'),('694f2666c3e44e650f0e5fdc','694f2665c3e44e650f0e5dda','694f2665c3e44e650f0e5e45'),('694f2666c3e44e650f0e5fdd','694f2665c3e44e650f0e5dda','694f2665c3e44e650f0e5e46'),('694f2666c3e44e650f0e5fde','694f2665c3e44e650f0e5dda','694f2665c3e44e650f0e5e35'),('694f2666c3e44e650f0e5fdf','694f2665c3e44e650f0e5dda','694f2665c3e44e650f0e5e36'),('694f2666c3e44e650f0e5fe0','694f2665c3e44e650f0e5dda','694f2665c3e44e650f0e5e29'),('694f2666c3e44e650f0e5fe1','694f2665c3e44e650f0e5dda','694f2665c3e44e650f0e5e2a'),('694f2666c3e44e650f0e5fe2','694f2665c3e44e650f0e5dda','694f2665c3e44e650f0e5e4e'),('694f2666c3e44e650f0e5fe3','694f2665c3e44e650f0e5dda','694f2665c3e44e650f0e5e4f'),('694f2666c3e44e650f0e5fe4','694f2665c3e44e650f0e5dda','694f2665c3e44e650f0e5e5e'),('694f2666c3e44e650f0e5fe5','694f2665c3e44e650f0e5dda','694f2665c3e44e650f0e5e5f'),('694f2666c3e44e650f0e5fe6','694f2665c3e44e650f0e5dda','694f2665c3e44e650f0e5e60'),('694f2666c3e44e650f0e5fe7','694f2665c3e44e650f0e5dda','694f2665c3e44e650f0e5e61'),('694f2666c3e44e650f0e5fe8','694f2665c3e44e650f0e5dda','694f2665c3e44e650f0e5e62'),('694f2666c3e44e650f0e5fe9','694f2665c3e44e650f0e5dda','694f2665c3e44e650f0e5e63'),('694f2666c3e44e650f0e5fea','694f2665c3e44e650f0e5dda','694f2665c3e44e650f0e5e64'),('694f2666c3e44e650f0e5feb','694f2665c3e44e650f0e5ddb','694f2665c3e44e650f0e5df3'),('694f2666c3e44e650f0e5fec','694f2665c3e44e650f0e5ddb','694f2665c3e44e650f0e5df4'),('694f2666c3e44e650f0e5fed','694f2665c3e44e650f0e5ddb','694f2665c3e44e650f0e5df5'),('694f2666c3e44e650f0e5fee','694f2665c3e44e650f0e5ddb','694f2665c3e44e650f0e5df6'),('694f2666c3e44e650f0e5fef','694f2665c3e44e650f0e5ddb','694f2665c3e44e650f0e5df7'),('694f2666c3e44e650f0e5ff0','694f2665c3e44e650f0e5ddb','694f2665c3e44e650f0e5df8'),('694f2666c3e44e650f0e5ff1','694f2665c3e44e650f0e5ddb','694f2665c3e44e650f0e5df9'),('694f2666c3e44e650f0e5ff2','694f2665c3e44e650f0e5ddb','694f2665c3e44e650f0e5dfb'),('694f2666c3e44e650f0e5ff3','694f2665c3e44e650f0e5ddb','694f2665c3e44e650f0e5dfc'),('694f2666c3e44e650f0e5ff4','694f2665c3e44e650f0e5ddb','694f2665c3e44e650f0e5dfd'),('694f2666c3e44e650f0e5ff5','694f2665c3e44e650f0e5ddb','694f2665c3e44e650f0e5dff'),('694f2666c3e44e650f0e5ff6','694f2665c3e44e650f0e5ddb','694f2665c3e44e650f0e5e08'),('694f2666c3e44e650f0e5ff7','694f2665c3e44e650f0e5ddb','694f2665c3e44e650f0e5e09'),('694f2666c3e44e650f0e5ff8','694f2665c3e44e650f0e5ddb','694f2665c3e44e650f0e5e0e'),('694f2666c3e44e650f0e5ff9','694f2665c3e44e650f0e5ddb','694f2665c3e44e650f0e5e01'),('694f2666c3e44e650f0e5ffa','694f2665c3e44e650f0e5ddb','694f2665c3e44e650f0e5e04'),('694f2666c3e44e650f0e5ffb','694f2665c3e44e650f0e5ddb','694f2665c3e44e650f0e5e30'),('694f2666c3e44e650f0e5ffc','694f2665c3e44e650f0e5ddb','694f2665c3e44e650f0e5e33'),('694f2666c3e44e650f0e5ffd','694f2665c3e44e650f0e5ddb','694f2665c3e44e650f0e5e42'),('694f2666c3e44e650f0e5ffe','694f2665c3e44e650f0e5ddb','694f2665c3e44e650f0e5e43'),('694f2666c3e44e650f0e5fff','694f2665c3e44e650f0e5ddb','694f2665c3e44e650f0e5e35'),('694f2666c3e44e650f0e6000','694f2665c3e44e650f0e5ddb','694f2665c3e44e650f0e5e36'),('694f2666c3e44e650f0e6001','694f2665c3e44e650f0e5ddb','694f2665c3e44e650f0e5e29'),('694f2666c3e44e650f0e6002','694f2665c3e44e650f0e5ddb','694f2665c3e44e650f0e5e2a'),('694f2666c3e44e650f0e6003','694f2665c3e44e650f0e5ddb','694f2665c3e44e650f0e5e4e'),('694f2666c3e44e650f0e6004','694f2665c3e44e650f0e5ddb','694f2665c3e44e650f0e5e4f'),('694f2666c3e44e650f0e6005','694f2665c3e44e650f0e5ddb','694f2665c3e44e650f0e5e5e'),('694f2666c3e44e650f0e6006','694f2665c3e44e650f0e5ddb','694f2665c3e44e650f0e5e5f'),('694f2666c3e44e650f0e6007','694f2665c3e44e650f0e5ddb','694f2665c3e44e650f0e5e61'),('694f2666c3e44e650f0e6008','694f2665c3e44e650f0e5ddb','694f2665c3e44e650f0e5e63'),('694f2666c3e44e650f0e6009','694f2665c3e44e650f0e5ddb','694f2665c3e44e650f0e5e64'),('694f2666c3e44e650f0e600a','694f2665c3e44e650f0e5ddc','694f2665c3e44e650f0e5df3'),('694f2666c3e44e650f0e600b','694f2665c3e44e650f0e5ddc','694f2665c3e44e650f0e5df4'),('694f2666c3e44e650f0e600c','694f2665c3e44e650f0e5ddc','694f2665c3e44e650f0e5df5'),('694f2666c3e44e650f0e600d','694f2665c3e44e650f0e5ddc','694f2665c3e44e650f0e5df6'),('694f2666c3e44e650f0e600e','694f2665c3e44e650f0e5ddc','694f2665c3e44e650f0e5df7'),('694f2666c3e44e650f0e600f','694f2665c3e44e650f0e5ddc','694f2665c3e44e650f0e5df8'),('694f2666c3e44e650f0e6010','694f2665c3e44e650f0e5ddc','694f2665c3e44e650f0e5df9'),('694f2666c3e44e650f0e6011','694f2665c3e44e650f0e5ddc','694f2665c3e44e650f0e5dfb'),('694f2666c3e44e650f0e6012','694f2665c3e44e650f0e5ddc','694f2665c3e44e650f0e5dfc'),('694f2666c3e44e650f0e6013','694f2665c3e44e650f0e5ddc','694f2665c3e44e650f0e5dfd'),('694f2666c3e44e650f0e6014','694f2665c3e44e650f0e5ddc','694f2665c3e44e650f0e5e08'),('694f2666c3e44e650f0e6015','694f2665c3e44e650f0e5ddc','694f2665c3e44e650f0e5e09'),('694f2666c3e44e650f0e6016','694f2665c3e44e650f0e5ddc','694f2665c3e44e650f0e5e0e'),('694f2666c3e44e650f0e6017','694f2665c3e44e650f0e5ddc','694f2665c3e44e650f0e5e01'),('694f2666c3e44e650f0e6018','694f2665c3e44e650f0e5ddc','694f2665c3e44e650f0e5e30'),('694f2666c3e44e650f0e6019','694f2665c3e44e650f0e5ddc','694f2665c3e44e650f0e5e33'),('694f2666c3e44e650f0e601a','694f2665c3e44e650f0e5ddc','694f2665c3e44e650f0e5e42'),('694f2666c3e44e650f0e601b','694f2665c3e44e650f0e5ddc','694f2665c3e44e650f0e5e43'),('694f2666c3e44e650f0e601c','694f2665c3e44e650f0e5ddc','694f2665c3e44e650f0e5e5e'),('694f2666c3e44e650f0e601d','694f2665c3e44e650f0e5ddc','694f2665c3e44e650f0e5e5f'),('694f2666c3e44e650f0e601e','694f2665c3e44e650f0e5ddc','694f2665c3e44e650f0e5e63'),('694f2666c3e44e650f0e601f','694f2665c3e44e650f0e5ddc','694f2665c3e44e650f0e5e64');
/*!40000 ALTER TABLE `permissions_roles` ENABLE KEYS */;
UNLOCK TABLES;

--
-- Table structure for table `permissions_users`
--

DROP TABLE IF EXISTS `permissions_users`;
/*!40101 SET @saved_cs_client     = @@character_set_client */;
/*!50503 SET character_set_client = utf8mb4 */;
CREATE TABLE `permissions_users` (
  `id` varchar(24) NOT NULL,
  `user_id` varchar(24) NOT NULL,
  `permission_id` varchar(24) NOT NULL,
  PRIMARY KEY (`id`)
) ENGINE=InnoDB DEFAULT CHARSET=utf8mb4 COLLATE=utf8mb4_0900_ai_ci;
/*!40101 SET character_set_client = @saved_cs_client */;

--
-- Dumping data for table `permissions_users`
--

LOCK TABLES `permissions_users` WRITE;
/*!40000 ALTER TABLE `permissions_users` DISABLE KEYS */;
/*!40000 ALTER TABLE `permissions_users` ENABLE KEYS */;
UNLOCK TABLES;

--
-- Table structure for table `post_revisions`
--

DROP TABLE IF EXISTS `post_revisions`;
/*!40101 SET @saved_cs_client     = @@character_set_client */;
/*!50503 SET character_set_client = utf8mb4 */;
CREATE TABLE `post_revisions` (
  `id` varchar(24) NOT NULL,
  `post_id` varchar(24) NOT NULL,
  `lexical` longtext,
  `created_at_ts` bigint NOT NULL,
  `created_at` datetime NOT NULL,
  `author_id` varchar(24) DEFAULT NULL,
  `title` varchar(2000) DEFAULT NULL,
  `post_status` varchar(50) DEFAULT NULL,
  `reason` varchar(50) DEFAULT NULL,
  `feature_image` varchar(2000) DEFAULT NULL,
  `feature_image_alt` varchar(2000) DEFAULT NULL,
  `feature_image_caption` text,
  `custom_excerpt` varchar(2000) DEFAULT NULL,
  PRIMARY KEY (`id`),
  KEY `post_revisions_post_id_index` (`post_id`),
  KEY `post_revs_author_id_foreign` (`author_id`),
  CONSTRAINT `post_revs_author_id_foreign` FOREIGN KEY (`author_id`) REFERENCES `users` (`id`)
) ENGINE=InnoDB DEFAULT CHARSET=utf8mb4 COLLATE=utf8mb4_0900_ai_ci;
/*!40101 SET character_set_client = @saved_cs_client */;

--
-- Dumping data for table `post_revisions`
--

LOCK TABLES `post_revisions` WRITE;
/*!40000 ALTER TABLE `post_revisions` DISABLE KEYS */;
INSERT INTO `post_revisions` VALUES ('69503e1f19c94fae00e3736a','6950348e19c94fae00e37362','{\"root\":{\"children\":[{\"type\":\"markdown\",\"version\":1,\"markdown\":\"# Reduce Context Switching\\n\\n## Subtitle\\nContext switching overhead becomes a measurable performance bottleneck in high-throughput systems when too many threads or processes compete for limited CPU resources.\\n\\n---\\n\\n## Executive Summary (TL;DR)\\nContext switching happens when the operating system stops one thread and starts another. Each switch requires coordination work and often forces the CPU to repeat work it was already doing efficiently. When context switching happens too often, the CPU spends more time coordinating execution than doing useful work. Reducing unnecessary context switches improves throughput, lowers latency (especially worst-case latency), and makes performance more predictable. This optimization matters most in CPU-bound workloads where thread count exceeds CPU core count.\\n\\n---\\n\\n## Problem Context\\nA common misconception is that increasing the number of threads automatically improves performance. This leads to designs where a new thread is created for every request or task. While this can work for I/O-bound workloads (where threads mostly wait), it becomes harmful for CPU-bound workloads.\\n\\nA CPU has a fixed number of cores, and each core can only execute one thread at a time. When an application creates more threads than available cores, the operating system must constantly stop and resume threads so they can take turns using the CPU. This constant stopping and starting introduces overhead and reduces efficiency.\\n\\nIn real systems, this often appears as:\\n- High CPU usage but low throughput\\n- Requests becoming slower under load\\n- Performance degrading as more threads are added\\n\\nTypical mistakes include:\\n- Creating thousands of threads\\n- Blocking threads on I/O\\n- Misconfigured thread pools\\n- Ignoring CPU core limits\\n- Mixing CPU-bound and I/O-bound work in the same execution model\\n\\n---\\n\\n## How Context Switching Works\\nA **context switch** occurs when the operating system:\\n1. Stops the currently running thread\\n2. Saves its execution state\\n3. Loads the execution state of another thread\\n4. Resumes execution of that thread\\n\\nEven though modern systems are fast, this process still takes time. More importantly, it interrupts the natural flow of execution. When this interruption happens repeatedly, the CPU becomes less efficient because it cannot build momentum executing useful work.\\n\\n---\\n\\n## Voluntary vs Involuntary Context Switching\\n\\n### Voluntary Context Switching\\nOccurs when a thread cannot continue and explicitly stops:\\n- Waiting for disk or network I/O\\n- Waiting for a lock\\n- Calling `sleep` or `yield`\\n\\n### Involuntary Context Switching\\nOccurs when the operating system interrupts a running thread:\\n- To give CPU time to another thread\\n- To enforce fairness when many threads compete\\n\\n**Key idea**  \\nVoluntary switches come from waiting.  \\nInvoluntary switches come from too many runnable threads.\\n\\n---\\n\\n## Why This Becomes a Bottleneck\\n\\nContext switching becomes a bottleneck when threads are interrupted so frequently that useful work is broken into very small pieces.\\n\\nA CPU core can only run one thread at a time. If more threads want to run than there are cores, the operating system must constantly rotate between them. Each rotation introduces overhead.\\n\\nWhen the number of threads is small, this overhead is negligible. When the number grows, interruptions become frequent and expensive.\\n\\nFrequent interruption slows execution. Slower execution means threads stay active longer. Threads that stay active longer increase contention. Increased contention causes even more interruptions.\\n\\nBlocking behavior worsens the situation. Threads that wait for I/O or locks stop and later resume. Each stop and resume adds more switching and more coordination work.\\n\\nMovement of threads between CPU cores also hurts efficiency. When a thread resumes execution on a different core, it must re-establish execution context, which slows progress and increases the likelihood of further interruption.\\n\\nAll these effects reinforce each other:\\n- More threads  more interruptions  \\n- More interruptions  slower progress  \\n- Slower progress  more contention  \\n- More contention  even more interruptions  \\n\\nThis feedback loop explains why systems with excessive threading often show high CPU usage, low throughput, poor scalability, and unstable latency.\\n\\n---\\n\\n## Why Fewer Context Switches Improve Overall Performance\\n\\nReducing context switching improves performance not because context switches are inherently bad, but because **every context switch interrupts multiple layers of work that are already operating efficiently**. When switches happen too often, those layers never reach a stable, optimized state.\\n\\nBelow are the most important performance relationships affected by context switching, explained without diving into hardware internals.\\n\\n---\\n\\n### 1. Longer Uninterrupted Execution Means Less Rework\\n\\nWhen a thread runs continuously, it makes steady progress. When it is interrupted frequently, its work is fragmented.\\n\\nFragmented work often requires:\\n- Re-checking conditions\\n- Re-entering code paths\\n- Re-establishing execution flow\\n\\nEven on fast CPUs, restarting work repeatedly is slower than continuing smoothly.\\n\\n**Fewer context switches  longer uninterrupted execution  less repeated work**\\n\\n---\\n\\n### 2. Better Reuse of Data and State\\n\\nWhile a thread is running, the system naturally keeps the data it uses readily accessible. When execution switches to another thread, different data is accessed and the previous data becomes less immediately available.\\n\\nWhen the original thread resumes, accessing its data again takes longer than if it had continued running.\\n\\nYou dont need to know hardware details to see the effect:\\n- Continuous execution keeps data close\\n- Frequent switching pushes data farther away\\n\\n**Fewer context switches  better data reuse  faster execution**\\n\\n---\\n\\n### 3. Less Time Spent Coordinating, More Time Doing Work\\n\\nEvery context switch requires the operating system to:\\n- Decide which thread should run\\n- Stop one execution\\n- Start another execution\\n\\nThis coordination work is necessary, but it produces no business value. It does not process requests, compute results, or serve users.\\n\\nReducing context switches reduces this pure overhead.\\n\\n**Fewer context switches  less coordination  more CPU time for real work**\\n\\n---\\n\\n### 4. Fewer Artificial Execution Interruptions\\n\\nContext switches often interrupt threads that are actively making progress. These interruptions break the natural flow of execution and reduce efficiency.\\n\\nAllowing threads to run longer reduces artificial pauses and improves overall execution efficiency.\\n\\n**Fewer context switches  smoother execution  higher efficiency**\\n\\n---\\n\\n### 5. Faster Completion of Tasks\\n\\nWhen threads are interrupted frequently, tasks take longer to complete because they are repeatedly paused and resumed.\\n\\nLonger task durations create secondary problems:\\n- Resources are held longer\\n- More threads remain active\\n- System pressure increases\\n\\nReducing context switching shortens task lifetimes and lowers overall contention.\\n\\n**Fewer context switches  faster task completion  less system pressure**\\n\\n---\\n\\n### 6. More Predictable Performance and Latency\\n\\nFrequent context switching introduces randomness:\\n- Some threads are interrupted many times\\n- Others run longer\\n- Latency becomes inconsistent\\n\\nReducing context switching stabilizes execution timing and improves predictability, especially for worst-case latency.\\n\\n**Fewer context switches  less variability  more predictable performance**\\n\\n---\\n\\n### 7. Better Behavior Under Load\\n\\nUnder load, negative effects amplify:\\n- Slower execution increases contention\\n- Increased contention increases switching\\n- Increased switching slows execution further\\n\\nReducing context switching weakens this negative feedback loop and allows the system to degrade more gracefully under stress.\\n\\n**Fewer context switches  weaker feedback loops  better scalability**\\n\\n---\\n\\n## How to Reduce Context Switching\\n\\nReducing context switching means **reducing how often threads are stopped and restarted**. The goal is not to eliminate concurrency, but to avoid unnecessary interruptions.\\n\\n---\\n\\n### 1. Use Thread Pools Instead of Creating Threads\\n\\n**What goes wrong**  \\nCreating a new thread for each task quickly leads to a large number of threads that all want CPU time. Since the CPU can only run a limited number of threads at once, the operating system must constantly stop one thread and start another so they can take turns.\\n\\n**What changes**  \\nThread pools create a fixed number of threads ahead of time and reuse them for many tasks. New work waits in a queue instead of creating new threads.\\n\\n**Why it helps**  \\nBecause the number of threads is limited, fewer threads compete for the CPU at the same time. This reduces how often the operating system needs to interrupt threads, which directly reduces context switching.\\n\\n---\\n\\n### 2. Limit Concurrency to CPU Cores for CPU-Bound Work\\n\\n**What goes wrong**  \\nCPU-bound tasks always want to run and do not naturally pause. If there are more CPU-bound threads than CPU cores, the operating system must constantly rotate between them, interrupting each one even while it is making progress.\\n\\n**What changes**  \\nConcurrency is limited so that only as many CPU-bound tasks run at the same time as there are CPU cores.\\n\\n**Why it helps**  \\nWhen the number of running threads matches the number of cores, threads can run longer without being interrupted. Fewer forced interruptions mean fewer involuntary context switches.\\n\\n---\\n\\n### 3. Use Async / Non-Blocking I/O Instead of Blocking I/O\\n\\n**What goes wrong**  \\nWith blocking I/O, a thread starts an operation (like a network or disk call) and then stops doing anything while waiting for the result. That thread must later be restarted when the operation completes, causing additional context switches.\\n\\n**What changes**  \\nWith async I/O, the thread starts the operation and immediately returns. The operating system notifies the application later when the I/O is complete, without stopping a thread just to wait.\\n\\n**Why it helps**  \\nThreads are no longer stopped and restarted simply to wait for I/O. Fewer threads move between running and waiting states, which significantly reduces context switching.\\n\\n---\\n\\n### 4. Minimize Lock Contention\\n\\n**What goes wrong**  \\nWhen multiple threads need the same lock, some threads must wait. Waiting threads stop running and later resume when the lock becomes available. Each wait and resume introduces context switches.\\n\\n**What changes**  \\nReducing shared state, using finer-grained locks, or redesigning code to avoid locks reduces how often threads must wait.\\n\\n**Why it helps**  \\nWhen threads wait less, they are stopped and restarted less often. This directly reduces both voluntary and involuntary context switches.\\n\\n---\\n\\n### 5. Reduce Thread Migration Between CPU Cores\\n\\n**What goes wrong**  \\nThreads may resume execution on a different CPU core than the one they previously ran on. When this happens, execution becomes less efficient and threads take longer to complete their work.\\n\\n**What changes**  \\nKeeping threads on the same CPU core when possible improves execution continuity.\\n\\n**Why it helps**  \\nThreads that run more efficiently finish their work faster and spend less time competing for CPU time, which indirectly reduces context switching.\\n\\n---\\n\\n### 6. Avoid `sleep`, `yield`, and Busy Waiting\\n\\n**What goes wrong**  \\nCalling `sleep` or `yield` explicitly tells the operating system to stop running the thread, even if no real reason exists. Busy waiting repeatedly checks for work and causes unnecessary scheduling activity.\\n\\n**What changes**  \\nEvent-driven or signal-based mechanisms allow threads to stop only when real work is unavailable and resume exactly when needed.\\n\\n**Why it helps**  \\nThreads are no longer interrupted artificially. They stop and resume only when necessary, reducing unnecessary context switches.\\n\\n---\\n\\n### 7. Batch Work Instead of Scheduling Tiny Tasks\\n\\n**What goes wrong**  \\nScheduling many very small tasks causes frequent thread scheduling and frequent interruptions. The overhead of stopping and starting threads can become comparable to the work itself.\\n\\n**What changes**  \\nMultiple small tasks are grouped together and processed in a single execution.\\n\\n**Why it helps**  \\nThreads do more useful work each time they run, which reduces how often they need to be interrupted and rescheduled.\\n\\n---\\n\\n### 8. Use Work-Stealing Schedulers\\n\\n**What goes wrong**  \\nSome threads become idle while others are overloaded. Idle threads may block while busy threads continue to accumulate work.\\n\\n**What changes**  \\nWork-stealing allows idle threads to take work from busy threads instead of blocking or creating new threads.\\n\\n**Why it helps**  \\nThreads remain productive without increasing the total number of threads or causing additional stop-and-restart cycles, reducing context switching overall.\\n\\n---\\n\\n## Summary and Key Takeaways\\nContext switching becomes harmful when threads are interrupted too often. The CPU performs best when threads can run long enough to make meaningful progress.\\n\\nThe goal is not fewer threads, but **fewer unnecessary interruptions**.\\n\\n**Rule of thumb:**  \\nIf adding more threads makes performance worse, excessive context switching is often the reason.\\n\\n---\\n\\n## Example in C#\\n\\n```csharp\\n//  Bad: create many threads\\nfor (int i = 0; i < 1000; i++)\\n{\\n    new Thread(() => DoWork()).Start();\\n}\\n\\n//  Good: bounded concurrency\\nParallel.For(0, 1000, i => DoWork());\\n\\n//  Best for I/O-bound work\\nawait Task.WhenAll(\\n    Enumerable.Range(0, 1000)\\n        .Select(_ => DoWorkAsync())\\n);\\n\"}],\"direction\":null,\"format\":\"\",\"indent\":0,\"type\":\"root\",\"version\":1}}',1766866463038,'2025-12-27 20:14:23','694f2665c3e44e650f0e5dd8','Reduce Context Switching','published','initial_revision',NULL,NULL,NULL,NULL),('69503e2319c94fae00e3736c','6950348e19c94fae00e37362','{\"root\":{\"children\":[{\"type\":\"markdown\",\"version\":1,\"markdown\":\"# Reduce Context Switching\\n\\n## Subtitle\\nContext switching overhead becomes a measurable performance bottleneck in high-throughput systems when too many threads or processes compete for limited CPU resources.\\n\\n---\\n\\n## Executive Summary (TL;DR)\\nContext switching happens when the operating system stops one thread and starts another. Each switch requires coordination work and often forces the CPU to repeat work it was already doing efficiently. When context switching happens too often, the CPU spends more time coordinating execution than doing useful work. Reducing unnecessary context switches improves throughput, lowers latency (especially worst-case latency), and makes performance more predictable. This optimization matters most in CPU-bound workloads where thread count exceeds CPU core count.\\n\\n---\\n\\n## Problem Context\\nA common misconception is that increasing the number of threads automatically improves performance. This leads to designs where a new thread is created for every request or task. While this can work for I/O-bound workloads (where threads mostly wait), it becomes harmful for CPU-bound workloads.\\n\\nA CPU has a fixed number of cores, and each core can only execute one thread at a time. When an application creates more threads than available cores, the operating system must constantly stop and resume threads so they can take turns using the CPU. This constant stopping and starting introduces overhead and reduces efficiency.\\n\\nIn real systems, this often appears as:\\n- High CPU usage but low throughput\\n- Requests becoming slower under load\\n- Performance degrading as more threads are added\\n\\nTypical mistakes include:\\n- Creating thousands of threads\\n- Blocking threads on I/O\\n- Misconfigured thread pools\\n- Ignoring CPU core limits\\n- Mixing CPU-bound and I/O-bound work in the same execution model\\n\\n---\\n\\n## How Context Switching Works\\nA **context switch** occurs when the operating system:\\n1. Stops the currently running thread\\n2. Saves its execution state\\n3. Loads the execution state of another thread\\n4. Resumes execution of that thread\\n\\nEven though modern systems are fast, this process still takes time. More importantly, it interrupts the natural flow of execution. When this interruption happens repeatedly, the CPU becomes less efficient because it cannot build momentum executing useful work.\\n\\n---\\n\\n## Voluntary vs Involuntary Context Switching\\n\\n### Voluntary Context Switching\\nOccurs when a thread cannot continue and explicitly stops:\\n- Waiting for disk or network I/O\\n- Waiting for a lock\\n- Calling `sleep` or `yield`\\n\\n### Involuntary Context Switching\\nOccurs when the operating system interrupts a running thread:\\n- To give CPU time to another thread\\n- To enforce fairness when many threads compete\\n\\n**Key idea**  \\nVoluntary switches come from waiting.  \\nInvoluntary switches come from too many runnable threads.\\n\\n---\\n\\n## Why This Becomes a Bottleneck\\n\\nContext switching becomes a bottleneck when threads are interrupted so frequently that useful work is broken into very small pieces.\\n\\nA CPU core can only run one thread at a time. If more threads want to run than there are cores, the operating system must constantly rotate between them. Each rotation introduces overhead.\\n\\nWhen the number of threads is small, this overhead is negligible. When the number grows, interruptions become frequent and expensive.\\n\\nFrequent interruption slows execution. Slower execution means threads stay active longer. Threads that stay active longer increase contention. Increased contention causes even more interruptions.\\n\\nBlocking behavior worsens the situation. Threads that wait for I/O or locks stop and later resume. Each stop and resume adds more switching and more coordination work.\\n\\nMovement of threads between CPU cores also hurts efficiency. When a thread resumes execution on a different core, it must re-establish execution context, which slows progress and increases the likelihood of further interruption.\\n\\nAll these effects reinforce each other:\\n- More threads  more interruptions  \\n- More interruptions  slower progress  \\n- Slower progress  more contention  \\n- More contention  even more interruptions  \\n\\nThis feedback loop explains why systems with excessive threading often show high CPU usage, low throughput, poor scalability, and unstable latency.\\n\\n---\\n\\n## Why Fewer Context Switches Improve Overall Performance\\n\\nReducing context switching improves performance not because context switches are inherently bad, but because **every context switch interrupts multiple layers of work that are already operating efficiently**. When switches happen too often, those layers never reach a stable, optimized state.\\n\\nBelow are the most important performance relationships affected by context switching, explained without diving into hardware internals.\\n\\n---\\n\\n### 1. Longer Uninterrupted Execution Means Less Rework\\n\\nWhen a thread runs continuously, it makes steady progress. When it is interrupted frequently, its work is fragmented.\\n\\nFragmented work often requires:\\n- Re-checking conditions\\n- Re-entering code paths\\n- Re-establishing execution flow\\n\\nEven on fast CPUs, restarting work repeatedly is slower than continuing smoothly.\\n\\n**Fewer context switches  longer uninterrupted execution  less repeated work**\\n\\n---\\n\\n### 2. Better Reuse of Data and State\\n\\nWhile a thread is running, the system naturally keeps the data it uses readily accessible. When execution switches to another thread, different data is accessed and the previous data becomes less immediately available.\\n\\nWhen the original thread resumes, accessing its data again takes longer than if it had continued running.\\n\\nYou dont need to know hardware details to see the effect:\\n- Continuous execution keeps data close\\n- Frequent switching pushes data farther away\\n\\n**Fewer context switches  better data reuse  faster execution**\\n\\n---\\n\\n### 3. Less Time Spent Coordinating, More Time Doing Work\\n\\nEvery context switch requires the operating system to:\\n- Decide which thread should run\\n- Stop one execution\\n- Start another execution\\n\\nThis coordination work is necessary, but it produces no business value. It does not process requests, compute results, or serve users.\\n\\nReducing context switches reduces this pure overhead.\\n\\n**Fewer context switches  less coordination  more CPU time for real work**\\n\\n---\\n\\n### 4. Fewer Artificial Execution Interruptions\\n\\nContext switches often interrupt threads that are actively making progress. These interruptions break the natural flow of execution and reduce efficiency.\\n\\nAllowing threads to run longer reduces artificial pauses and improves overall execution efficiency.\\n\\n**Fewer context switches  smoother execution  higher efficiency**\\n\\n---\\n\\n### 5. Faster Completion of Tasks\\n\\nWhen threads are interrupted frequently, tasks take longer to complete because they are repeatedly paused and resumed.\\n\\nLonger task durations create secondary problems:\\n- Resources are held longer\\n- More threads remain active\\n- System pressure increases\\n\\nReducing context switching shortens task lifetimes and lowers overall contention.\\n\\n**Fewer context switches  faster task completion  less system pressure**\\n\\n---\\n\\n### 6. More Predictable Performance and Latency\\n\\nFrequent context switching introduces randomness:\\n- Some threads are interrupted many times\\n- Others run longer\\n- Latency becomes inconsistent\\n\\nReducing context switching stabilizes execution timing and improves predictability, especially for worst-case latency.\\n\\n**Fewer context switches  less variability  more predictable performance**\\n\\n---\\n\\n### 7. Better Behavior Under Load\\n\\nUnder load, negative effects amplify:\\n- Slower execution increases contention\\n- Increased contention increases switching\\n- Increased switching slows execution further\\n\\nReducing context switching weakens this negative feedback loop and allows the system to degrade more gracefully under stress.\\n\\n**Fewer context switches  weaker feedback loops  better scalability**\\n\\n---\\n\\n## How to Reduce Context Switching\\n\\nReducing context switching means **reducing how often threads are stopped and restarted**. The goal is not to eliminate concurrency, but to avoid unnecessary interruptions.\\n\\n---\\n\\n### 1. Use Thread Pools Instead of Creating Threads\\n\\n**What goes wrong**  \\nCreating a new thread for each task quickly leads to a large number of threads that all want CPU time. Since the CPU can only run a limited number of threads at once, the operating system must constantly stop one thread and start another so they can take turns.\\n\\n**What changes**  \\nThread pools create a fixed number of threads ahead of time and reuse them for many tasks. New work waits in a queue instead of creating new threads.\\n\\n**Why it helps**  \\nBecause the number of threads is limited, fewer threads compete for the CPU at the same time. This reduces how often the operating system needs to interrupt threads, which directly reduces context switching.\\n\\n---\\n\\n### 2. Limit Concurrency to CPU Cores for CPU-Bound Work\\n\\n**What goes wrong**  \\nCPU-bound tasks always want to run and do not naturally pause. If there are more CPU-bound threads than CPU cores, the operating system must constantly rotate between them, interrupting each one even while it is making progress.\\n\\n**What changes**  \\nConcurrency is limited so that only as many CPU-bound tasks run at the same time as there are CPU cores.\\n\\n**Why it helps**  \\nWhen the number of running threads matches the number of cores, threads can run longer without being interrupted. Fewer forced interruptions mean fewer involuntary context switches.\\n\\n---\\n\\n### 3. Use Async / Non-Blocking I/O Instead of Blocking I/O\\n\\n**What goes wrong**  \\nWith blocking I/O, a thread starts an operation (like a network or disk call) and then stops doing anything while waiting for the result. That thread must later be restarted when the operation completes, causing additional context switches.\\n\\n**What changes**  \\nWith async I/O, the thread starts the operation and immediately returns. The operating system notifies the application later when the I/O is complete, without stopping a thread just to wait.\\n\\n**Why it helps**  \\nThreads are no longer stopped and restarted simply to wait for I/O. Fewer threads move between running and waiting states, which significantly reduces context switching.\\n\\n---\\n\\n### 4. Minimize Lock Contention\\n\\n**What goes wrong**  \\nWhen multiple threads need the same lock, some threads must wait. Waiting threads stop running and later resume when the lock becomes available. Each wait and resume introduces context switches.\\n\\n**What changes**  \\nReducing shared state, using finer-grained locks, or redesigning code to avoid locks reduces how often threads must wait.\\n\\n**Why it helps**  \\nWhen threads wait less, they are stopped and restarted less often. This directly reduces both voluntary and involuntary context switches.\\n\\n---\\n\\n### 5. Reduce Thread Migration Between CPU Cores\\n\\n**What goes wrong**  \\nThreads may resume execution on a different CPU core than the one they previously ran on. When this happens, execution becomes less efficient and threads take longer to complete their work.\\n\\n**What changes**  \\nKeeping threads on the same CPU core when possible improves execution continuity.\\n\\n**Why it helps**  \\nThreads that run more efficiently finish their work faster and spend less time competing for CPU time, which indirectly reduces context switching.\\n\\n---\\n\\n### 6. Avoid `sleep`, `yield`, and Busy Waiting\\n\\n**What goes wrong**  \\nCalling `sleep` or `yield` explicitly tells the operating system to stop running the thread, even if no real reason exists. Busy waiting repeatedly checks for work and causes unnecessary scheduling activity.\\n\\n**What changes**  \\nEvent-driven or signal-based mechanisms allow threads to stop only when real work is unavailable and resume exactly when needed.\\n\\n**Why it helps**  \\nThreads are no longer interrupted artificially. They stop and resume only when necessary, reducing unnecessary context switches.\\n\\n---\\n\\n### 7. Batch Work Instead of Scheduling Tiny Tasks\\n\\n**What goes wrong**  \\nScheduling many very small tasks causes frequent thread scheduling and frequent interruptions. The overhead of stopping and starting threads can become comparable to the work itself.\\n\\n**What changes**  \\nMultiple small tasks are grouped together and processed in a single execution.\\n\\n**Why it helps**  \\nThreads do more useful work each time they run, which reduces how often they need to be interrupted and rescheduled.\\n\\n---\\n\\n### 8. Use Work-Stealing Schedulers\\n\\n**What goes wrong**  \\nSome threads become idle while others are overloaded. Idle threads may block while busy threads continue to accumulate work.\\n\\n**What changes**  \\nWork-stealing allows idle threads to take work from busy threads instead of blocking or creating new threads.\\n\\n**Why it helps**  \\nThreads remain productive without increasing the total number of threads or causing additional stop-and-restart cycles, reducing context switching overall.\\n\\n---\\n\\n## Summary and Key Takeaways\\nContext switching becomes harmful when threads are interrupted too often. The CPU performs best when threads can run long enough to make meaningful progress.\\n\\nThe goal is not fewer threads, but **fewer unnecessary interruptions**.\\n\\n**Rule of thumb:**  \\nIf adding more threads makes performance worse, excessive context switching is often the reason.\\n\\n---\\n\\n## Example in C#\\n\\n```csharp\\n//  Bad: create many threads\\nfor (int i = 0; i < 1000; i++)\\n{\\n    new Thread(() => DoWork()).Start();\\n}\\n\\n//  Good: bounded concurrency\\nParallel.For(0, 1000, i => DoWork());\\n\\n//  Best for I/O-bound work\\nawait Task.WhenAll(\\n    Enumerable.Range(0, 1000)\\n        .Select(_ => DoWorkAsync())\\n);\\n\"}],\"direction\":null,\"format\":\"\",\"indent\":0,\"type\":\"root\",\"version\":1}}',1766866467743,'2025-12-27 20:14:27','694f2665c3e44e650f0e5dd8','Reduce Context Switching','draft','unpublished',NULL,NULL,NULL,NULL),('69503e9419c94fae00e3736e','6950348e19c94fae00e37362','{\"root\":{\"children\":[{\"type\":\"markdown\",\"version\":1,\"markdown\":\"# Reduce Context Switching\\n\\n## Subtitle\\nContext switching overhead becomes a measurable performance bottleneck in high-throughput systems when too many threads or processes compete for limited CPU resources.\\n\\n---\\n\\n## Executive Summary (TL;DR)\\nContext switching happens when the operating system stops one thread and starts another. Each switch requires coordination work and often forces the CPU to repeat work it was already doing efficiently. When context switching happens too often, the CPU spends more time coordinating execution than doing useful work. Reducing unnecessary context switches improves throughput, lowers latency (especially worst-case latency), and makes performance more predictable. This optimization matters most in CPU-bound workloads where thread count exceeds CPU core count.\\n\\n---\\n\\n## Problem Context\\nA common misconception is that increasing the number of threads automatically improves performance. This leads to designs where a new thread is created for every request or task. While this can work for I/O-bound workloads (where threads mostly wait), it becomes harmful for CPU-bound workloads.\\n\\nA CPU has a fixed number of cores, and each core can only execute one thread at a time. When an application creates more threads than available cores, the operating system must constantly stop and resume threads so they can take turns using the CPU. This constant stopping and starting introduces overhead and reduces efficiency.\\n\\nIn real systems, this often appears as:\\n- High CPU usage but low throughput\\n- Requests becoming slower under load\\n- Performance degrading as more threads are added\\n\\nTypical mistakes include:\\n- Creating thousands of threads\\n- Blocking threads on I/O\\n- Misconfigured thread pools\\n- Ignoring CPU core limits\\n- Mixing CPU-bound and I/O-bound work in the same execution model\\n\\n---\\n\\n## How Context Switching Works\\nA **context switch** occurs when the operating system:\\n1. Stops the currently running thread\\n2. Saves its execution state\\n3. Loads the execution state of another thread\\n4. Resumes execution of that thread\\n\\nEven though modern systems are fast, this process still takes time. More importantly, it interrupts the natural flow of execution. When this interruption happens repeatedly, the CPU becomes less efficient because it cannot build momentum executing useful work.\\n\\n---\\n\\n## Voluntary vs Involuntary Context Switching\\n\\n### Voluntary Context Switching\\nOccurs when a thread cannot continue and explicitly stops:\\n- Waiting for disk or network I/O\\n- Waiting for a lock\\n- Calling `sleep` or `yield`\\n\\n### Involuntary Context Switching\\nOccurs when the operating system interrupts a running thread:\\n- To give CPU time to another thread\\n- To enforce fairness when many threads compete\\n\\n**Key idea**  \\nVoluntary switches come from waiting.  \\nInvoluntary switches come from too many runnable threads.\\n\\n---\\n\\n## Why This Becomes a Bottleneck\\n\\nContext switching becomes a bottleneck when threads are interrupted so frequently that useful work is broken into very small pieces.\\n\\nA CPU core can only run one thread at a time. If more threads want to run than there are cores, the operating system must constantly rotate between them. Each rotation introduces overhead.\\n\\nWhen the number of threads is small, this overhead is negligible. When the number grows, interruptions become frequent and expensive.\\n\\nFrequent interruption slows execution. Slower execution means threads stay active longer. Threads that stay active longer increase contention. Increased contention causes even more interruptions.\\n\\nBlocking behavior worsens the situation. Threads that wait for I/O or locks stop and later resume. Each stop and resume adds more switching and more coordination work.\\n\\nMovement of threads between CPU cores also hurts efficiency. When a thread resumes execution on a different core, it must re-establish execution context, which slows progress and increases the likelihood of further interruption.\\n\\nAll these effects reinforce each other:\\n- More threads  more interruptions  \\n- More interruptions  slower progress  \\n- Slower progress  more contention  \\n- More contention  even more interruptions  \\n\\nThis feedback loop explains why systems with excessive threading often show high CPU usage, low throughput, poor scalability, and unstable latency.\\n\\n---\\n\\n## Why Fewer Context Switches Improve Overall Performance\\n\\nReducing context switching improves performance not because context switches are inherently bad, but because **every context switch interrupts multiple layers of work that are already operating efficiently**. When switches happen too often, those layers never reach a stable, optimized state.\\n\\nBelow are the most important performance relationships affected by context switching, explained without diving into hardware internals.\\n\\n---\\n\\n### 1. Longer Uninterrupted Execution Means Less Rework\\n\\nWhen a thread runs continuously, it makes steady progress. When it is interrupted frequently, its work is fragmented.\\n\\nFragmented work often requires:\\n- Re-checking conditions\\n- Re-entering code paths\\n- Re-establishing execution flow\\n\\nEven on fast CPUs, restarting work repeatedly is slower than continuing smoothly.\\n\\n**Fewer context switches  longer uninterrupted execution  less repeated work**\\n\\n---\\n\\n### 2. Better Reuse of Data and State\\n\\nWhile a thread is running, the system naturally keeps the data it uses readily accessible. When execution switches to another thread, different data is accessed and the previous data becomes less immediately available.\\n\\nWhen the original thread resumes, accessing its data again takes longer than if it had continued running.\\n\\nYou dont need to know hardware details to see the effect:\\n- Continuous execution keeps data close\\n- Frequent switching pushes data farther away\\n\\n**Fewer context switches  better data reuse  faster execution**\\n\\n---\\n\\n### 3. Less Time Spent Coordinating, More Time Doing Work\\n\\nEvery context switch requires the operating system to:\\n- Decide which thread should run\\n- Stop one execution\\n- Start another execution\\n\\nThis coordination work is necessary, but it produces no business value. It does not process requests, compute results, or serve users.\\n\\nReducing context switches reduces this pure overhead.\\n\\n**Fewer context switches  less coordination  more CPU time for real work**\\n\\n---\\n\\n### 4. Fewer Artificial Execution Interruptions\\n\\nContext switches often interrupt threads that are actively making progress. These interruptions break the natural flow of execution and reduce efficiency.\\n\\nAllowing threads to run longer reduces artificial pauses and improves overall execution efficiency.\\n\\n**Fewer context switches  smoother execution  higher efficiency**\\n\\n---\\n\\n### 5. Faster Completion of Tasks\\n\\nWhen threads are interrupted frequently, tasks take longer to complete because they are repeatedly paused and resumed.\\n\\nLonger task durations create secondary problems:\\n- Resources are held longer\\n- More threads remain active\\n- System pressure increases\\n\\nReducing context switching shortens task lifetimes and lowers overall contention.\\n\\n**Fewer context switches  faster task completion  less system pressure**\\n\\n---\\n\\n### 6. More Predictable Performance and Latency\\n\\nFrequent context switching introduces randomness:\\n- Some threads are interrupted many times\\n- Others run longer\\n- Latency becomes inconsistent\\n\\nReducing context switching stabilizes execution timing and improves predictability, especially for worst-case latency.\\n\\n**Fewer context switches  less variability  more predictable performance**\\n\\n---\\n\\n### 7. Better Behavior Under Load\\n\\nUnder load, negative effects amplify:\\n- Slower execution increases contention\\n- Increased contention increases switching\\n- Increased switching slows execution further\\n\\nReducing context switching weakens this negative feedback loop and allows the system to degrade more gracefully under stress.\\n\\n**Fewer context switches  weaker feedback loops  better scalability**\\n\\n---\\n\\n## How to Reduce Context Switching\\n\\nReducing context switching means **reducing how often threads are stopped and restarted**. The goal is not to eliminate concurrency, but to avoid unnecessary interruptions.\\n\\n---\\n\\n### 1. Use Thread Pools Instead of Creating Threads\\n\\n**What goes wrong**  \\nCreating a new thread for each task quickly leads to a large number of threads that all want CPU time. Since the CPU can only run a limited number of threads at once, the operating system must constantly stop one thread and start another so they can take turns.\\n\\n**What changes**  \\nThread pools create a fixed number of threads ahead of time and reuse them for many tasks. New work waits in a queue instead of creating new threads.\\n\\n**Why it helps**  \\nBecause the number of threads is limited, fewer threads compete for the CPU at the same time. This reduces how often the operating system needs to interrupt threads, which directly reduces context switching.\\n\\n---\\n\\n### 2. Limit Concurrency to CPU Cores for CPU-Bound Work\\n\\n**What goes wrong**  \\nCPU-bound tasks always want to run and do not naturally pause. If there are more CPU-bound threads than CPU cores, the operating system must constantly rotate between them, interrupting each one even while it is making progress.\\n\\n**What changes**  \\nConcurrency is limited so that only as many CPU-bound tasks run at the same time as there are CPU cores.\\n\\n**Why it helps**  \\nWhen the number of running threads matches the number of cores, threads can run longer without being interrupted. Fewer forced interruptions mean fewer involuntary context switches.\\n\\n---\\n\\n### 3. Use Async / Non-Blocking I/O Instead of Blocking I/O\\n\\n**What goes wrong**  \\nWith blocking I/O, a thread starts an operation (like a network or disk call) and then stops doing anything while waiting for the result. That thread must later be restarted when the operation completes, causing additional context switches.\\n\\n**What changes**  \\nWith async I/O, the thread starts the operation and immediately returns. The operating system notifies the application later when the I/O is complete, without stopping a thread just to wait.\\n\\n**Why it helps**  \\nThreads are no longer stopped and restarted simply to wait for I/O. Fewer threads move between running and waiting states, which significantly reduces context switching.\\n\\n---\\n\\n### 4. Minimize Lock Contention\\n\\n**What goes wrong**  \\nWhen multiple threads need the same lock, some threads must wait. Waiting threads stop running and later resume when the lock becomes available. Each wait and resume introduces context switches.\\n\\n**What changes**  \\nReducing shared state, using finer-grained locks, or redesigning code to avoid locks reduces how often threads must wait.\\n\\n**Why it helps**  \\nWhen threads wait less, they are stopped and restarted less often. This directly reduces both voluntary and involuntary context switches.\\n\\n---\\n\\n### 5. Reduce Thread Migration Between CPU Cores\\n\\n**What goes wrong**  \\nThreads may resume execution on a different CPU core than the one they previously ran on. When this happens, execution becomes less efficient and threads take longer to complete their work.\\n\\n**What changes**  \\nKeeping threads on the same CPU core when possible improves execution continuity.\\n\\n**Why it helps**  \\nThreads that run more efficiently finish their work faster and spend less time competing for CPU time, which indirectly reduces context switching.\\n\\n---\\n\\n### 6. Avoid `sleep`, `yield`, and Busy Waiting\\n\\n**What goes wrong**  \\nCalling `sleep` or `yield` explicitly tells the operating system to stop running the thread, even if no real reason exists. Busy waiting repeatedly checks for work and causes unnecessary scheduling activity.\\n\\n**What changes**  \\nEvent-driven or signal-based mechanisms allow threads to stop only when real work is unavailable and resume exactly when needed.\\n\\n**Why it helps**  \\nThreads are no longer interrupted artificially. They stop and resume only when necessary, reducing unnecessary context switches.\\n\\n---\\n\\n### 7. Batch Work Instead of Scheduling Tiny Tasks\\n\\n**What goes wrong**  \\nScheduling many very small tasks causes frequent thread scheduling and frequent interruptions. The overhead of stopping and starting threads can become comparable to the work itself.\\n\\n**What changes**  \\nMultiple small tasks are grouped together and processed in a single execution.\\n\\n**Why it helps**  \\nThreads do more useful work each time they run, which reduces how often they need to be interrupted and rescheduled.\\n\\n---\\n\\n### 8. Use Work-Stealing Schedulers\\n\\n**What goes wrong**  \\nSome threads become idle while others are overloaded. Idle threads may block while busy threads continue to accumulate work.\\n\\n**What changes**  \\nWork-stealing allows idle threads to take work from busy threads instead of blocking or creating new threads.\\n\\n**Why it helps**  \\nThreads remain productive without increasing the total number of threads or causing additional stop-and-restart cycles, reducing context switching overall.\\n\\n---\\n\\n## Summary and Key Takeaways\\nContext switching becomes harmful when threads are interrupted too often. The CPU performs best when threads can run long enough to make meaningful progress.\\n\\nThe goal is not fewer threads, but **fewer unnecessary interruptions**.\\n\\n**Rule of thumb:**  \\nIf adding more threads makes performance worse, excessive context switching is often the reason.\\n\\n---\\n\\n## Example in C#\\n\\n```csharp\\n//  Bad: create many threads\\nfor (int i = 0; i < 1000; i++)\\n{\\n    new Thread(() => DoWork()).Start();\\n}\\n\\n//  Good: bounded concurrency\\nParallel.For(0, 1000, i => DoWork());\\n\\n//  Best for I/O-bound work\\nawait Task.WhenAll(\\n    Enumerable.Range(0, 1000)\\n        .Select(_ => DoWorkAsync())\\n);\\n\"}],\"direction\":null,\"format\":\"\",\"indent\":0,\"type\":\"root\",\"version\":1}}',1766866580244,'2025-12-27 20:16:20','694f2665c3e44e650f0e5dd8','Reduce Context Switching','published','published',NULL,NULL,NULL,NULL),('6951b07819c94fae00e373af','6951b07819c94fae00e373ad','{\"root\":{\"children\":[{\"children\":[],\"direction\":null,\"format\":\"\",\"indent\":0,\"type\":\"paragraph\",\"version\":1}],\"direction\":null,\"format\":\"\",\"indent\":0,\"type\":\"root\",\"version\":1}}',1766961272399,'2025-12-28 22:34:32','694f2665c3e44e650f0e5dd8','Privacy Policy','draft','initial_revision',NULL,NULL,NULL,NULL),('6951b0a919c94fae00e373b3','6951b07819c94fae00e373ad','{\"root\":{\"children\":[{\"children\":[{\"detail\":0,\"format\":0,\"mode\":\"normal\",\"style\":\"\",\"text\":\"Privacy Policy\",\"type\":\"extended-text\",\"version\":1}],\"direction\":\"ltr\",\"format\":\"\",\"indent\":0,\"type\":\"extended-heading\",\"version\":1,\"tag\":\"h1\"},{\"children\":[{\"detail\":0,\"format\":0,\"mode\":\"normal\",\"style\":\"\",\"text\":\"This website uses cookies and similar technologies to enhance user experience, analyze traffic, and display advertisements.\",\"type\":\"extended-text\",\"version\":1}],\"direction\":\"ltr\",\"format\":\"\",\"indent\":0,\"type\":\"paragraph\",\"version\":1},{\"children\":[{\"detail\":0,\"format\":0,\"mode\":\"normal\",\"style\":\"\",\"text\":\"Third-party vendors, including Google, use cookies to serve ads based on a user\'s prior visits to this website or other websites.\",\"type\":\"extended-text\",\"version\":1}],\"direction\":\"ltr\",\"format\":\"\",\"indent\":0,\"type\":\"paragraph\",\"version\":1},{\"children\":[{\"detail\":0,\"format\":0,\"mode\":\"normal\",\"style\":\"\",\"text\":\"Google\'s use of advertising cookies enables it and its partners to serve ads to users based on their visits to this site and/or other sites on the Internet.\",\"type\":\"extended-text\",\"version\":1}],\"direction\":\"ltr\",\"format\":\"\",\"indent\":0,\"type\":\"paragraph\",\"version\":1},{\"children\":[{\"detail\":0,\"format\":0,\"mode\":\"normal\",\"style\":\"\",\"text\":\"Users may opt out of personalized advertising by visiting:\",\"type\":\"extended-text\",\"version\":1},{\"type\":\"linebreak\",\"version\":1},{\"children\":[{\"detail\":0,\"format\":0,\"mode\":\"normal\",\"style\":\"\",\"text\":\"https://adssettings.google.com\",\"type\":\"extended-text\",\"version\":1}],\"direction\":\"ltr\",\"format\":\"\",\"indent\":0,\"type\":\"link\",\"version\":1,\"rel\":null,\"target\":null,\"title\":null,\"url\":\"https://adssettings.google.com\"}],\"direction\":\"ltr\",\"format\":\"\",\"indent\":0,\"type\":\"paragraph\",\"version\":1},{\"children\":[{\"detail\":0,\"format\":0,\"mode\":\"normal\",\"style\":\"\",\"text\":\"We do not sell, trade, or otherwise transfer your personally identifiable information to outside parties except as required by law.\",\"type\":\"extended-text\",\"version\":1}],\"direction\":\"ltr\",\"format\":\"\",\"indent\":0,\"type\":\"paragraph\",\"version\":1},{\"children\":[{\"detail\":0,\"format\":0,\"mode\":\"normal\",\"style\":\"\",\"text\":\"By using this website, you consent to this Privacy Policy.\",\"type\":\"extended-text\",\"version\":1}],\"direction\":\"ltr\",\"format\":\"\",\"indent\":0,\"type\":\"paragraph\",\"version\":1},{\"children\":[{\"detail\":0,\"format\":0,\"mode\":\"normal\",\"style\":\"\",\"text\":\"If you have any questions regarding this Privacy Policy, please contact us through the Contact page.\",\"type\":\"extended-text\",\"version\":1}],\"direction\":\"ltr\",\"format\":\"\",\"indent\":0,\"type\":\"paragraph\",\"version\":1}],\"direction\":\"ltr\",\"format\":\"\",\"indent\":0,\"type\":\"root\",\"version\":1}}',1766961321183,'2025-12-28 22:35:21','694f2665c3e44e650f0e5dd8','Privacy Policy','published','published',NULL,NULL,NULL,NULL),('6951b0ef19c94fae00e373b7','6951b0ef19c94fae00e373b5','{\"root\":{\"children\":[{\"children\":[],\"direction\":null,\"format\":\"\",\"indent\":0,\"type\":\"paragraph\",\"version\":1}],\"direction\":null,\"format\":\"\",\"indent\":0,\"type\":\"root\",\"version\":1}}',1766961391177,'2025-12-28 22:36:31','694f2665c3e44e650f0e5dd8','Terms of Service','draft','initial_revision',NULL,NULL,NULL,NULL),('6951b10019c94fae00e373bb','6951b0ef19c94fae00e373b5','{\"root\":{\"children\":[{\"children\":[{\"detail\":0,\"format\":0,\"mode\":\"normal\",\"style\":\"\",\"text\":\"Terms of Service\",\"type\":\"extended-text\",\"version\":1}],\"direction\":\"ltr\",\"format\":\"\",\"indent\":0,\"type\":\"extended-heading\",\"version\":1,\"tag\":\"h1\"},{\"children\":[{\"detail\":0,\"format\":0,\"mode\":\"normal\",\"style\":\"\",\"text\":\"By accessing and using this website, you accept and agree to be bound by the terms and provision of this agreement.\",\"type\":\"extended-text\",\"version\":1}],\"direction\":\"ltr\",\"format\":\"\",\"indent\":0,\"type\":\"paragraph\",\"version\":1},{\"children\":[{\"detail\":0,\"format\":0,\"mode\":\"normal\",\"style\":\"\",\"text\":\"The content provided on this website is for informational purposes only. We make no representations or warranties of any kind regarding the accuracy, reliability, or completeness of any information.\",\"type\":\"extended-text\",\"version\":1}],\"direction\":\"ltr\",\"format\":\"\",\"indent\":0,\"type\":\"paragraph\",\"version\":1},{\"children\":[{\"detail\":0,\"format\":0,\"mode\":\"normal\",\"style\":\"\",\"text\":\"We reserve the right to modify, suspend, or discontinue any part of the website at any time without notice.\",\"type\":\"extended-text\",\"version\":1}],\"direction\":\"ltr\",\"format\":\"\",\"indent\":0,\"type\":\"paragraph\",\"version\":1},{\"children\":[{\"detail\":0,\"format\":0,\"mode\":\"normal\",\"style\":\"\",\"text\":\"Your use of this website is at your own risk.\",\"type\":\"extended-text\",\"version\":1}],\"direction\":\"ltr\",\"format\":\"\",\"indent\":0,\"type\":\"paragraph\",\"version\":1},{\"children\":[{\"detail\":0,\"format\":0,\"mode\":\"normal\",\"style\":\"\",\"text\":\"These Terms of Service are subject to change without notice.\",\"type\":\"extended-text\",\"version\":1}],\"direction\":\"ltr\",\"format\":\"\",\"indent\":0,\"type\":\"paragraph\",\"version\":1}],\"direction\":\"ltr\",\"format\":\"\",\"indent\":0,\"type\":\"root\",\"version\":1}}',1766961408344,'2025-12-28 22:36:48','694f2665c3e44e650f0e5dd8','Terms of Service','published','published',NULL,NULL,NULL,NULL),('6951b1c119c94fae00e373bf','6951b1c119c94fae00e373bd','{\"root\":{\"children\":[{\"children\":[],\"direction\":null,\"format\":\"\",\"indent\":0,\"type\":\"paragraph\",\"version\":1}],\"direction\":null,\"format\":\"\",\"indent\":0,\"type\":\"root\",\"version\":1}}',1766961601325,'2025-12-28 22:40:01','694f2665c3e44e650f0e5dd8','Contact','draft','initial_revision',NULL,NULL,NULL,NULL),('6951b1d319c94fae00e373c4','6951b1c119c94fae00e373bd','{\"root\":{\"children\":[{\"children\":[{\"detail\":0,\"format\":0,\"mode\":\"normal\",\"style\":\"\",\"text\":\"Contact\",\"type\":\"extended-text\",\"version\":1}],\"direction\":\"ltr\",\"format\":\"\",\"indent\":0,\"type\":\"extended-heading\",\"version\":1,\"tag\":\"h1\"},{\"children\":[{\"detail\":0,\"format\":0,\"mode\":\"normal\",\"style\":\"\",\"text\":\"If you have any questions, feedback, or business inquiries, please contact us using the information below:\",\"type\":\"extended-text\",\"version\":1}],\"direction\":\"ltr\",\"format\":\"\",\"indent\":0,\"type\":\"paragraph\",\"version\":1},{\"children\":[{\"detail\":0,\"format\":1,\"mode\":\"normal\",\"style\":\"\",\"text\":\"Email:\",\"type\":\"extended-text\",\"version\":1},{\"detail\":0,\"format\":0,\"mode\":\"normal\",\"style\":\"\",\"text\":\" rdgztorres19@gmail.com\",\"type\":\"extended-text\",\"version\":1}],\"direction\":\"ltr\",\"format\":\"\",\"indent\":0,\"type\":\"paragraph\",\"version\":1},{\"children\":[{\"detail\":0,\"format\":0,\"mode\":\"normal\",\"style\":\"\",\"text\":\"We aim to respond within 2448 hours.\",\"type\":\"extended-text\",\"version\":1}],\"direction\":\"ltr\",\"format\":\"\",\"indent\":0,\"type\":\"paragraph\",\"version\":1}],\"direction\":\"ltr\",\"format\":\"\",\"indent\":0,\"type\":\"root\",\"version\":1}}',1766961619868,'2025-12-28 22:40:19','694f2665c3e44e650f0e5dd8','Contact','published','published',NULL,NULL,NULL,NULL),('6951c88119c94fae00e373c7','6951b07819c94fae00e373ad','{\"root\":{\"children\":[{\"children\":[{\"detail\":0,\"format\":0,\"mode\":\"normal\",\"style\":\"\",\"text\":\"This website uses cookies and similar technologies to enhance user experience, analyze traffic, and display advertisements.\",\"type\":\"extended-text\",\"version\":1}],\"direction\":\"ltr\",\"format\":\"\",\"indent\":0,\"type\":\"paragraph\",\"version\":1},{\"children\":[{\"detail\":0,\"format\":0,\"mode\":\"normal\",\"style\":\"\",\"text\":\"Third-party vendors, including Google, use cookies to serve ads based on a user\'s prior visits to this website or other websites.\",\"type\":\"extended-text\",\"version\":1}],\"direction\":\"ltr\",\"format\":\"\",\"indent\":0,\"type\":\"paragraph\",\"version\":1},{\"children\":[{\"detail\":0,\"format\":0,\"mode\":\"normal\",\"style\":\"\",\"text\":\"Google\'s use of advertising cookies enables it and its partners to serve ads to users based on their visits to this site and/or other sites on the Internet.\",\"type\":\"extended-text\",\"version\":1}],\"direction\":\"ltr\",\"format\":\"\",\"indent\":0,\"type\":\"paragraph\",\"version\":1},{\"children\":[{\"detail\":0,\"format\":0,\"mode\":\"normal\",\"style\":\"\",\"text\":\"Users may opt out of personalized advertising by visiting:\",\"type\":\"extended-text\",\"version\":1},{\"type\":\"linebreak\",\"version\":1},{\"children\":[{\"detail\":0,\"format\":0,\"mode\":\"normal\",\"style\":\"\",\"text\":\"https://adssettings.google.com\",\"type\":\"extended-text\",\"version\":1}],\"direction\":\"ltr\",\"format\":\"\",\"indent\":0,\"type\":\"link\",\"version\":1,\"rel\":null,\"target\":null,\"title\":null,\"url\":\"https://adssettings.google.com\"}],\"direction\":\"ltr\",\"format\":\"\",\"indent\":0,\"type\":\"paragraph\",\"version\":1},{\"children\":[{\"detail\":0,\"format\":0,\"mode\":\"normal\",\"style\":\"\",\"text\":\"We do not sell, trade, or otherwise transfer your personally identifiable information to outside parties except as required by law.\",\"type\":\"extended-text\",\"version\":1}],\"direction\":\"ltr\",\"format\":\"\",\"indent\":0,\"type\":\"paragraph\",\"version\":1},{\"children\":[{\"detail\":0,\"format\":0,\"mode\":\"normal\",\"style\":\"\",\"text\":\"By using this website, you consent to this Privacy Policy.\",\"type\":\"extended-text\",\"version\":1}],\"direction\":\"ltr\",\"format\":\"\",\"indent\":0,\"type\":\"paragraph\",\"version\":1},{\"children\":[{\"detail\":0,\"format\":0,\"mode\":\"normal\",\"style\":\"\",\"text\":\"If you have any questions regarding this Privacy Policy, please contact us through the Contact page.\",\"type\":\"extended-text\",\"version\":1}],\"direction\":\"ltr\",\"format\":\"\",\"indent\":0,\"type\":\"paragraph\",\"version\":1}],\"direction\":\"ltr\",\"format\":\"\",\"indent\":0,\"type\":\"root\",\"version\":1}}',1766967425337,'2025-12-29 00:17:05','694f2665c3e44e650f0e5dd8','Privacy Policy','published','published',NULL,NULL,NULL,NULL),('6951c89a19c94fae00e373c9','6951b0ef19c94fae00e373b5','{\"root\":{\"children\":[{\"children\":[{\"detail\":0,\"format\":0,\"mode\":\"normal\",\"style\":\"\",\"text\":\"By accessing and using this website, you accept and agree to be bound by the terms and provision of this agreement.\",\"type\":\"extended-text\",\"version\":1}],\"direction\":\"ltr\",\"format\":\"\",\"indent\":0,\"type\":\"paragraph\",\"version\":1},{\"children\":[{\"detail\":0,\"format\":0,\"mode\":\"normal\",\"style\":\"\",\"text\":\"The content provided on this website is for informational purposes only. We make no representations or warranties of any kind regarding the accuracy, reliability, or completeness of any information.\",\"type\":\"extended-text\",\"version\":1}],\"direction\":\"ltr\",\"format\":\"\",\"indent\":0,\"type\":\"paragraph\",\"version\":1},{\"children\":[{\"detail\":0,\"format\":0,\"mode\":\"normal\",\"style\":\"\",\"text\":\"We reserve the right to modify, suspend, or discontinue any part of the website at any time without notice.\",\"type\":\"extended-text\",\"version\":1}],\"direction\":\"ltr\",\"format\":\"\",\"indent\":0,\"type\":\"paragraph\",\"version\":1},{\"children\":[{\"detail\":0,\"format\":0,\"mode\":\"normal\",\"style\":\"\",\"text\":\"Your use of this website is at your own risk.\",\"type\":\"extended-text\",\"version\":1}],\"direction\":\"ltr\",\"format\":\"\",\"indent\":0,\"type\":\"paragraph\",\"version\":1},{\"children\":[{\"detail\":0,\"format\":0,\"mode\":\"normal\",\"style\":\"\",\"text\":\"These Terms of Service are subject to change without notice.\",\"type\":\"extended-text\",\"version\":1}],\"direction\":\"ltr\",\"format\":\"\",\"indent\":0,\"type\":\"paragraph\",\"version\":1}],\"direction\":\"ltr\",\"format\":\"\",\"indent\":0,\"type\":\"root\",\"version\":1}}',1766967450889,'2025-12-29 00:17:30','694f2665c3e44e650f0e5dd8','Terms of Service','published','published',NULL,NULL,NULL,NULL),('6951c94119c94fae00e373cc','6951b1c119c94fae00e373bd','{\"root\":{\"children\":[{\"children\":[{\"detail\":0,\"format\":0,\"mode\":\"normal\",\"style\":\"\",\"text\":\"Contact\",\"type\":\"extended-text\",\"version\":1}],\"direction\":\"ltr\",\"format\":\"\",\"indent\":0,\"type\":\"extended-heading\",\"version\":1,\"tag\":\"h1\"},{\"children\":[{\"detail\":0,\"format\":0,\"mode\":\"normal\",\"style\":\"\",\"text\":\"If you have any questions, feedback, or business inquiries, please contact us using the information below:\",\"type\":\"extended-text\",\"version\":1}],\"direction\":\"ltr\",\"format\":\"\",\"indent\":0,\"type\":\"paragraph\",\"version\":1},{\"children\":[{\"detail\":0,\"format\":1,\"mode\":\"normal\",\"style\":\"\",\"text\":\"Email:\",\"type\":\"extended-text\",\"version\":1},{\"detail\":0,\"format\":0,\"mode\":\"normal\",\"style\":\"\",\"text\":\" jackodes404@gmail.com\",\"type\":\"extended-text\",\"version\":1}],\"direction\":\"ltr\",\"format\":\"\",\"indent\":0,\"type\":\"paragraph\",\"version\":1},{\"children\":[{\"detail\":0,\"format\":0,\"mode\":\"normal\",\"style\":\"\",\"text\":\"We aim to respond within 2448 hours.\",\"type\":\"extended-text\",\"version\":1}],\"direction\":\"ltr\",\"format\":\"\",\"indent\":0,\"type\":\"paragraph\",\"version\":1}],\"direction\":\"ltr\",\"format\":\"\",\"indent\":0,\"type\":\"root\",\"version\":1}}',1766967617504,'2025-12-29 00:20:17','694f2665c3e44e650f0e5dd8','Contact','published','published',NULL,NULL,NULL,NULL),('69520b9b19c94fae00e373ce','6950348e19c94fae00e37362','{\"root\":{\"children\":[{\"type\":\"markdown\",\"version\":1,\"markdown\":\"# Reduce Context Switching\\n\\n## Subtitle\\nContext switching overhead becomes a measurable performance bottleneck in high-throughput systems when too many threads or processes compete for limited CPU resources.\\n\\n---\\n\\n## Executive Summary (TL;DR)\\nContext switching happens when the operating system stops one thread and starts another. Each switch requires coordination work and often forces the CPU to repeat work it was already doing efficiently. When context switching happens too often, the CPU spends more time coordinating execution than doing useful work. Reducing unnecessary context switches improves throughput, lowers latency (especially worst-case latency), and makes performance more predictable. This optimization matters most in CPU-bound workloads where thread count exceeds CPU core count.\\n\\n---\\n\\n## Problem Context\\nA common misconception is that increasing the number of threads automatically improves performance. This leads to designs where a new thread is created for every request or task. While this can work for I/O-bound workloads (where threads mostly wait), it becomes harmful for CPU-bound workloads.\\n\\nA CPU has a fixed number of cores, and each core can only execute one thread at a time. When an application creates more threads than available cores, the operating system must constantly stop and resume threads so they can take turns using the CPU. This constant stopping and starting introduces overhead and reduces efficiency.\\n\\nIn real systems, this often appears as:\\n- High CPU usage but low throughput\\n- Requests becoming slower under load\\n- Performance degrading as more threads are added\\n\\nTypical mistakes include:\\n- Creating thousands of threads\\n- Blocking threads on I/O\\n- Misconfigured thread pools\\n- Ignoring CPU core limits\\n- Mixing CPU-bound and I/O-bound work in the same execution model\\n\\n---\\n\\n## How Context Switching Works\\nA **context switch** occurs when the operating system:\\n1. Stops the currently running thread\\n2. Saves its execution state\\n3. Loads the execution state of another thread\\n4. Resumes execution of that thread\\n\\nEven though modern systems are fast, this process still takes time. More importantly, it interrupts the natural flow of execution. When this interruption happens repeatedly, the CPU becomes less efficient because it cannot build momentum executing useful work.\\n\\n---\\n\\n## Voluntary vs Involuntary Context Switching\\n\\n### Voluntary Context Switching\\nOccurs when a thread cannot continue and explicitly stops:\\n- Waiting for disk or network I/O\\n- Waiting for a lock\\n- Calling `sleep` or `yield`\\n\\n### Involuntary Context Switching\\nOccurs when the operating system interrupts a running thread:\\n- To give CPU time to another thread\\n- To enforce fairness when many threads compete\\n\\n**Key idea**  \\nVoluntary switches come from waiting.  \\nInvoluntary switches come from too many runnable threads.\\n\\n---\\n\\n## Why This Becomes a Bottleneck\\n\\nContext switching becomes a bottleneck when threads are interrupted so frequently that useful work is broken into very small pieces.\\n\\nA CPU core can only run one thread at a time. If more threads want to run than there are cores, the operating system must constantly rotate between them. Each rotation introduces overhead.\\n\\nWhen the number of threads is small, this overhead is negligible. When the number grows, interruptions become frequent and expensive.\\n\\nFrequent interruption slows execution. Slower execution means threads stay active longer. Threads that stay active longer increase contention. Increased contention causes even more interruptions.\\n\\nBlocking behavior worsens the situation. Threads that wait for I/O or locks stop and later resume. Each stop and resume adds more switching and more coordination work.\\n\\nMovement of threads between CPU cores also hurts efficiency. When a thread resumes execution on a different core, it must re-establish execution context, which slows progress and increases the likelihood of further interruption.\\n\\nAll these effects reinforce each other:\\n- More threads  more interruptions  \\n- More interruptions  slower progress  \\n- Slower progress  more contention  \\n- More contention  even more interruptions  \\n\\nThis feedback loop explains why systems with excessive threading often show high CPU usage, low throughput, poor scalability, and unstable latency.\\n\\n---\\n\\n---\\n\\n## Time Quantum and Scheduling Behavior\\n\\nA **time quantum** (also called *time slice*) is the amount of CPU time a runnable thread is allowed to execute before the operating system scheduler may preempt it and switch to another thread.\\n\\n### Is the time quantum fixed?\\nNo. The time quantum is **not a fixed, universal duration**.\\n\\nIts effective length varies depending on:\\n- The operating system scheduler implementation\\n- The scheduling policy in use\\n- The number of runnable threads in the run queue\\n- Thread priorities\\n- Overall system load\\n\\nModern schedulers do not aim to give each thread a fixed time slice. Instead, they aim for **fairness** across all runnable threads.\\n\\n### How the quantum behaves in practice\\n- When **few threads** are runnable, each thread runs longer without interruption.\\n- When **many threads** are runnable, the scheduler reduces how long each thread can run.\\n- As the run queue grows, uninterrupted execution time per thread shrinks.\\n\\nIn effect:\\n- More runnable threads  shorter execution windows\\n- Shorter execution windows  more preemption\\n- More preemption  more involuntary context switches\\n\\nThis is why oversubscribing the CPU with runnable threads directly increases context switching overhead.\\n\\n---\\n\\n## What Causes a Thread to Context Switch\\n\\nA thread context switch occurs whenever the operating system stops one thread and allows another to run. This can happen for several reasons.\\n\\n### 1. Time Quantum Expiration (Preemption)\\nIf a thread exhausts its allowed execution window, the scheduler may preempt it to give another runnable thread CPU time.\\n\\nThis is the most common cause of **involuntary context switching**, especially when:\\n- There are more runnable threads than CPU cores\\n- Threads are CPU-bound and do not block naturally\\n\\n---\\n\\n### 2. Blocking Operations (Voluntary Switching)\\nA thread voluntarily stops executing when it cannot continue:\\n- Waiting for disk or network I/O\\n- Waiting to acquire a lock\\n- Waiting on a condition variable\\n- Calling `sleep` or `yield`\\n\\nThe scheduler then switches to another runnable thread.\\n\\n---\\n\\n### 3. Lock Contention\\nWhen a thread attempts to acquire a lock that is already held:\\n- The thread blocks\\n- The scheduler switches to another thread\\n- The blocked thread later resumes when the lock becomes available\\n\\nHigh lock contention causes frequent stop-and-resume cycles and increases context switching.\\n\\n---\\n\\n### 4. Higher-Priority Threads Becoming Runnable\\nIf a higher-priority thread becomes runnable:\\n- The scheduler may immediately preempt the currently running thread\\n- A context switch occurs even if the current thread is making progress\\n\\n---\\n\\n### 5. Thread Migration Between CPU Cores\\nThreads may resume execution on a different CPU core than where they previously ran.\\n\\nWhile this does not always cause an immediate context switch by itself, it:\\n- Slows execution\\n- Increases execution time\\n- Increases the likelihood of further preemption\\n\\nIndirectly, this leads to more context switching.\\n\\n---\\n\\n### 6. Operating System Interrupts\\nHardware interrupts (network, disk, timers) temporarily stop user threads so the kernel can handle events.\\n\\nAfter interrupt handling completes, the scheduler decides which thread runs next, which may result in a context switch.\\n\\n---\\n\\n## Why Fewer Context Switches Improve Overall Performance\\n\\nReducing context switching improves performance not because context switches are inherently bad, but because **every context switch interrupts multiple layers of work that are already operating efficiently**. When switches happen too often, those layers never reach a stable, optimized state.\\n\\nBelow are the most important performance relationships affected by context switching, explained without diving into hardware internals.\\n\\n---\\n\\n### 1. Longer Uninterrupted Execution Means Less Rework\\n\\nWhen a thread runs continuously, it makes steady progress. When it is interrupted frequently, its work is fragmented.\\n\\nFragmented work often requires:\\n- Re-checking conditions\\n- Re-entering code paths\\n- Re-establishing execution flow\\n\\nEven on fast CPUs, restarting work repeatedly is slower than continuing smoothly.\\n\\n**Fewer context switches  longer uninterrupted execution  less repeated work**\\n\\n---\\n\\n### 2. Better Reuse of Data and State\\n\\nWhile a thread is running, the system naturally keeps the data it uses readily accessible. When execution switches to another thread, different data is accessed and the previous data becomes less immediately available.\\n\\nWhen the original thread resumes, accessing its data again takes longer than if it had continued running.\\n\\nYou dont need to know hardware details to see the effect:\\n- Continuous execution keeps data close\\n- Frequent switching pushes data farther away\\n\\n**Fewer context switches  better data reuse  faster execution**\\n\\n---\\n\\n### 3. Less Time Spent Coordinating, More Time Doing Work\\n\\nEvery context switch requires the operating system to:\\n- Decide which thread should run\\n- Stop one execution\\n- Start another execution\\n\\nThis coordination work is necessary, but it produces no business value. It does not process requests, compute results, or serve users.\\n\\nReducing context switches reduces this pure overhead.\\n\\n**Fewer context switches  less coordination  more CPU time for real work**\\n\\n---\\n\\n### 4. Fewer Artificial Execution Interruptions\\n\\nContext switches often interrupt threads that are actively making progress. These interruptions break the natural flow of execution and reduce efficiency.\\n\\nAllowing threads to run longer reduces artificial pauses and improves overall execution efficiency.\\n\\n**Fewer context switches  smoother execution  higher efficiency**\\n\\n---\\n\\n### 5. Faster Completion of Tasks\\n\\nWhen threads are interrupted frequently, tasks take longer to complete because they are repeatedly paused and resumed.\\n\\nLonger task durations create secondary problems:\\n- Resources are held longer\\n- More threads remain active\\n- System pressure increases\\n\\nReducing context switching shortens task lifetimes and lowers overall contention.\\n\\n**Fewer context switches  faster task completion  less system pressure**\\n\\n---\\n\\n### 6. More Predictable Performance and Latency\\n\\nFrequent context switching introduces randomness:\\n- Some threads are interrupted many times\\n- Others run longer\\n- Latency becomes inconsistent\\n\\nReducing context switching stabilizes execution timing and improves predictability, especially for worst-case latency.\\n\\n**Fewer context switches  less variability  more predictable performance**\\n\\n---\\n\\n### 7. Better Behavior Under Load\\n\\nUnder load, negative effects amplify:\\n- Slower execution increases contention\\n- Increased contention increases switching\\n- Increased switching slows execution further\\n\\nReducing context switching weakens this negative feedback loop and allows the system to degrade more gracefully under stress.\\n\\n**Fewer context switches  weaker feedback loops  better scalability**\\n\\n---\\n\\n## How to Reduce Context Switching\\n\\nReducing context switching means **reducing how often threads are stopped and restarted**. The goal is not to eliminate concurrency, but to avoid unnecessary interruptions.\\n\\n---\\n\\n### 1. Use Thread Pools Instead of Creating Threads\\n\\n**What goes wrong**  \\nCreating a new thread for each task quickly leads to a large number of threads that all want CPU time. Since the CPU can only run a limited number of threads at once, the operating system must constantly stop one thread and start another so they can take turns.\\n\\n**What changes**  \\nThread pools create a fixed number of threads ahead of time and reuse them for many tasks. New work waits in a queue instead of creating new threads.\\n\\n**Why it helps**  \\nBecause the number of threads is limited, fewer threads compete for the CPU at the same time. This reduces how often the operating system needs to interrupt threads, which directly reduces context switching.\\n\\n---\\n\\n### 2. Limit Concurrency to CPU Cores for CPU-Bound Work\\n\\n**What goes wrong**  \\nCPU-bound tasks always want to run and do not naturally pause. If there are more CPU-bound threads than CPU cores, the operating system must constantly rotate between them, interrupting each one even while it is making progress.\\n\\n**What changes**  \\nConcurrency is limited so that only as many CPU-bound tasks run at the same time as there are CPU cores.\\n\\n**Why it helps**  \\nWhen the number of running threads matches the number of cores, threads can run longer without being interrupted. Fewer forced interruptions mean fewer involuntary context switches.\\n\\n---\\n\\n### 3. Use Async / Non-Blocking I/O Instead of Blocking I/O\\n\\n**What goes wrong**  \\nWith blocking I/O, a thread starts an operation (like a network or disk call) and then stops doing anything while waiting for the result. That thread must later be restarted when the operation completes, causing additional context switches.\\n\\n**What changes**  \\nWith async I/O, the thread starts the operation and immediately returns. The operating system notifies the application later when the I/O is complete, without stopping a thread just to wait.\\n\\n**Why it helps**  \\nThreads are no longer stopped and restarted simply to wait for I/O. Fewer threads move between running and waiting states, which significantly reduces context switching.\\n\\n---\\n\\n### 4. Minimize Lock Contention\\n\\n**What goes wrong**  \\nWhen multiple threads need the same lock, some threads must wait. Waiting threads stop running and later resume when the lock becomes available. Each wait and resume introduces context switches.\\n\\n**What changes**  \\nReducing shared state, using finer-grained locks, or redesigning code to avoid locks reduces how often threads must wait.\\n\\n**Why it helps**  \\nWhen threads wait less, they are stopped and restarted less often. This directly reduces both voluntary and involuntary context switches.\\n\\n---\\n\\n### 5. Reduce Thread Migration Between CPU Cores\\n\\n**What goes wrong**  \\nThreads may resume execution on a different CPU core than the one they previously ran on. When this happens, execution becomes less efficient and threads take longer to complete their work.\\n\\n**What changes**  \\nKeeping threads on the same CPU core when possible improves execution continuity.\\n\\n**Why it helps**  \\nThreads that run more efficiently finish their work faster and spend less time competing for CPU time, which indirectly reduces context switching.\\n\\n---\\n\\n### 6. Avoid `sleep`, `yield`, and Busy Waiting\\n\\n**What goes wrong**  \\nCalling `sleep` or `yield` explicitly tells the operating system to stop running the thread, even if no real reason exists. Busy waiting repeatedly checks for work and causes unnecessary scheduling activity.\\n\\n**What changes**  \\nEvent-driven or signal-based mechanisms allow threads to stop only when real work is unavailable and resume exactly when needed.\\n\\n**Why it helps**  \\nThreads are no longer interrupted artificially. They stop and resume only when necessary, reducing unnecessary context switches.\\n\\n---\\n\\n### 7. Batch Work Instead of Scheduling Tiny Tasks\\n\\n**What goes wrong**  \\nScheduling many very small tasks causes frequent thread scheduling and frequent interruptions. The overhead of stopping and starting threads can become comparable to the work itself.\\n\\n**What changes**  \\nMultiple small tasks are grouped together and processed in a single execution.\\n\\n**Why it helps**  \\nThreads do more useful work each time they run, which reduces how often they need to be interrupted and rescheduled.\\n\\n---\\n\\n### 8. Use Work-Stealing Schedulers\\n\\n**What goes wrong**  \\nSome threads become idle while others are overloaded. Idle threads may block while busy threads continue to accumulate work.\\n\\n**What changes**  \\nWork-stealing allows idle threads to take work from busy threads instead of blocking or creating new threads.\\n\\n**Why it helps**  \\nThreads remain productive without increasing the total number of threads or causing additional stop-and-restart cycles, reducing context switching overall.\\n\\n---\\n\\n## Summary and Key Takeaways\\nContext switching becomes harmful when threads are interrupted too often. The CPU performs best when threads can run long enough to make meaningful progress.\\n\\nThe goal is not fewer threads, but **fewer unnecessary interruptions**.\\n\\n**Rule of thumb:**  \\nIf adding more threads makes performance worse, excessive context switching is often the reason.\\n\\n---\\n\\n## Example in C#\\n\\n```csharp\\n//  Bad: create many threads\\nfor (int i = 0; i < 1000; i++)\\n{\\n    new Thread(() => DoWork()).Start();\\n}\\n\\n//  Good: bounded concurrency\\nParallel.For(0, 1000, i => DoWork());\\n\\n//  Best for I/O-bound work\\nawait Task.WhenAll(\\n    Enumerable.Range(0, 1000)\\n        .Select(_ => DoWorkAsync())\\n);\\n\"}],\"direction\":null,\"format\":\"\",\"indent\":0,\"type\":\"root\",\"version\":1}}',1766984603640,'2025-12-29 05:03:23','694f2665c3e44e650f0e5dd8','Reduce Context Switching','published','published',NULL,NULL,NULL,NULL),('69520baa19c94fae00e373d0','6950348e19c94fae00e37362','{\"root\":{\"children\":[{\"type\":\"markdown\",\"version\":1,\"markdown\":\"## Subtitle\\nContext switching overhead becomes a measurable performance bottleneck in high-throughput systems when too many threads or processes compete for limited CPU resources.\\n\\n---\\n\\n## Executive Summary (TL;DR)\\nContext switching happens when the operating system stops one thread and starts another. Each switch requires coordination work and often forces the CPU to repeat work it was already doing efficiently. When context switching happens too often, the CPU spends more time coordinating execution than doing useful work. Reducing unnecessary context switches improves throughput, lowers latency (especially worst-case latency), and makes performance more predictable. This optimization matters most in CPU-bound workloads where thread count exceeds CPU core count.\\n\\n---\\n\\n## Problem Context\\nA common misconception is that increasing the number of threads automatically improves performance. This leads to designs where a new thread is created for every request or task. While this can work for I/O-bound workloads (where threads mostly wait), it becomes harmful for CPU-bound workloads.\\n\\nA CPU has a fixed number of cores, and each core can only execute one thread at a time. When an application creates more threads than available cores, the operating system must constantly stop and resume threads so they can take turns using the CPU. This constant stopping and starting introduces overhead and reduces efficiency.\\n\\nIn real systems, this often appears as:\\n- High CPU usage but low throughput\\n- Requests becoming slower under load\\n- Performance degrading as more threads are added\\n\\nTypical mistakes include:\\n- Creating thousands of threads\\n- Blocking threads on I/O\\n- Misconfigured thread pools\\n- Ignoring CPU core limits\\n- Mixing CPU-bound and I/O-bound work in the same execution model\\n\\n---\\n\\n## How Context Switching Works\\nA **context switch** occurs when the operating system:\\n1. Stops the currently running thread\\n2. Saves its execution state\\n3. Loads the execution state of another thread\\n4. Resumes execution of that thread\\n\\nEven though modern systems are fast, this process still takes time. More importantly, it interrupts the natural flow of execution. When this interruption happens repeatedly, the CPU becomes less efficient because it cannot build momentum executing useful work.\\n\\n---\\n\\n## Voluntary vs Involuntary Context Switching\\n\\n### Voluntary Context Switching\\nOccurs when a thread cannot continue and explicitly stops:\\n- Waiting for disk or network I/O\\n- Waiting for a lock\\n- Calling `sleep` or `yield`\\n\\n### Involuntary Context Switching\\nOccurs when the operating system interrupts a running thread:\\n- To give CPU time to another thread\\n- To enforce fairness when many threads compete\\n\\n**Key idea**  \\nVoluntary switches come from waiting.  \\nInvoluntary switches come from too many runnable threads.\\n\\n---\\n\\n## Why This Becomes a Bottleneck\\n\\nContext switching becomes a bottleneck when threads are interrupted so frequently that useful work is broken into very small pieces.\\n\\nA CPU core can only run one thread at a time. If more threads want to run than there are cores, the operating system must constantly rotate between them. Each rotation introduces overhead.\\n\\nWhen the number of threads is small, this overhead is negligible. When the number grows, interruptions become frequent and expensive.\\n\\nFrequent interruption slows execution. Slower execution means threads stay active longer. Threads that stay active longer increase contention. Increased contention causes even more interruptions.\\n\\nBlocking behavior worsens the situation. Threads that wait for I/O or locks stop and later resume. Each stop and resume adds more switching and more coordination work.\\n\\nMovement of threads between CPU cores also hurts efficiency. When a thread resumes execution on a different core, it must re-establish execution context, which slows progress and increases the likelihood of further interruption.\\n\\nAll these effects reinforce each other:\\n- More threads  more interruptions  \\n- More interruptions  slower progress  \\n- Slower progress  more contention  \\n- More contention  even more interruptions  \\n\\nThis feedback loop explains why systems with excessive threading often show high CPU usage, low throughput, poor scalability, and unstable latency.\\n\\n---\\n\\n---\\n\\n## Time Quantum and Scheduling Behavior\\n\\nA **time quantum** (also called *time slice*) is the amount of CPU time a runnable thread is allowed to execute before the operating system scheduler may preempt it and switch to another thread.\\n\\n### Is the time quantum fixed?\\nNo. The time quantum is **not a fixed, universal duration**.\\n\\nIts effective length varies depending on:\\n- The operating system scheduler implementation\\n- The scheduling policy in use\\n- The number of runnable threads in the run queue\\n- Thread priorities\\n- Overall system load\\n\\nModern schedulers do not aim to give each thread a fixed time slice. Instead, they aim for **fairness** across all runnable threads.\\n\\n### How the quantum behaves in practice\\n- When **few threads** are runnable, each thread runs longer without interruption.\\n- When **many threads** are runnable, the scheduler reduces how long each thread can run.\\n- As the run queue grows, uninterrupted execution time per thread shrinks.\\n\\nIn effect:\\n- More runnable threads  shorter execution windows\\n- Shorter execution windows  more preemption\\n- More preemption  more involuntary context switches\\n\\nThis is why oversubscribing the CPU with runnable threads directly increases context switching overhead.\\n\\n---\\n\\n## What Causes a Thread to Context Switch\\n\\nA thread context switch occurs whenever the operating system stops one thread and allows another to run. This can happen for several reasons.\\n\\n### 1. Time Quantum Expiration (Preemption)\\nIf a thread exhausts its allowed execution window, the scheduler may preempt it to give another runnable thread CPU time.\\n\\nThis is the most common cause of **involuntary context switching**, especially when:\\n- There are more runnable threads than CPU cores\\n- Threads are CPU-bound and do not block naturally\\n\\n---\\n\\n### 2. Blocking Operations (Voluntary Switching)\\nA thread voluntarily stops executing when it cannot continue:\\n- Waiting for disk or network I/O\\n- Waiting to acquire a lock\\n- Waiting on a condition variable\\n- Calling `sleep` or `yield`\\n\\nThe scheduler then switches to another runnable thread.\\n\\n---\\n\\n### 3. Lock Contention\\nWhen a thread attempts to acquire a lock that is already held:\\n- The thread blocks\\n- The scheduler switches to another thread\\n- The blocked thread later resumes when the lock becomes available\\n\\nHigh lock contention causes frequent stop-and-resume cycles and increases context switching.\\n\\n---\\n\\n### 4. Higher-Priority Threads Becoming Runnable\\nIf a higher-priority thread becomes runnable:\\n- The scheduler may immediately preempt the currently running thread\\n- A context switch occurs even if the current thread is making progress\\n\\n---\\n\\n### 5. Thread Migration Between CPU Cores\\nThreads may resume execution on a different CPU core than where they previously ran.\\n\\nWhile this does not always cause an immediate context switch by itself, it:\\n- Slows execution\\n- Increases execution time\\n- Increases the likelihood of further preemption\\n\\nIndirectly, this leads to more context switching.\\n\\n---\\n\\n### 6. Operating System Interrupts\\nHardware interrupts (network, disk, timers) temporarily stop user threads so the kernel can handle events.\\n\\nAfter interrupt handling completes, the scheduler decides which thread runs next, which may result in a context switch.\\n\\n---\\n\\n## Why Fewer Context Switches Improve Overall Performance\\n\\nReducing context switching improves performance not because context switches are inherently bad, but because **every context switch interrupts multiple layers of work that are already operating efficiently**. When switches happen too often, those layers never reach a stable, optimized state.\\n\\nBelow are the most important performance relationships affected by context switching, explained without diving into hardware internals.\\n\\n---\\n\\n### 1. Longer Uninterrupted Execution Means Less Rework\\n\\nWhen a thread runs continuously, it makes steady progress. When it is interrupted frequently, its work is fragmented.\\n\\nFragmented work often requires:\\n- Re-checking conditions\\n- Re-entering code paths\\n- Re-establishing execution flow\\n\\nEven on fast CPUs, restarting work repeatedly is slower than continuing smoothly.\\n\\n**Fewer context switches  longer uninterrupted execution  less repeated work**\\n\\n---\\n\\n### 2. Better Reuse of Data and State\\n\\nWhile a thread is running, the system naturally keeps the data it uses readily accessible. When execution switches to another thread, different data is accessed and the previous data becomes less immediately available.\\n\\nWhen the original thread resumes, accessing its data again takes longer than if it had continued running.\\n\\nYou dont need to know hardware details to see the effect:\\n- Continuous execution keeps data close\\n- Frequent switching pushes data farther away\\n\\n**Fewer context switches  better data reuse  faster execution**\\n\\n---\\n\\n### 3. Less Time Spent Coordinating, More Time Doing Work\\n\\nEvery context switch requires the operating system to:\\n- Decide which thread should run\\n- Stop one execution\\n- Start another execution\\n\\nThis coordination work is necessary, but it produces no business value. It does not process requests, compute results, or serve users.\\n\\nReducing context switches reduces this pure overhead.\\n\\n**Fewer context switches  less coordination  more CPU time for real work**\\n\\n---\\n\\n### 4. Fewer Artificial Execution Interruptions\\n\\nContext switches often interrupt threads that are actively making progress. These interruptions break the natural flow of execution and reduce efficiency.\\n\\nAllowing threads to run longer reduces artificial pauses and improves overall execution efficiency.\\n\\n**Fewer context switches  smoother execution  higher efficiency**\\n\\n---\\n\\n### 5. Faster Completion of Tasks\\n\\nWhen threads are interrupted frequently, tasks take longer to complete because they are repeatedly paused and resumed.\\n\\nLonger task durations create secondary problems:\\n- Resources are held longer\\n- More threads remain active\\n- System pressure increases\\n\\nReducing context switching shortens task lifetimes and lowers overall contention.\\n\\n**Fewer context switches  faster task completion  less system pressure**\\n\\n---\\n\\n### 6. More Predictable Performance and Latency\\n\\nFrequent context switching introduces randomness:\\n- Some threads are interrupted many times\\n- Others run longer\\n- Latency becomes inconsistent\\n\\nReducing context switching stabilizes execution timing and improves predictability, especially for worst-case latency.\\n\\n**Fewer context switches  less variability  more predictable performance**\\n\\n---\\n\\n### 7. Better Behavior Under Load\\n\\nUnder load, negative effects amplify:\\n- Slower execution increases contention\\n- Increased contention increases switching\\n- Increased switching slows execution further\\n\\nReducing context switching weakens this negative feedback loop and allows the system to degrade more gracefully under stress.\\n\\n**Fewer context switches  weaker feedback loops  better scalability**\\n\\n---\\n\\n## How to Reduce Context Switching\\n\\nReducing context switching means **reducing how often threads are stopped and restarted**. The goal is not to eliminate concurrency, but to avoid unnecessary interruptions.\\n\\n---\\n\\n### 1. Use Thread Pools Instead of Creating Threads\\n\\n**What goes wrong**  \\nCreating a new thread for each task quickly leads to a large number of threads that all want CPU time. Since the CPU can only run a limited number of threads at once, the operating system must constantly stop one thread and start another so they can take turns.\\n\\n**What changes**  \\nThread pools create a fixed number of threads ahead of time and reuse them for many tasks. New work waits in a queue instead of creating new threads.\\n\\n**Why it helps**  \\nBecause the number of threads is limited, fewer threads compete for the CPU at the same time. This reduces how often the operating system needs to interrupt threads, which directly reduces context switching.\\n\\n---\\n\\n### 2. Limit Concurrency to CPU Cores for CPU-Bound Work\\n\\n**What goes wrong**  \\nCPU-bound tasks always want to run and do not naturally pause. If there are more CPU-bound threads than CPU cores, the operating system must constantly rotate between them, interrupting each one even while it is making progress.\\n\\n**What changes**  \\nConcurrency is limited so that only as many CPU-bound tasks run at the same time as there are CPU cores.\\n\\n**Why it helps**  \\nWhen the number of running threads matches the number of cores, threads can run longer without being interrupted. Fewer forced interruptions mean fewer involuntary context switches.\\n\\n---\\n\\n### 3. Use Async / Non-Blocking I/O Instead of Blocking I/O\\n\\n**What goes wrong**  \\nWith blocking I/O, a thread starts an operation (like a network or disk call) and then stops doing anything while waiting for the result. That thread must later be restarted when the operation completes, causing additional context switches.\\n\\n**What changes**  \\nWith async I/O, the thread starts the operation and immediately returns. The operating system notifies the application later when the I/O is complete, without stopping a thread just to wait.\\n\\n**Why it helps**  \\nThreads are no longer stopped and restarted simply to wait for I/O. Fewer threads move between running and waiting states, which significantly reduces context switching.\\n\\n---\\n\\n### 4. Minimize Lock Contention\\n\\n**What goes wrong**  \\nWhen multiple threads need the same lock, some threads must wait. Waiting threads stop running and later resume when the lock becomes available. Each wait and resume introduces context switches.\\n\\n**What changes**  \\nReducing shared state, using finer-grained locks, or redesigning code to avoid locks reduces how often threads must wait.\\n\\n**Why it helps**  \\nWhen threads wait less, they are stopped and restarted less often. This directly reduces both voluntary and involuntary context switches.\\n\\n---\\n\\n### 5. Reduce Thread Migration Between CPU Cores\\n\\n**What goes wrong**  \\nThreads may resume execution on a different CPU core than the one they previously ran on. When this happens, execution becomes less efficient and threads take longer to complete their work.\\n\\n**What changes**  \\nKeeping threads on the same CPU core when possible improves execution continuity.\\n\\n**Why it helps**  \\nThreads that run more efficiently finish their work faster and spend less time competing for CPU time, which indirectly reduces context switching.\\n\\n---\\n\\n### 6. Avoid `sleep`, `yield`, and Busy Waiting\\n\\n**What goes wrong**  \\nCalling `sleep` or `yield` explicitly tells the operating system to stop running the thread, even if no real reason exists. Busy waiting repeatedly checks for work and causes unnecessary scheduling activity.\\n\\n**What changes**  \\nEvent-driven or signal-based mechanisms allow threads to stop only when real work is unavailable and resume exactly when needed.\\n\\n**Why it helps**  \\nThreads are no longer interrupted artificially. They stop and resume only when necessary, reducing unnecessary context switches.\\n\\n---\\n\\n### 7. Batch Work Instead of Scheduling Tiny Tasks\\n\\n**What goes wrong**  \\nScheduling many very small tasks causes frequent thread scheduling and frequent interruptions. The overhead of stopping and starting threads can become comparable to the work itself.\\n\\n**What changes**  \\nMultiple small tasks are grouped together and processed in a single execution.\\n\\n**Why it helps**  \\nThreads do more useful work each time they run, which reduces how often they need to be interrupted and rescheduled.\\n\\n---\\n\\n### 8. Use Work-Stealing Schedulers\\n\\n**What goes wrong**  \\nSome threads become idle while others are overloaded. Idle threads may block while busy threads continue to accumulate work.\\n\\n**What changes**  \\nWork-stealing allows idle threads to take work from busy threads instead of blocking or creating new threads.\\n\\n**Why it helps**  \\nThreads remain productive without increasing the total number of threads or causing additional stop-and-restart cycles, reducing context switching overall.\\n\\n---\\n\\n## Summary and Key Takeaways\\nContext switching becomes harmful when threads are interrupted too often. The CPU performs best when threads can run long enough to make meaningful progress.\\n\\nThe goal is not fewer threads, but **fewer unnecessary interruptions**.\\n\\n**Rule of thumb:**  \\nIf adding more threads makes performance worse, excessive context switching is often the reason.\\n\\n---\\n\\n## Example in C#\\n\\n```csharp\\n//  Bad: create many threads\\nfor (int i = 0; i < 1000; i++)\\n{\\n    new Thread(() => DoWork()).Start();\\n}\\n\\n//  Good: bounded concurrency\\nParallel.For(0, 1000, i => DoWork());\\n\\n//  Best for I/O-bound work\\nawait Task.WhenAll(\\n    Enumerable.Range(0, 1000)\\n        .Select(_ => DoWorkAsync())\\n);\\n\"}],\"direction\":null,\"format\":\"\",\"indent\":0,\"type\":\"root\",\"version\":1}}',1766984618868,'2025-12-29 05:03:38','694f2665c3e44e650f0e5dd8','Reduce Context Switching','published','published',NULL,NULL,NULL,NULL),('69520bc719c94fae00e373d2','6950348e19c94fae00e37362','{\"root\":{\"children\":[{\"type\":\"markdown\",\"version\":1,\"markdown\":\"## Subtitle\\nContext switching overhead becomes a measurable performance bottleneck in high-throughput systems when too many threads or processes compete for limited CPU resources.\\n\\n---\\n\\n## Executive Summary (TL;DR)\\nContext switching happens when the operating system stops one thread and starts another. Each switch requires coordination work and often forces the CPU to repeat work it was already doing efficiently. When context switching happens too often, the CPU spends more time coordinating execution than doing useful work. Reducing unnecessary context switches improves throughput, lowers latency (especially worst-case latency), and makes performance more predictable. This optimization matters most in CPU-bound workloads where thread count exceeds CPU core count.\\n\\n---\\n\\n## Problem Context\\nA common misconception is that increasing the number of threads automatically improves performance. This leads to designs where a new thread is created for every request or task. While this can work for I/O-bound workloads (where threads mostly wait), it becomes harmful for CPU-bound workloads.\\n\\nA CPU has a fixed number of cores, and each core can only execute one thread at a time. When an application creates more threads than available cores, the operating system must constantly stop and resume threads so they can take turns using the CPU. This constant stopping and starting introduces overhead and reduces efficiency.\\n\\nIn real systems, this often appears as:\\n- High CPU usage but low throughput\\n- Requests becoming slower under load\\n- Performance degrading as more threads are added\\n\\nTypical mistakes include:\\n- Creating thousands of threads\\n- Blocking threads on I/O\\n- Misconfigured thread pools\\n- Ignoring CPU core limits\\n- Mixing CPU-bound and I/O-bound work in the same execution model\\n\\n---\\n\\n## How Context Switching Works\\nA **context switch** occurs when the operating system:\\n1. Stops the currently running thread\\n2. Saves its execution state\\n3. Loads the execution state of another thread\\n4. Resumes execution of that thread\\n\\nEven though modern systems are fast, this process still takes time. More importantly, it interrupts the natural flow of execution. When this interruption happens repeatedly, the CPU becomes less efficient because it cannot build momentum executing useful work.\\n\\n---\\n\\n## Voluntary vs Involuntary Context Switching\\n\\n### Voluntary Context Switching\\nOccurs when a thread cannot continue and explicitly stops:\\n- Waiting for disk or network I/O\\n- Waiting for a lock\\n- Calling `sleep` or `yield`\\n\\n### Involuntary Context Switching\\nOccurs when the operating system interrupts a running thread:\\n- To give CPU time to another thread\\n- To enforce fairness when many threads compete\\n\\n**Key idea**  \\nVoluntary switches come from waiting.  \\nInvoluntary switches come from too many runnable threads.\\n\\n---\\n\\n## Why This Becomes a Bottleneck\\n\\nContext switching becomes a bottleneck when threads are interrupted so frequently that useful work is broken into very small pieces.\\n\\nA CPU core can only run one thread at a time. If more threads want to run than there are cores, the operating system must constantly rotate between them. Each rotation introduces overhead.\\n\\nWhen the number of threads is small, this overhead is negligible. When the number grows, interruptions become frequent and expensive.\\n\\nFrequent interruption slows execution. Slower execution means threads stay active longer. Threads that stay active longer increase contention. Increased contention causes even more interruptions.\\n\\nBlocking behavior worsens the situation. Threads that wait for I/O or locks stop and later resume. Each stop and resume adds more switching and more coordination work.\\n\\nMovement of threads between CPU cores also hurts efficiency. When a thread resumes execution on a different core, it must re-establish execution context, which slows progress and increases the likelihood of further interruption.\\n\\nAll these effects reinforce each other:\\n- More threads  more interruptions  \\n- More interruptions  slower progress  \\n- Slower progress  more contention  \\n- More contention  even more interruptions  \\n\\nThis feedback loop explains why systems with excessive threading often show high CPU usage, low throughput, poor scalability, and unstable latency.\\n\\n---\\n\\n## Time Quantum and Scheduling Behavior\\n\\nA **time quantum** (also called *time slice*) is the amount of CPU time a runnable thread is allowed to execute before the operating system scheduler may preempt it and switch to another thread.\\n\\n### Is the time quantum fixed?\\nNo. The time quantum is **not a fixed, universal duration**.\\n\\nIts effective length varies depending on:\\n- The operating system scheduler implementation\\n- The scheduling policy in use\\n- The number of runnable threads in the run queue\\n- Thread priorities\\n- Overall system load\\n\\nModern schedulers do not aim to give each thread a fixed time slice. Instead, they aim for **fairness** across all runnable threads.\\n\\n### How the quantum behaves in practice\\n- When **few threads** are runnable, each thread runs longer without interruption.\\n- When **many threads** are runnable, the scheduler reduces how long each thread can run.\\n- As the run queue grows, uninterrupted execution time per thread shrinks.\\n\\nIn effect:\\n- More runnable threads  shorter execution windows\\n- Shorter execution windows  more preemption\\n- More preemption  more involuntary context switches\\n\\nThis is why oversubscribing the CPU with runnable threads directly increases context switching overhead.\\n\\n---\\n\\n## What Causes a Thread to Context Switch\\n\\nA thread context switch occurs whenever the operating system stops one thread and allows another to run. This can happen for several reasons.\\n\\n### 1. Time Quantum Expiration (Preemption)\\nIf a thread exhausts its allowed execution window, the scheduler may preempt it to give another runnable thread CPU time.\\n\\nThis is the most common cause of **involuntary context switching**, especially when:\\n- There are more runnable threads than CPU cores\\n- Threads are CPU-bound and do not block naturally\\n\\n---\\n\\n### 2. Blocking Operations (Voluntary Switching)\\nA thread voluntarily stops executing when it cannot continue:\\n- Waiting for disk or network I/O\\n- Waiting to acquire a lock\\n- Waiting on a condition variable\\n- Calling `sleep` or `yield`\\n\\nThe scheduler then switches to another runnable thread.\\n\\n---\\n\\n### 3. Lock Contention\\nWhen a thread attempts to acquire a lock that is already held:\\n- The thread blocks\\n- The scheduler switches to another thread\\n- The blocked thread later resumes when the lock becomes available\\n\\nHigh lock contention causes frequent stop-and-resume cycles and increases context switching.\\n\\n---\\n\\n### 4. Higher-Priority Threads Becoming Runnable\\nIf a higher-priority thread becomes runnable:\\n- The scheduler may immediately preempt the currently running thread\\n- A context switch occurs even if the current thread is making progress\\n\\n---\\n\\n### 5. Thread Migration Between CPU Cores\\nThreads may resume execution on a different CPU core than where they previously ran.\\n\\nWhile this does not always cause an immediate context switch by itself, it:\\n- Slows execution\\n- Increases execution time\\n- Increases the likelihood of further preemption\\n\\nIndirectly, this leads to more context switching.\\n\\n---\\n\\n### 6. Operating System Interrupts\\nHardware interrupts (network, disk, timers) temporarily stop user threads so the kernel can handle events.\\n\\nAfter interrupt handling completes, the scheduler decides which thread runs next, which may result in a context switch.\\n\\n---\\n\\n## Why Fewer Context Switches Improve Overall Performance\\n\\nReducing context switching improves performance not because context switches are inherently bad, but because **every context switch interrupts multiple layers of work that are already operating efficiently**. When switches happen too often, those layers never reach a stable, optimized state.\\n\\nBelow are the most important performance relationships affected by context switching, explained without diving into hardware internals.\\n\\n---\\n\\n### 1. Longer Uninterrupted Execution Means Less Rework\\n\\nWhen a thread runs continuously, it makes steady progress. When it is interrupted frequently, its work is fragmented.\\n\\nFragmented work often requires:\\n- Re-checking conditions\\n- Re-entering code paths\\n- Re-establishing execution flow\\n\\nEven on fast CPUs, restarting work repeatedly is slower than continuing smoothly.\\n\\n**Fewer context switches  longer uninterrupted execution  less repeated work**\\n\\n---\\n\\n### 2. Better Reuse of Data and State\\n\\nWhile a thread is running, the system naturally keeps the data it uses readily accessible. When execution switches to another thread, different data is accessed and the previous data becomes less immediately available.\\n\\nWhen the original thread resumes, accessing its data again takes longer than if it had continued running.\\n\\nYou dont need to know hardware details to see the effect:\\n- Continuous execution keeps data close\\n- Frequent switching pushes data farther away\\n\\n**Fewer context switches  better data reuse  faster execution**\\n\\n---\\n\\n### 3. Less Time Spent Coordinating, More Time Doing Work\\n\\nEvery context switch requires the operating system to:\\n- Decide which thread should run\\n- Stop one execution\\n- Start another execution\\n\\nThis coordination work is necessary, but it produces no business value. It does not process requests, compute results, or serve users.\\n\\nReducing context switches reduces this pure overhead.\\n\\n**Fewer context switches  less coordination  more CPU time for real work**\\n\\n---\\n\\n### 4. Fewer Artificial Execution Interruptions\\n\\nContext switches often interrupt threads that are actively making progress. These interruptions break the natural flow of execution and reduce efficiency.\\n\\nAllowing threads to run longer reduces artificial pauses and improves overall execution efficiency.\\n\\n**Fewer context switches  smoother execution  higher efficiency**\\n\\n---\\n\\n### 5. Faster Completion of Tasks\\n\\nWhen threads are interrupted frequently, tasks take longer to complete because they are repeatedly paused and resumed.\\n\\nLonger task durations create secondary problems:\\n- Resources are held longer\\n- More threads remain active\\n- System pressure increases\\n\\nReducing context switching shortens task lifetimes and lowers overall contention.\\n\\n**Fewer context switches  faster task completion  less system pressure**\\n\\n---\\n\\n### 6. More Predictable Performance and Latency\\n\\nFrequent context switching introduces randomness:\\n- Some threads are interrupted many times\\n- Others run longer\\n- Latency becomes inconsistent\\n\\nReducing context switching stabilizes execution timing and improves predictability, especially for worst-case latency.\\n\\n**Fewer context switches  less variability  more predictable performance**\\n\\n---\\n\\n### 7. Better Behavior Under Load\\n\\nUnder load, negative effects amplify:\\n- Slower execution increases contention\\n- Increased contention increases switching\\n- Increased switching slows execution further\\n\\nReducing context switching weakens this negative feedback loop and allows the system to degrade more gracefully under stress.\\n\\n**Fewer context switches  weaker feedback loops  better scalability**\\n\\n---\\n\\n## How to Reduce Context Switching\\n\\nReducing context switching means **reducing how often threads are stopped and restarted**. The goal is not to eliminate concurrency, but to avoid unnecessary interruptions.\\n\\n---\\n\\n### 1. Use Thread Pools Instead of Creating Threads\\n\\n**What goes wrong**  \\nCreating a new thread for each task quickly leads to a large number of threads that all want CPU time. Since the CPU can only run a limited number of threads at once, the operating system must constantly stop one thread and start another so they can take turns.\\n\\n**What changes**  \\nThread pools create a fixed number of threads ahead of time and reuse them for many tasks. New work waits in a queue instead of creating new threads.\\n\\n**Why it helps**  \\nBecause the number of threads is limited, fewer threads compete for the CPU at the same time. This reduces how often the operating system needs to interrupt threads, which directly reduces context switching.\\n\\n---\\n\\n### 2. Limit Concurrency to CPU Cores for CPU-Bound Work\\n\\n**What goes wrong**  \\nCPU-bound tasks always want to run and do not naturally pause. If there are more CPU-bound threads than CPU cores, the operating system must constantly rotate between them, interrupting each one even while it is making progress.\\n\\n**What changes**  \\nConcurrency is limited so that only as many CPU-bound tasks run at the same time as there are CPU cores.\\n\\n**Why it helps**  \\nWhen the number of running threads matches the number of cores, threads can run longer without being interrupted. Fewer forced interruptions mean fewer involuntary context switches.\\n\\n---\\n\\n### 3. Use Async / Non-Blocking I/O Instead of Blocking I/O\\n\\n**What goes wrong**  \\nWith blocking I/O, a thread starts an operation (like a network or disk call) and then stops doing anything while waiting for the result. That thread must later be restarted when the operation completes, causing additional context switches.\\n\\n**What changes**  \\nWith async I/O, the thread starts the operation and immediately returns. The operating system notifies the application later when the I/O is complete, without stopping a thread just to wait.\\n\\n**Why it helps**  \\nThreads are no longer stopped and restarted simply to wait for I/O. Fewer threads move between running and waiting states, which significantly reduces context switching.\\n\\n---\\n\\n### 4. Minimize Lock Contention\\n\\n**What goes wrong**  \\nWhen multiple threads need the same lock, some threads must wait. Waiting threads stop running and later resume when the lock becomes available. Each wait and resume introduces context switches.\\n\\n**What changes**  \\nReducing shared state, using finer-grained locks, or redesigning code to avoid locks reduces how often threads must wait.\\n\\n**Why it helps**  \\nWhen threads wait less, they are stopped and restarted less often. This directly reduces both voluntary and involuntary context switches.\\n\\n---\\n\\n### 5. Reduce Thread Migration Between CPU Cores\\n\\n**What goes wrong**  \\nThreads may resume execution on a different CPU core than the one they previously ran on. When this happens, execution becomes less efficient and threads take longer to complete their work.\\n\\n**What changes**  \\nKeeping threads on the same CPU core when possible improves execution continuity.\\n\\n**Why it helps**  \\nThreads that run more efficiently finish their work faster and spend less time competing for CPU time, which indirectly reduces context switching.\\n\\n---\\n\\n### 6. Avoid `sleep`, `yield`, and Busy Waiting\\n\\n**What goes wrong**  \\nCalling `sleep` or `yield` explicitly tells the operating system to stop running the thread, even if no real reason exists. Busy waiting repeatedly checks for work and causes unnecessary scheduling activity.\\n\\n**What changes**  \\nEvent-driven or signal-based mechanisms allow threads to stop only when real work is unavailable and resume exactly when needed.\\n\\n**Why it helps**  \\nThreads are no longer interrupted artificially. They stop and resume only when necessary, reducing unnecessary context switches.\\n\\n---\\n\\n### 7. Batch Work Instead of Scheduling Tiny Tasks\\n\\n**What goes wrong**  \\nScheduling many very small tasks causes frequent thread scheduling and frequent interruptions. The overhead of stopping and starting threads can become comparable to the work itself.\\n\\n**What changes**  \\nMultiple small tasks are grouped together and processed in a single execution.\\n\\n**Why it helps**  \\nThreads do more useful work each time they run, which reduces how often they need to be interrupted and rescheduled.\\n\\n---\\n\\n### 8. Use Work-Stealing Schedulers\\n\\n**What goes wrong**  \\nSome threads become idle while others are overloaded. Idle threads may block while busy threads continue to accumulate work.\\n\\n**What changes**  \\nWork-stealing allows idle threads to take work from busy threads instead of blocking or creating new threads.\\n\\n**Why it helps**  \\nThreads remain productive without increasing the total number of threads or causing additional stop-and-restart cycles, reducing context switching overall.\\n\\n---\\n\\n## Summary and Key Takeaways\\nContext switching becomes harmful when threads are interrupted too often. The CPU performs best when threads can run long enough to make meaningful progress.\\n\\nThe goal is not fewer threads, but **fewer unnecessary interruptions**.\\n\\n**Rule of thumb:**  \\nIf adding more threads makes performance worse, excessive context switching is often the reason.\\n\\n---\\n\\n## Example in C#\\n\\n```csharp\\n//  Bad: create many threads\\nfor (int i = 0; i < 1000; i++)\\n{\\n    new Thread(() => DoWork()).Start();\\n}\\n\\n//  Good: bounded concurrency\\nParallel.For(0, 1000, i => DoWork());\\n\\n//  Best for I/O-bound work\\nawait Task.WhenAll(\\n    Enumerable.Range(0, 1000)\\n        .Select(_ => DoWorkAsync())\\n);\\n\"}],\"direction\":null,\"format\":\"\",\"indent\":0,\"type\":\"root\",\"version\":1}}',1766984647689,'2025-12-29 05:04:07','694f2665c3e44e650f0e5dd8','Reduce Context Switching','published','published',NULL,NULL,NULL,NULL),('69520eb019c94fae00e373d6','6951521f19c94fae00e3739b','{\"root\":{\"children\":[{\"type\":\"markdown\",\"version\":1,\"markdown\":\"**Control CPU affinity to improve execution predictability and reduce unnecessary movement between CPU cores, at the cost of scheduler flexibility.**\\n\\n---\\n\\n## Executive Summary (TL;DR)\\n\\nCPU affinity (also called *CPU pinning*) means telling the operating system that a process or thread should run only on specific CPU cores. By default, the operating system is free to move threads between cores to balance load. While this is usually beneficial, it can hurt performance for latency-sensitive or cache-sensitive workloads.\\n\\nPinning execution to specific cores reduces unnecessary movement, improves execution continuity, and makes performance more predictable. Typical improvements range from **520%** for cache-sensitive workloads and can be much higher on multi-socket (NUMA) systems. The trade-off is reduced scheduler flexibility and potential load imbalance.\\n\\nUse CPU pinning when predictability matters more than maximum flexibility.\\n\\n---\\n\\n## A Simple Mental Model (Read This First)\\n\\nThink of a CPU core as a **desk**, and a thread as a **worker**.\\n\\n- When the worker stays at the same desk, their tools are already laid out.\\n- When the worker is moved to another desk, they must set everything up again.\\n- If the worker is moved frequently, more time is spent setting up than working.\\n\\nCPU pinning simply tells the operating system:  \\n**Stop moving this worker between desks.**\\n\\nYou dont need to know what the tools are internally.  \\nLess movement means less wasted time.\\n\\n---\\n\\n## Problem Context\\n\\nModern operating systems use schedulers designed to keep all CPU cores busy. To achieve this, they dynamically move processes and threads between cores to balance load, improve fairness, and optimize power usage.\\n\\nFor most applications, this behavior is ideal. However, for workloads that:\\n- require consistent timing\\n- repeatedly access the same data\\n- are sensitive to latency spikes\\n\\nfrequent core migration becomes expensive.\\n\\nEach migration interrupts execution and forces internal state to be rebuilt. Over time, this creates instability and unpredictable performance.\\n\\n---\\n\\n## What Is CPU Affinity?\\n\\nCPU affinity defines **which CPU cores a process or thread is allowed to run on**.\\n\\n- **Without affinity**: the OS may run your code on any core\\n- **With affinity**: the OS is restricted to a specific set of cores\\n\\nOnce affinity is set, the scheduler respects that constraint and avoids migrating execution outside the allowed cores.\\n\\n---\\n\\n## Default Thread Affinity and Why Threads Move Between Cores\\n\\nBy default, operating systems do **not** strictly bind threads to specific CPU cores.\\n\\nInstead, the scheduler uses a *soft affinity* strategy:\\n- It prefers to run a thread on the same core it ran on last\\n- But it is free to move the thread whenever it decides it is beneficial\\n\\nThis means:\\n- Threads can run on any core\\n- Core assignment is dynamic\\n- There is no guarantee that a thread will stay on the same core\\n\\n### Why Threads Are Migrated\\n\\nThreads move between cores for several reasons:\\n\\n- **Load balancing**  \\n  To prevent some cores from being overloaded while others are idle\\n\\n- **Fairness**  \\n  To ensure all runnable threads get CPU time\\n\\n- **Power and thermal management**  \\n  To spread heat and optimize energy usage\\n\\n- **Oversubscription**  \\n  When more threads are active than available cores\\n\\n- **System activity**  \\n  Interrupt handling, kernel threads, and background services\\n\\nFrom the schedulers perspective, migration is often the right choice.  \\nFrom the applications perspective, migration can introduce overhead and unpredictability.\\n\\nCPU pinning exists to override this behavior when predictability matters more than flexibility.\\n\\n---\\n\\n## Why Uncontrolled Core Migration Hurts Performance\\n\\n### 1. Execution Is Repeatedly Interrupted\\n\\nEach time a thread moves to another core, execution is paused and resumed elsewhere. Even though this is fast, doing it repeatedly adds noticeable overhead.\\n\\n---\\n\\n### 2. Internal Execution State Is Rebuilt\\n\\nWhile running, the CPU prepares internal state to execute code efficiently. When execution moves to another core, that preparation must be rebuilt, slowing progress.\\n\\nYou dont need hardware details to understand this:\\n**movement causes repetition**.\\n\\n---\\n\\n### 3. Latency Becomes Unpredictable\\n\\nSome executions run uninterrupted, others are moved several times. This creates variability and hurts worst-case latency.\\n\\n---\\n\\n### 4. Problems Amplify Under Load\\n\\nAs load increases:\\n- more threads compete for cores\\n- more migration occurs\\n- delays compound\\n\\nThis creates a feedback loop that rapidly degrades performance.\\n\\n---\\n\\n## Why Pinning Improves Many Things at Once\\n\\nCPU pinning improves performance not by optimizing one component, but by removing an entire class of inefficiency.\\n\\n- Less movement  less repeated setup\\n- Longer uninterrupted execution  more useful work\\n- Faster task completion  less system pressure\\n- More stable timing  better tail latency\\n\\nThis is why pinning often improves latency, throughput, and predictability simultaneously.\\n\\n---\\n\\n## How CPU Pinning Works (High-Level)\\n\\n### Hardware Perspective\\n- Each CPU core maintains its own execution context\\n- Staying on the same core preserves continuity\\n- Multi-socket (NUMA) systems amplify the cost of movement\\n\\n### Operating System Perspective\\n- The OS maintains an affinity mask for each process or thread\\n- The scheduler respects this mask\\n- Context switches still happen, but core migration is reduced or eliminated\\n\\n---\\n\\n## Advantages\\n\\n- More predictable performance\\n- Reduced execution interruption\\n- Improved behavior under load\\n- Strong benefits in NUMA systems\\n- Better isolation for critical workloads\\n\\n---\\n\\n## Disadvantages and Trade-offs\\n\\n- Reduced scheduler flexibility\\n- Risk of load imbalance\\n- Requires hardware awareness\\n- Can waste resources if misused\\n- Not suitable for dynamic workloads\\n\\nCPU pinning trades flexibility for predictability.\\n\\n---\\n\\n## When to Use CPU Pinning\\n\\n- Real-time or near-real-time systems\\n- Latency-critical services\\n- Multi-socket (NUMA) servers\\n- Cache-sensitive, predictable workloads\\n- Performance isolation requirements\\n\\n---\\n\\n## When Not to Use It\\n\\n- General-purpose web applications\\n- Highly dynamic or bursty workloads\\n- Cloud environments with abstracted CPUs\\n- Batch processing jobs\\n- Development and testing environments\\n\\n---\\n\\n## Why Thread Affinity Improves Performance (Deeper Explanation)\\n\\nThread and CPU affinity improve performance primarily by **preserving execution locality**. When a thread remains on the same CPU core, the system avoids repeatedly rebuilding execution context that was already optimized.\\n\\nThis improvement comes from several reinforcing effects.\\n\\n---\\n\\n### 1. Preserving CPU Cache Locality\\n\\nEach CPU core maintains private caches that store recently used data and instructions. When a thread runs continuously on the same core, the data it needs remains readily available.\\n\\nWhen a thread migrates to another core:\\n- Its data is no longer in the local cache\\n- The new core must fetch data again\\n- Execution stalls while data is reloaded\\n\\nThis work was already done once and must now be repeated.\\n\\n**Thread affinity keeps caches warm**, reducing cache misses and avoiding repeated data loading.\\n\\n---\\n\\n### 2. Fewer Cache Misses Means Fewer CPU Stalls\\n\\nCache misses force the CPU to wait for data. These waits add latency even if no context switch occurs.\\n\\nCore migration dramatically increases cache misses:\\n- Local cache state is lost\\n- Execution restarts in a colder environment\\n- Progress slows\\n\\nBy keeping threads on the same core, affinity reduces cache miss frequency and improves sustained execution speed.\\n\\n---\\n\\n### 3. Longer Uninterrupted Execution Windows\\n\\nThread migration is often correlated with:\\n- Preemption\\n- Short execution slices\\n- Increased context switching\\n\\nWhen a thread is pinned to a core:\\n- It is more likely to continue running\\n- It completes work faster\\n- It exits the run queue sooner\\n\\n**Faster completion reduces contention**, which indirectly reduces both voluntary and involuntary context switches.\\n\\n---\\n\\n### 4. Reduced Context Switching Pressure\\n\\nAlthough affinity does not eliminate context switching, it reduces **unnecessary switches caused by migration and rebalancing**.\\n\\nWith affinity:\\n- Scheduler decisions are simpler\\n- Threads are less likely to be displaced\\n- Execution becomes more stable\\n\\nLess scheduling churn means more CPU time is spent executing useful work instead of coordinating execution.\\n\\n---\\n\\n### 5. Better Execution Predictability\\n\\nWithout affinity, execution timing varies depending on:\\n- Which core runs the thread\\n- Cache warmth\\n- Migration frequency\\n- System activity\\n\\nAffinity reduces these variables.\\n\\nThis leads to:\\n- Lower latency jitter\\n- More consistent response times\\n- Improved worst-case latency\\n\\nThis predictability is often more valuable than raw throughput.\\n\\n---\\n\\n### 6. Amplified Benefits on NUMA Systems\\n\\nOn multi-socket systems, memory access latency depends on proximity to the CPU core.\\n\\nWithout affinity:\\n- Threads may run far from their memory\\n- Memory access becomes slower\\n- Execution time increases\\n\\nWith affinity:\\n- Threads stay close to their memory\\n- Remote memory access is reduced\\n- Latency becomes more stable\\n\\nNUMA systems benefit disproportionately from proper pinning.\\n\\n---\\n\\n## C# Examples\\n\\n### Process Affinity (Most Common)\\n\\n```csharp\\nusing System;\\nusing System.Diagnostics;\\n\\nclass Program\\n{\\n    static void Main()\\n    {\\n        var process = Process.GetCurrentProcess();\\n\\n        // Pin process to CPU 0 and 1 (binary mask: 00000011)\\n        process.ProcessorAffinity = new IntPtr(0b00000011);\\n\\n        Console.WriteLine(\\\"Process pinned to CPU 0 and 1\\\");\\n    }\\n}\\n\"}],\"direction\":null,\"format\":\"\",\"indent\":0,\"type\":\"root\",\"version\":1}}',1766985392211,'2025-12-29 05:16:32','694f2665c3e44e650f0e5dd8','Pin Processes and Threads to Specific CPU Cores for Predictable Performance','published','initial_revision',NULL,NULL,NULL,NULL),('695210e019c94fae00e373d8','6951521f19c94fae00e3739b','{\"root\":{\"children\":[{\"type\":\"markdown\",\"version\":1,\"markdown\":\"**Control CPU affinity to improve execution predictability and reduce unnecessary movement between CPU cores, at the cost of scheduler flexibility.**\\n\\n---\\n\\n## Executive Summary (TL;DR)\\n\\nCPU affinity (also called *CPU pinning*) means telling the operating system that a process or thread should run only on specific CPU cores. By default, the operating system is free to move threads between cores to balance load. While this is usually beneficial, it can hurt performance for latency-sensitive or cache-sensitive workloads.\\n\\nPinning execution to specific cores reduces unnecessary movement, improves execution continuity, and makes performance more predictable. Typical improvements range from **520%** for cache-sensitive workloads and can be much higher on multi-socket (NUMA) systems. The trade-off is reduced scheduler flexibility and potential load imbalance.\\n\\nUse CPU pinning when predictability matters more than maximum flexibility.\\n\\n---\\n\\n## A Simple Mental Model (Read This First)\\n\\nThink of a CPU core as a **desk**, and a thread as a **worker**.\\n\\n- When the worker stays at the same desk, their tools are already laid out.\\n- When the worker is moved to another desk, they must set everything up again.\\n- If the worker is moved frequently, more time is spent setting up than working.\\n\\nCPU pinning simply tells the operating system:  \\n**Stop moving this worker between desks.**\\n\\nYou dont need to know what the tools are internally.  \\nLess movement means less wasted time.\\n\\n---\\n\\n## Problem Context\\n\\nModern operating systems use schedulers designed to keep all CPU cores busy. To achieve this, they dynamically move processes and threads between cores to balance load, improve fairness, and optimize power usage.\\n\\nFor most applications, this behavior is ideal. However, for workloads that:\\n- require consistent timing\\n- repeatedly access the same data\\n- are sensitive to latency spikes\\n\\nfrequent core migration becomes expensive.\\n\\nEach migration interrupts execution and forces internal state to be rebuilt. Over time, this creates instability and unpredictable performance.\\n\\n---\\n\\n## What Is CPU Affinity?\\n\\nCPU affinity defines **which CPU cores a process or thread is allowed to run on**.\\n\\n- **Without affinity**: the OS may run your code on any core\\n- **With affinity**: the OS is restricted to a specific set of cores\\n\\nOnce affinity is set, the scheduler respects that constraint and avoids migrating execution outside the allowed cores.\\n\\n---\\n\\n## Default Thread Affinity and Why Threads Move Between Cores\\n\\nBy default, operating systems do **not** strictly bind threads to specific CPU cores.\\n\\nInstead, the scheduler uses a *soft affinity* strategy:\\n- It prefers to run a thread on the same core it ran on last\\n- But it is free to move the thread whenever it decides it is beneficial\\n\\nThis means:\\n- Threads can run on any core\\n- Core assignment is dynamic\\n- There is no guarantee that a thread will stay on the same core\\n\\n### Why Threads Are Migrated\\n\\nThreads move between cores for several reasons:\\n\\n- **Load balancing**  \\n  To prevent some cores from being overloaded while others are idle\\n\\n- **Fairness**  \\n  To ensure all runnable threads get CPU time\\n\\n- **Power and thermal management**  \\n  To spread heat and optimize energy usage\\n\\n- **Oversubscription**  \\n  When more threads are active than available cores\\n\\n- **System activity**  \\n  Interrupt handling, kernel threads, and background services\\n\\nFrom the schedulers perspective, migration is often the right choice.  \\nFrom the applications perspective, migration can introduce overhead and unpredictability.\\n\\nCPU pinning exists to override this behavior when predictability matters more than flexibility.\\n\\n---\\n\\n## Why Uncontrolled Core Migration Hurts Performance\\n\\n### 1. Execution Is Repeatedly Interrupted\\n\\nEach time a thread moves to another core, execution is paused and resumed elsewhere. Even though this is fast, doing it repeatedly adds noticeable overhead.\\n\\n---\\n\\n### 2. Internal Execution State Is Rebuilt\\n\\nWhile running, the CPU prepares internal state to execute code efficiently. When execution moves to another core, that preparation must be rebuilt, slowing progress.\\n\\nYou dont need hardware details to understand this:\\n**movement causes repetition**.\\n\\n---\\n\\n### 3. Latency Becomes Unpredictable\\n\\nSome executions run uninterrupted, others are moved several times. This creates variability and hurts worst-case latency.\\n\\n---\\n\\n### 4. Problems Amplify Under Load\\n\\nAs load increases:\\n- more threads compete for cores\\n- more migration occurs\\n- delays compound\\n\\nThis creates a feedback loop that rapidly degrades performance.\\n\\n---\\n\\n## Why Pinning Improves Many Things at Once\\n\\nCPU pinning improves performance not by optimizing one component, but by removing an entire class of inefficiency.\\n\\n- Less movement  less repeated setup\\n- Longer uninterrupted execution  more useful work\\n- Faster task completion  less system pressure\\n- More stable timing  better tail latency\\n\\nThis is why pinning often improves latency, throughput, and predictability simultaneously.\\n\\n---\\n\\n## How CPU Pinning Works (High-Level)\\n\\n### Hardware Perspective\\n- Each CPU core maintains its own execution context\\n- Staying on the same core preserves continuity\\n- Multi-socket (NUMA) systems amplify the cost of movement\\n\\n### Operating System Perspective\\n- The OS maintains an affinity mask for each process or thread\\n- The scheduler respects this mask\\n- Context switches still happen, but core migration is reduced or eliminated\\n\\n---\\n\\n## Advantages\\n\\n- More predictable performance\\n- Reduced execution interruption\\n- Improved behavior under load\\n- Strong benefits in NUMA systems\\n- Better isolation for critical workloads\\n\\n---\\n\\n## Disadvantages and Trade-offs\\n\\n- Reduced scheduler flexibility\\n- Risk of load imbalance\\n- Requires hardware awareness\\n- Can waste resources if misused\\n- Not suitable for dynamic workloads\\n\\nCPU pinning trades flexibility for predictability.\\n\\n---\\n\\n## Why Thread Affinity Improves Performance (Deeper Explanation)\\n\\nThread and CPU affinity improve performance primarily by **preserving execution locality**. When a thread remains on the same CPU core, the system avoids repeatedly rebuilding execution context that was already optimized.\\n\\nThis improvement comes from several reinforcing effects.\\n\\n---\\n\\n### 1. Preserving CPU Cache Locality\\n\\nEach CPU core maintains private caches that store recently used data and instructions. When a thread runs continuously on the same core, the data it needs remains readily available.\\n\\nWhen a thread migrates to another core:\\n- Its data is no longer in the local cache\\n- The new core must fetch data again\\n- Execution stalls while data is reloaded\\n\\nThis work was already done once and must now be repeated.\\n\\n**Thread affinity keeps caches warm**, reducing cache misses and avoiding repeated data loading.\\n\\n---\\n\\n### 2. Fewer Cache Misses Means Fewer CPU Stalls\\n\\nCache misses force the CPU to wait for data. These waits add latency even if no context switch occurs.\\n\\nCore migration dramatically increases cache misses:\\n- Local cache state is lost\\n- Execution restarts in a colder environment\\n- Progress slows\\n\\nBy keeping threads on the same core, affinity reduces cache miss frequency and improves sustained execution speed.\\n\\n---\\n\\n### 3. Longer Uninterrupted Execution Windows\\n\\nThread migration is often correlated with:\\n- Preemption\\n- Short execution slices\\n- Increased context switching\\n\\nWhen a thread is pinned to a core:\\n- It is more likely to continue running\\n- It completes work faster\\n- It exits the run queue sooner\\n\\n**Faster completion reduces contention**, which indirectly reduces both voluntary and involuntary context switches.\\n\\n---\\n\\n### 4. Reduced Context Switching Pressure\\n\\nAlthough affinity does not eliminate context switching, it reduces **unnecessary switches caused by migration and rebalancing**.\\n\\nWith affinity:\\n- Scheduler decisions are simpler\\n- Threads are less likely to be displaced\\n- Execution becomes more stable\\n\\nLess scheduling churn means more CPU time is spent executing useful work instead of coordinating execution.\\n\\n---\\n\\n### 5. Better Execution Predictability\\n\\nWithout affinity, execution timing varies depending on:\\n- Which core runs the thread\\n- Cache warmth\\n- Migration frequency\\n- System activity\\n\\nAffinity reduces these variables.\\n\\nThis leads to:\\n- Lower latency jitter\\n- More consistent response times\\n- Improved worst-case latency\\n\\nThis predictability is often more valuable than raw throughput.\\n\\n---\\n\\n### 6. Amplified Benefits on NUMA Systems\\n\\nOn multi-socket systems, memory access latency depends on proximity to the CPU core.\\n\\nWithout affinity:\\n- Threads may run far from their memory\\n- Memory access becomes slower\\n- Execution time increases\\n\\nWith affinity:\\n- Threads stay close to their memory\\n- Remote memory access is reduced\\n- Latency becomes more stable\\n\\nNUMA systems benefit disproportionately from proper pinning.\\n\\n---\\n\\n## When CPU / Thread Affinity Is Most Beneficial\\n\\nCPU / thread affinity is useful when a thread performs similar work repeatedly and loses performance because it is moved between CPU cores.  \\nMoving a thread is not free: the CPU must warm up again before it can run efficiently.\\n\\nAffinity helps by **letting a thread stay where it already works well**.\\n\\n---\\n\\n### 1. CPU-Bound Applications\\n**What this means**  \\nThe application spends most of its time doing calculations, not waiting for disk or network.\\n\\n**What goes wrong without affinity**  \\nThe thread is constantly ready to run, but the scheduler moves it between cores to balance load.  \\nEach move forces the CPU to restart execution in a colder state.\\n\\n**Why affinity helps**  \\nKeeping the thread on one core allows it to run longer without interruption and avoids repeated warm-up.\\n\\n**Typical effect**\\n- More work done per second\\n- Less CPU time wasted restarting execution\\n\\n---\\n\\n### 2. Long-Lived Worker Threads\\n**What this means**  \\nThe application uses a fixed set of worker threads that stay alive for a long time and process similar tasks.\\n\\n**What goes wrong without affinity**  \\nWorkers are moved between cores even though their work pattern does not change.  \\nEach move discards progress the CPU already made optimizing execution.\\n\\n**Why affinity helps**  \\nA worker that stays on one core becomes more efficient over time and finishes tasks faster.\\n\\n**Typical effect**\\n- Better steady-state performance\\n- More stable timing once the system warms up\\n\\n---\\n\\n### 3. Cache-Sensitive Workloads\\n**What this means**  \\nThe application repeatedly accesses the same data in memory.\\n\\n**What goes wrong without affinity**  \\nWhen a thread moves to another core, the data it was using is no longer nearby.  \\nThe CPU must fetch the same data again, which takes time.\\n\\n**Why affinity helps**  \\nKeeping the thread on the same core keeps its data close, so memory access stays fast.\\n\\n**Typical effect**\\n- Lower memory latency\\n- Faster execution of repeated operations\\n\\n---\\n\\n### 4. Latency-Sensitive Applications\\n**What this means**  \\nResponse time matters more than average throughput, and delays must be predictable.\\n\\n**What goes wrong without affinity**  \\nSome requests run on warm cores and are fast, others resume on cold cores and are slow.  \\nThis creates unpredictable latency spikes.\\n\\n**Why affinity helps**  \\nThreads resume execution in a similar environment every time, reducing randomness.\\n\\n**Typical effect**\\n- More consistent response times\\n- Better worst-case latency (P99 / P999)\\n\\n---\\n\\n### 5. Predictable, Steady Workloads\\n**What this means**  \\nThe workload shape does not change much over time.\\n\\n**What goes wrong without affinity**  \\nThe scheduler keeps moving threads even though there is no real imbalance to fix.\\n\\n**Why affinity helps**  \\nFixed core assignment avoids unnecessary movement and keeps execution stable.\\n\\n**Typical effect**\\n- Repeatable performance\\n- Easier tuning and capacity planning\\n\\n---\\n\\n### 6. Dedicated or Single-Tenant Systems\\n**What this means**  \\nOne application owns the machine.\\n\\n**What goes wrong without affinity**  \\nThreads are moved around for fairness that the application does not need.\\n\\n**Why affinity helps**  \\nRestricting movement reduces interference and improves isolation.\\n\\n**Typical effect**\\n- Better resource usage\\n- More predictable system behavior\\n\\n---\\n\\n### 7. NUMA Systems (Without NUMA-Aware Code)\\n**What this means**  \\nThe machine has multiple CPU sockets, each with its own memory.\\n\\n**What goes wrong without affinity**  \\nThreads move between sockets and access memory that is far away, which is slower.\\n\\n**Why affinity helps**  \\nKeeping threads near their memory avoids expensive remote access.\\n\\n**Typical effect**\\n- Lower memory access latency\\n- More consistent throughput\\n\\n## Rule of Thumb\\n\\n> Use CPU / thread affinity when threads do similar work for a long time and lose performance because they are moved between CPU cores.\\n---\\n\\n## C# Examples\\n\\n### Process Affinity (Most Common)\\n\\n```csharp\\nusing System;\\nusing System.Diagnostics;\\n\\nclass Program\\n{\\n    static void Main()\\n    {\\n        var process = Process.GetCurrentProcess();\\n\\n        // Pin process to CPU 0 and 1 (binary mask: 00000011)\\n        process.ProcessorAffinity = new IntPtr(0b00000011);\\n\\n        Console.WriteLine(\\\"Process pinned to CPU 0 and 1\\\");\\n    }\\n}\\n\"}],\"direction\":null,\"format\":\"\",\"indent\":0,\"type\":\"root\",\"version\":1}}',1766985952689,'2025-12-29 05:25:52','694f2665c3e44e650f0e5dd8','Pin Processes and Threads to Specific CPU Cores for Predictable Performance','published','published',NULL,NULL,NULL,NULL),('69532e3aebcfaae683eec9a7','69532d9aebcfaae683eec996','{\"root\":{\"children\":[{\"type\":\"markdown\",\"version\":1,\"markdown\":\"## Executive Summary (TL;DR)\\n\\nFalse sharing happens when **multiple threads write to different variables that live inside the same CPU cache line** (usually 64 bytes).\\n\\nEven though the variables are logically independent, the CPU cache works with **cache lines**, not variables.  \\nWhen one thread writes, other cores must invalidate their copy of the entire cache line.  \\nThe cache line then **bounces between cores**, creating hidden serialization.\\n\\nThis causes:\\n- Poor scalability\\n- High CPU usage with low throughput\\n- Performance degradation (1050% or worse)\\n\\nFalse sharing **does NOT break correctness**, only performance  which makes it hard to notice.\\n\\n**Solution:** ensure that data frequently written by different threads lives in **different cache lines**, using:\\n- Per-thread data (preferred)\\n- Padding and alignment (when necessary)\\n\\nOnly apply after **profiling confirms cache contention**.\\n\\n---\\n\\n## 1. What Is a Cache Line? (From Zero)\\n\\nThe CPU does **not** load individual variables into cache.\\n\\nInstead, it loads memory in **fixed-size blocks** called **cache lines**.\\n\\n- Typical size: **64 bytes** (x86-64)\\n- Some ARM CPUs: **128 bytes**\\n- Minimum unit moved between:\\n  - RAM  cache\\n  - Core  core\\n\\n> You cannot partially load or partially invalidate a cache line.  \\n> It is always **all or nothing**.\\n\\n---\\n\\n## 2. Simple Analogy (Very Important)\\n\\nThink of memory like this:\\n\\n- **RAM** = a large filing cabinet\\n- **CPU cache** = your desk\\n- **Cache line** = a folder\\n\\nEven if you only need **one page**, you must bring **the entire folder** to your desk.\\n\\nThe CPU works exactly the same way.\\n\\n---\\n\\n## 3. How Variables End Up Sharing Cache Lines\\n\\n```csharp\\nclass Counters {\\n    public long A; // 8 bytes\\n    public long B; // 8 bytes\\n}\\n```\\n\\n**Memory layout (simplified):**\\n\\n```\\n| A (8B) | B (8B) | other data ... |\\n|<---------- 64 bytes (one cache line) ---------->|\\n```\\n\\nEven though A and B are:\\n- Different variables\\n- Used by different threads\\n\\nThey live in the same cache line.\\n\\n---\\n\\n## 4. The Golden Rule (Explains Everything)\\n\\n**If two threads write to anything inside the same cache line, they compete.**\\n\\nIt does **NOT** matter:\\n- That variables are different\\n- That there are no locks\\n- That the code is correct\\n\\nThis is false sharing.\\n\\n---\\n\\n## 5. Why the CPU Forces This (Cache Coherency)\\n\\nEach CPU core has its own cache.\\n\\nThe CPU must guarantee:\\n\\n> \\\"All cores see a consistent view of memory.\\\"\\n\\nTo do this, it uses cache coherency protocols (e.g., MESI).\\n\\n**Core rule:**\\n- Only one core can modify a cache line at a time\\n- Other cores must invalidate their copy before writing\\n\\nCorrect behavior  but expensive.\\n\\n---\\n\\n## 6. False Sharing Explained as a Timeline (Cronograma)\\n\\n### Scenario Setup\\n\\n- **Core 0**  Thread 0  writes to variable `A`\\n- **Core 1**  Thread 1  writes to variable `B`\\n- `A` and `B` share the same cache line\\n\\n### Why Different Cores Have the Same Cache Line\\n\\nWhen Thread 0 on Core 0 first reads `A`, the CPU loads the entire 64-byte cache line containing `A` (and `B`) into Core 0\'s L1 cache.\\n\\nLater, when Thread 1 on Core 1 reads `B`, the CPU loads the same cache line (now containing both `A` and `B`) into Core 1\'s L1 cache.\\n\\n**Both cores now have identical copies of the same cache line in their local caches.**\\n\\n### Detailed Timeline\\n\\n```\\nTime    Core 0 (Thread 0)              Core 1 (Thread 1)              Cache Line State\\n\\nT0      Reads A                        -                               Core 0: Exclusive (E)\\n                                                                        Cache line loaded from RAM\\n                                                                        Cost: ~100-300 cycles\\n\\nT1      -                              Reads B                         Core 0: Shared (S)\\n                                                                        Core 1: Shared (S)\\n                                                                        Both have read-only copies\\n                                                                        Cost: ~40-100 cycles (transfer from Core 0)\\n\\nT2      Writes to A                    -                               Core 0: Modified (M)\\n                                                                        Core 1: Invalid (I)  INVALIDATION!\\n                                                                        \\n                                                                        Core 0 sends \\\"invalidate\\\" message to Core 1\\n                                                                        Cost: ~10-20 cycles (inter-core communication)\\n\\nT3      (continues working)            Wants to write B                Core 1 detects cache line is Invalid\\n                                                                        Core 1 must request cache line from Core 0\\n                                                                        \\n                                                                        Request sent to Core 0: \\\"I need this cache line\\\"\\n                                                                        Cost: ~10-20 cycles (request)\\n\\nT4      Receives request               (waiting...)                    Core 0 must write back to memory (if Modified)\\n                                                                        Core 0 sends cache line to Core 1\\n                                                                        Cost: ~40-100 cycles (write-back + transfer)\\n\\nT5      -                              Receives cache line             Core 0: Shared (S)\\n                                                                        Core 1: Modified (M)\\n                                                                        Now Core 1 has exclusive ownership\\n                                                                        Cost: ~10 cycles (state update)\\n\\nT6      (wants A again)                Writes to B                     Core 1: Modified (M)\\n                                                                        Core 0: Invalid (I)  INVALIDATION AGAIN!\\n                                                                        \\n                                                                        Process repeats...\\n                                                                        Cost: ~40-100 cycles per cycle\\n```\\n\\n### Who Controls the Cache Line Updates?\\n\\n**The CPU\'s cache coherency protocol (MESI) controls everything automatically:**\\n\\n1. **Hardware-level**: No software involvement requiredit happens in CPU hardware\\n2. **Cache controller**: Each core has a cache controller that manages MESI states\\n3. **Interconnect**: Cores communicate through the CPU interconnect (bus or mesh)\\n4. **Snooping**: Cores \\\"snoop\\\" on each other\'s cache transactions to maintain coherency\\n\\n### Cost Breakdown\\n\\n**Per false sharing cycle:**\\n- Invalidation message: ~10-20 cycles\\n- Cache line request: ~10-20 cycles  \\n- Write-back to memory (if needed): ~10-30 cycles\\n- Cache line transfer: ~40-100 cycles\\n- State updates: ~5-10 cycles\\n\\n**Total per cycle: ~75-180 CPU cycles**\\n\\nIf Thread 0 writes to `A` 1 million times per second, and Thread 1 writes to `B` 1 million times per second:\\n- **Potential cache line transfers: 2 million per second**\\n- **Wasted cycles: ~150-360 million cycles per second**\\n- **On a 3GHz CPU: 5-12% of total CPU time wasted on false sharing overhead**\\n\\n### Why This Creates Serialization\\n\\nEven though Thread 0 and Thread 1 are running on different cores (true parallelism), they **cannot write simultaneously** because:\\n\\n1. Only one core can have the cache line in Modified state\\n2. The other core must wait for the transfer to complete\\n3. This creates **implicit serialization** at the hardware level\\n\\n**Result**: What looks like parallel execution is actually serial execution with expensive synchronization.\\n\\n### Visual Representation\\n\\n```\\nNormal Parallel Execution (no false sharing):\\n\\nCore 0: [Write A][Write A][Write A][Write A]...   Continuous\\nCore 1: [Write B][Write B][Write B][Write B]...   Continuous\\n         Both working simultaneously\\n\\nWith False Sharing:\\n\\nCore 0: [Write A][----WAIT----][Write A][----WAIT----]...\\nCore 1: [----WAIT----][Write B][----WAIT----][Write B]...\\n         Taking turns (serialized!)\\n         Wasted cycles during WAIT\\n```\\n\\nThis hidden serialization is why performance degrades even though your code looks perfectly parallel.\\n\\n---\\n\\n### Common Misconceptions\\n\\n**\\\"Separate variables mean separate memory locations\\\"**\\n- The CPU caches data in 64-byte chunks called cache lines. Two variables declared separately can end up on the same cache line if they\'re close in memory. Think of it like apartment buildings: even though you live in apartment 101 and your neighbor in 102, you share the same building (cache line).\\n\\n**\\\"Lock-free code is automatically fast\\\"**\\n- Lock-free data structures avoid blocking but can suffer from false sharing when multiple threads update adjacent fields. Atomic operations still trigger cache invalidations.\\n\\n**\\\"High CPU usage means good parallelization\\\"**\\n- False sharing can cause high CPU usage while destroying actual parallelism. CPUs spend time waiting for cache lines to transfer between cores, not doing useful work.\\n\\n**\\\"The compiler/runtime will optimize this away\\\"**\\n- Compilers don\'t automatically pad structures to prevent false sharing. They optimize for single-threaded performance, not multi-threaded cache behavior.\\n\\n### Why Naive Solutions Fail\\n\\n**Adding locks**: Makes it worse by serializing execution completely. The problem is cache contention, not lack of synchronization.\\n\\n**Increasing thread count**: More threads mean more cache invalidations (O(n) growth). The problem compounds.\\n\\n**Logical separation**: Different variables can still share cache lines if they\'re physically close in memory. CPU sees physical layout, not code structure.\\n\\n**Algorithm optimization alone**: Doesn\'t help if threads are fighting over cache lines at the hardware level.\\n\\n## Why This Becomes a Bottleneck\\n\\n**Cache Line Ping-Pong**: Cache lines constantly bounce between cores. Each transfer consumes bandwidth and creates latency. Impact: 10-50% performance degradation.\\n\\n**Serialization Despite Parallelism**: Threads appear to run in parallel but wait for each other at the cache level. Impact: Applications that should scale linearly instead plateau or degrade.\\n\\n**Memory Bus Saturation**: Cache line transfers consume bandwidth, competing with actual data access. Impact: System-wide performance degradation.\\n\\n**NUMA Amplification**: In NUMA systems, remote cache transfers are 2-3x slower. Impact: 50-100% additional latency.\\n\\n**Scalability Collapse**: Performance degrades with more threads instead of improving. Impact: Applications fail to utilize available CPU cores.\\n\\n## When to Use This Approach\\n\\n**High-performance multi-threaded applications**: Many threads, high throughput requirements, processing large volumes of data.\\n\\n**Frequently-updated shared state**: Per-thread counters, statistics structures, lock-free data structures with adjacent fields.\\n\\n**Profiling indicates cache contention**: High cache miss rates, poor scalability, cache line transfers detected by tools.\\n\\n**High-thread-count systems**: 8+ cores where false sharing effects are amplified. Especially important in NUMA systems.\\n\\n**Latency-sensitive parallel workloads**: Real-time systems, financial trading, game engines, media processing.\\n\\n**Lock-free algorithms**: Lock-free queues, stacks, hash tables. These are particularly susceptible to false sharing.\\n\\n## When Not to Use It\\n\\n**Single-threaded applications**: No parallelism means no false sharing.\\n\\n**Read-only shared data**: False sharing only occurs with writes. Multiple threads reading is fine (Shared state).\\n\\n**Infrequently accessed data**: Padding overhead isn\'t justified by occasional access.\\n\\n**Small data structures**: Structures that naturally span multiple cache lines might not need explicit padding.\\n\\n**Cloud/containerized environments**: Hardware topology is abstracted, cache line sizes may vary.\\n\\n**Development/prototyping**: Premature optimization distracts from correctness. Profile first.\\n\\n**Memory-constrained systems**: Embedded systems, mobile devices. Padding overhead might be unacceptable.\\n\\n**When profiling shows no issue**: Don\'t optimize what isn\'t broken. Use tools to confirm before adding padding.\\n\\n---\\n\\n## How to Avoid False Sharing (General Principles)\\n\\n### Strategy 1: Per-Thread Data (Preferred)\\n\\n**Best approach**: Give each thread its own copy of data. No sharing = no false sharing.\\n\\n**When to use**: \\n- Per-thread counters, statistics, or accumulators\\n- Thread-local state that\'s aggregated later\\n\\n**Benefits**:\\n- No padding overhead\\n- No false sharing (each thread has separate memory)\\n- Cleaner, simpler code\\n\\n**Trade-off**: Must aggregate results when needed (but this is usually infrequent).\\n\\n**Example pattern**:\\n- Each thread maintains its own counter/state\\n- Periodically (or at end), aggregate across all threads\\n- Much cheaper than constant cache line contention\\n\\n### Strategy 2: Padding and Alignment\\n\\n**When per-thread data isn\'t feasible**: Use padding to separate shared data into different cache lines.\\n\\n**Key principles**:\\n- Ensure frequently-written variables start at cache line boundaries\\n- Pad each variable to at least cache line size (64 or 128 bytes)\\n- Use compiler directives to enforce alignment\\n\\n**Benefits**:\\n- Works when data must be shared\\n- Predictable memory layout\\n\\n**Trade-offs**:\\n- Increased memory usage (4-8x)\\n- More complex code\\n- Platform-specific (cache line sizes vary)\\n\\n### Strategy 3: Separate Data Structures\\n\\n**Design approach**: Design data structures so hot fields written by different threads are naturally separated.\\n\\n**Principles**:\\n- Place head/tail pointers in separate cache lines\\n- Separate producer/consumer fields\\n- Group data by access pattern (hot vs. cold)\\n\\n**Benefits**:\\n- Natural separation, less artificial padding\\n- Better overall data structure design\\n\\n### Strategy 4: Reduce Write Frequency\\n\\n**Optimization**: Reduce how often threads write to shared data.\\n\\n**Techniques**:\\n- Batch updates (write every N operations instead of every operation)\\n- Use local accumulators, then periodically update shared state\\n- Prefer read-heavy patterns\\n\\n**Benefits**:\\n- Less false sharing even if data shares cache lines\\n- Better cache efficiency overall\\n\\n**When to use**: When you can\'t avoid sharing but can reduce write frequency.\\n\\n### Strategy 5: Cache Line Size Awareness\\n\\n**Know your platform**:\\n- x86-64: 64 bytes\\n- Some ARM: 128 bytes\\n- Test on target hardware\\n\\n**Implementation**:\\n- Use constants for cache line size\\n- Consider padding to 128 bytes for cross-platform safety\\n- Document assumptions\\n\\n### General Checklist\\n\\n1. **Profile first**: Use `perf c2c` or VTune to identify false sharing\\n2. **Choose strategy**: Prefer per-thread data when possible\\n3. **Measure impact**: Verify improvements after changes\\n4. **Document decisions**: Explain why padding/alignment exists\\n5. **Test on target**: Different platforms have different cache line sizes\\n\\n---\\n\\n## How to Avoid False Sharing in C#\\n\\n### Method 1: ThreadLocal<T> (Best for Per-Thread Data)\\n\\n**Use when**: Each thread needs its own accumulator, counter, or state.\\n\\n```csharp\\nusing System.Threading;\\n\\npublic class RequestCounter {\\n    // Each thread gets its own counter - no sharing!\\n    private readonly ThreadLocal<long> _counter = new ThreadLocal<long>(() => 0);\\n    \\n    public void Increment() {\\n        _counter.Value++;  // Thread-local, no false sharing\\n    }\\n    \\n    public long GetTotal() {\\n        // Aggregate across all threads when needed\\n        long total = 0;\\n        // Note: ThreadLocal doesn\'t provide easy enumeration\\n        // You might need to track threads manually or use a different approach\\n        return total;\\n    }\\n}\\n\\n// Better: Use ThreadLocal with explicit thread tracking\\npublic class ThreadSafeCounter {\\n    private readonly ThreadLocal<long> _counter = new ThreadLocal<long>(() => 0);\\n    private readonly ConcurrentDictionary<int, long> _threadCounters = new();\\n    \\n    public void Increment() {\\n        _counter.Value++;\\n        _threadCounters[Thread.CurrentThread.ManagedThreadId] = _counter.Value;\\n    }\\n    \\n    public long GetTotal() {\\n        return _threadCounters.Values.Sum();\\n    }\\n}\\n```\\n\\n**Why it works**: Each thread accesses completely separate memory locations. No cache line sharing possible.\\n\\n**When to use**: Counters, statistics, accumulators that need per-thread isolation.\\n\\n### Method 2: StructLayout with Padding (For Shared Data)\\n\\n**Use when**: Data must be shared but needs cache line separation.\\n\\n```csharp\\nusing System.Runtime.InteropServices;\\n\\n// Option 1: Explicit size with padding\\n[StructLayout(LayoutKind.Explicit, Size = 128)]  // Pad to 128 bytes (safe for 64 and 128-byte cache lines)\\npublic struct PaddedCounter {\\n    [FieldOffset(0)]\\n    public long Value;\\n    // Rest is automatic padding to 128 bytes\\n}\\n\\npublic class ThreadSafeCounters {\\n    private readonly PaddedCounter[] _counters;\\n    \\n    public ThreadSafeCounters(int threadCount) {\\n        _counters = new PaddedCounter[threadCount];\\n    }\\n    \\n    public void Increment(int threadId) {\\n        Interlocked.Increment(ref _counters[threadId].Value);\\n    }\\n}\\n\\n// Option 2: Manual padding fields\\npublic class ManualPaddedCounter {\\n    private long _counter;\\n    \\n    // Pad to ensure next instance starts at new cache line\\n    // Cache line is 64 bytes, long is 8 bytes\\n    // Need 7 more longs (56 bytes) to reach 64 bytes total\\n    private long _padding1, _padding2, _padding3, _padding4,\\n                 _padding5, _padding6, _padding7;\\n    \\n    public long Value {\\n        get => _counter;\\n        set => _counter = value;\\n    }\\n}\\n```\\n\\n**Why it works**: Forces each `PaddedCounter` to occupy a full cache line (128 bytes), ensuring separate cache lines.\\n\\n**When to use**: Arrays of per-thread data that must be indexed by thread ID.\\n\\n### Method 3: Separate Cache Lines for Lock-Free Structures\\n\\n**Use when**: Building lock-free queues, stacks, or other concurrent structures.\\n\\n```csharp\\nusing System.Runtime.InteropServices;\\n\\n[StructLayout(LayoutKind.Explicit, Size = 128)]\\npublic class LockFreeQueue<T> where T : class {\\n    private readonly T[] _buffer;\\n    private readonly int _capacity;\\n    \\n    [FieldOffset(0)]\\n    private volatile int _head;  // Consumer writes here - first cache line\\n    \\n    // Padding to next cache line (64 bytes)\\n    [FieldOffset(64)]\\n    private volatile int _tail;  // Producer writes here - second cache line\\n    \\n    public LockFreeQueue(int capacity) {\\n        _capacity = capacity;\\n        _buffer = new T[capacity];\\n        _head = 0;\\n        _tail = 0;\\n    }\\n    \\n    public bool TryEnqueue(T item) {\\n        int currentTail = _tail;\\n        int nextTail = (currentTail + 1) % _capacity;\\n        \\n        if (nextTail == _head) {\\n            return false; // Queue full\\n        }\\n        \\n        _buffer[currentTail] = item;\\n        _tail = nextTail;  // Producer writes to separate cache line\\n        return true;\\n    }\\n    \\n    public bool TryDequeue(out T item) {\\n        int currentHead = _head;\\n        \\n        if (currentHead == _tail) {\\n            item = default(T);\\n            return false; // Queue empty\\n        }\\n        \\n        item = _buffer[currentHead];\\n        _buffer[currentHead] = null;\\n        _head = (currentHead + 1) % _capacity;  // Consumer writes to separate cache line\\n        return true;\\n    }\\n}\\n```\\n\\n**Why it works**: Producer (`_tail`) and consumer (`_head`) write to different cache lines, eliminating contention.\\n\\n### Method 4: Separate Arrays for Different Threads\\n\\n**Use when**: You need indexed access but can separate by thread.\\n\\n```csharp\\n//  Bad: All counters in one array - false sharing\\npublic class BadCounters {\\n    private readonly long[] _counters = new long[Environment.ProcessorCount];\\n    \\n    public void Increment(int threadId) {\\n        _counters[threadId]++;  // False sharing if elements share cache lines\\n    }\\n}\\n\\n//  Good: Use padded structures\\npublic class GoodCounters {\\n    private readonly PaddedCounter[] _counters;\\n    \\n    public GoodCounters() {\\n        int threadCount = Environment.ProcessorCount;\\n        _counters = new PaddedCounter[threadCount];\\n    }\\n    \\n    public void Increment(int threadId) {\\n        Interlocked.Increment(ref _counters[threadId].Value);\\n    }\\n}\\n\\n// Using the PaddedCounter struct from Method 2\\n[StructLayout(LayoutKind.Explicit, Size = 128)]\\npublic struct PaddedCounter {\\n    [FieldOffset(0)]\\n    public long Value;\\n}\\n```\\n\\n### Method 5: Reduce Write Frequency (Batching)\\n\\n**Use when**: You can\'t avoid sharing but can reduce write frequency.\\n\\n```csharp\\npublic class BatchedCounter {\\n    // Thread-local accumulator\\n    private readonly ThreadLocal<long> _localCounter = new ThreadLocal<long>(() => 0);\\n    \\n    // Shared counter, updated less frequently\\n    private long _sharedCounter;\\n    private readonly object _lock = new object();\\n    private const int BATCH_SIZE = 1000;\\n    \\n    public void Increment() {\\n        _localCounter.Value++;\\n        \\n        // Only update shared counter every BATCH_SIZE increments\\n        if (_localCounter.Value % BATCH_SIZE == 0) {\\n            lock (_lock) {\\n                _sharedCounter += BATCH_SIZE;\\n            }\\n        }\\n    }\\n    \\n    public long GetTotal() {\\n        long total = _sharedCounter;\\n        // Add any remaining in thread-local counters\\n        // (simplified - in practice, you\'d need to track all threads)\\n        return total;\\n    }\\n}\\n```\\n\\n**Why it works**: Reduces writes to shared data by 1000x (from every increment to every 1000 increments).\\n\\n### Method 6: Using Memory-Mapped or Aligned Allocation (Advanced)\\n\\n**Use when**: You need precise control over memory layout.\\n\\n```csharp\\nusing System.Runtime.InteropServices;\\n\\n// Note: This requires unsafe code and platform-specific implementation\\npublic unsafe class AlignedCounter {\\n    private long* _counter;\\n    \\n    public AlignedCounter() {\\n        // Allocate aligned to cache line boundary (64 bytes)\\n        // This is platform-specific and requires P/Invoke or native allocation\\n        _counter = (long*)AlignedAlloc(64, sizeof(long));\\n    }\\n    \\n    private void* AlignedAlloc(ulong alignment, ulong size) {\\n        // Platform-specific implementation needed:\\n        // - Windows: _aligned_malloc\\n        // - Linux: posix_memalign or aligned_alloc\\n        // - Use DllImport or NativeMemory.AlignedAlloc (modern .NET)\\n        throw new NotImplementedException(\\\"Platform-specific implementation\\\");\\n    }\\n    \\n    // Modern .NET alternative (if available)\\n    public void ModernApproach() {\\n        // .NET 6+ has NativeMemory.AlignedAlloc\\n        // IntPtr ptr = NativeMemory.AlignedAlloc((nuint)sizeof(long), 64);\\n    }\\n}\\n```\\n\\n**Why it works**: Ensures memory starts exactly at a cache line boundary.\\n\\n**When to use**: When you need guaranteed alignment and can\'t use `StructLayout`.\\n\\n### C# Best Practices Summary\\n\\n1. **Prefer ThreadLocal<T>**: Simplest and most effective for per-thread data\\n2. **Use StructLayout for arrays**: When you need indexed access to per-thread data\\n3. **Separate producer/consumer fields**: For lock-free structures, ensure 64+ bytes separation\\n4. **Batch updates**: Reduce write frequency when you can\'t avoid sharing\\n5. **Use constants**: Define `CACHE_LINE_SIZE = 64` or `128` as a constant\\n6. **Verify with tools**: Use profiling tools to confirm false sharing is fixed\\n7. **Document**: Add comments explaining why padding exists\\n\\n### Common C# Pitfalls\\n\\n**Pitfall 1: Assuming array elements are separate**\\n```csharp\\n//  Bad: Array of longs - elements might share cache lines\\nlong[] counters = new long[8];  // 8 * 8 = 64 bytes - all in one cache line!\\n\\n//  Good: Use padded structures\\nPaddedCounter[] counters = new PaddedCounter[8];  // Each is 128 bytes - separate cache lines\\n```\\n\\n**Pitfall 2: Not using StructLayout**\\n```csharp\\n//  Bad: Compiler might reorder fields\\npublic struct Counter {\\n    public long Value;\\n    public long Padding1, Padding2, ...;  // Might not work!\\n}\\n\\n//  Good: Explicit layout\\n[StructLayout(LayoutKind.Explicit, Size = 128)]\\npublic struct Counter {\\n    [FieldOffset(0)]\\n    public long Value;\\n}\\n```\\n\\n**Pitfall 3: Forgetting about object headers**\\n```csharp\\n// In C#, objects have headers (overhead)\\n// PaddedCounter struct is fine, but arrays of objects might have additional overhead\\n// Prefer structs over classes for per-thread data\\n```\\n\\n### Performance Impact in C#\\n\\nTypical improvements when fixing false sharing in C#:\\n- **Per-thread counters**: 30-50% throughput improvement\\n- **Lock-free queues**: 40-60% latency reduction\\n- **Thread pool statistics**: 25-40% overhead reduction\\n- **Scalability**: Can often restore linear scaling up to 16-32 threads\\n\\n---\"}],\"direction\":null,\"format\":\"\",\"indent\":0,\"type\":\"root\",\"version\":1}}',1767059002595,'2025-12-30 01:43:22','694f2665c3e44e650f0e5dd8','Avoid False Sharing and Cache Line Contention','published','initial_revision',NULL,NULL,NULL,NULL),('69545034ebcfaae683eec9c1','6954500debcfaae683eec9b2','{\"root\":{\"children\":[{\"type\":\"markdown\",\"version\":1,\"markdown\":\"**Write code with predictable control flow to minimize branch mispredictions and maximize CPU pipeline efficiency.**\\n\\n---\\n\\n## Executive Summary (TL;DR)\\n\\nBranch prediction is a CPU optimization technique that guesses which path code will take at conditional statements (if/else, loops, switches) before the condition is actually evaluated. When predictions are correct, the CPU pipeline continues smoothly. When predictions are wrong, the CPU must discard speculative work and restart, costing 10-20 CPU cycles per misprediction. To optimize for branch prediction, write code with predictable patterns: move common cases first, use branchless operations where possible, and separate unpredictable branches. This typically improves performance by 5-30% in code-heavy with conditionals. The trade-off is that it may reduce code readability and requires profiling to identify problematic branches. Apply this optimization primarily to hot paths in performance-critical code after profiling confirms branch mispredictions.\\n\\n---\\n\\n## Problem Context\\n\\nModern CPUs can execute multiple instructions simultaneously through **pipelining**think of it like an assembly line where different stages process different instructions at the same time. However, conditional branches (if/else, loops, switches) create a problem: the CPU doesn\'t know which path to take until it evaluates the condition, but it needs to know *now* to keep the pipeline full.\\n\\n**What is pipelining?** Imagine a factory assembly line. Instead of building one car completely before starting the next, you have stages: frame, engine, wheels, paint. While one car is being painted, the next is getting wheels, and the one after that is getting an engine. Similarly, a CPU pipeline has stages like: fetch instruction, decode, execute, write result. Modern CPUs have 10-20 pipeline stages, and when full, they can process multiple instructions simultaneously.\\n\\n**What is a branch?** Any point in code where execution can take different paths: if/else statements, loops (should we continue?), switch statements, function calls, etc.\\n\\n**The problem**: When the CPU encounters a branch, it must wait for the condition to be evaluated before knowing which instructions to load next. This **stalls the pipeline**, wasting cycles. To avoid this, CPUs use **branch prediction**they guess which path will be taken based on historical patterns.\\n\\n### Common Misconceptions\\n\\n**\\\"Branch prediction is something I control\\\"**\\n- Branch prediction happens automatically in CPU hardware. You don\'t explicitly control it, but your code patterns influence how well it works.\\n\\n**\\\"All branches are equally expensive\\\"**\\n- Branches that are predictable (always true, always false, or follow patterns) have near-zero cost. Unpredictable branches cost 10-20 cycles per misprediction.\\n\\n**\\\"Modern CPUs are so fast, branches don\'t matter\\\"**\\n- Modern CPUs are fast *because* of optimizations like branch prediction. When prediction fails, you still pay the penalty. In tight loops with branches, this adds up quickly.\\n\\n**\\\"I should eliminate all branches\\\"**\\n- Not necessary. Eliminate or optimize *unpredictable* branches in hot paths. Predictable branches have minimal cost.\\n\\n### Why Naive Solutions Fail\\n\\n**Adding more branches to \\\"clarify\\\" logic**\\n- More branches mean more prediction opportunities. If they\'re unpredictable, performance gets worse.\\n\\n**Complex nested conditionals**\\n- Nested branches compound the problem. If the outer branch is mispredicted, inner branches may be evaluated speculatively (wrong path), wasting more cycles.\\n\\n**Assuming the compiler optimizes everything**\\n- Compilers do optimize, but they can\'t fix fundamentally unpredictable control flow patterns. The CPU\'s hardware predictor works with patterns, not logic.\\n\\n**Ignoring profiling data**\\n- Without profiling, you might optimize the wrong branches or miss the ones causing real performance problems.\\n\\n---\\n\\n## How It Works\\n\\n### Understanding CPU Pipelines\\n\\n**What is a CPU pipeline?** Modern CPUs break instruction execution into stages. While one instruction is being executed, the next is being decoded, and the one after that is being fetched from memory. This parallelism allows CPUs to process multiple instructions simultaneously.\\n\\n**Pipeline stages** (simplified):\\n1. **Fetch**: Load instruction from memory\\n2. **Decode**: Determine what the instruction does\\n3. **Execute**: Perform the operation\\n4. **Memory**: Access data memory (if needed)\\n5. **Write-back**: Store result\\n\\nWith a 5-stage pipeline, theoretically 5 instructions can be in flight at once. Modern CPUs have 10-20 stages.\\n\\n**The pipeline problem**: To keep the pipeline full, the CPU must fetch the next instruction *before* the current one finishes. But with branches, the CPU doesn\'t know which instruction comes next until the branch condition is evaluated. This creates a **hazard**a situation that prevents the pipeline from proceeding.\\n\\n**What is a hazard?** A situation where the pipeline cannot continue because it lacks necessary information. Branch hazards occur because we don\'t know which instruction to fetch next until the branch is resolved.\\n\\n### Branch Prediction Explained\\n\\n**What is branch prediction?** The CPU guesses which path a branch will take based on:\\n- **Static prediction**: Simple heuristics (e.g., forward branches are usually not taken, backward branches usually are)\\n- **Dynamic prediction**: Historical patterns (e.g., this branch was taken 90% of the time recently, so predict \\\"taken\\\")\\n- **Branch target prediction**: Predicting the target address for indirect branches\\n\\n**Modern branch predictors** use sophisticated algorithms:\\n- **Pattern history tables**: Track taken/not-taken patterns\\n- **Branch target buffers**: Cache target addresses\\n- **Global vs. local history**: Consider recent branches globally or per-branch\\n\\n**What happens when prediction is correct?**\\n- Pipeline continues smoothly\\n- Next instructions are already being processed\\n- Near-zero penalty (maybe 1 cycle for prediction logic)\\n\\n**What happens when prediction is wrong?**\\n- CPU must **flush the pipeline**discard all speculatively executed instructions\\n- Restart from the correct path\\n- **Branch misprediction penalty**: 10-20 cycles on modern CPUs\\n- All the work done speculatively is wasted\\n\\n**Why is the penalty so high?** The pipeline is deep (10-20 stages). When a misprediction is discovered, all instructions in the wrong path must be discarded, and the pipeline must restart. The deeper the pipeline, the higher the cost.\\n\\n### Real-World Example: Loop with Condition\\n\\n```csharp\\n// Loop that processes items, conditionally incrementing a counter\\nint count = 0;\\nforeach (var item in items) {\\n    if (item.Value > threshold) {  // Branch inside hot loop\\n        count++;\\n    }\\n}\\n```\\n\\n**What the CPU sees**:\\n- For each iteration, must predict: will `item.Value > threshold` be true or false?\\n- If items are mostly above threshold, predictor learns \\\"taken\\\"\\n- If items are mostly below, predictor learns \\\"not taken\\\"\\n- If it\'s random, predictor fails frequently  many mispredictions\\n\\n**Cost calculation**:\\n- 1,000,000 items\\n- If branch is 50/50 unpredictable: ~500,000 mispredictions\\n- 500,000  15 cycles = 7,500,000 wasted cycles\\n- On a 3GHz CPU: 2.5ms wasted (significant in a tight loop!)\\n\\n---\\n\\n## Why This Becomes a Bottleneck\\n\\n### Pipeline Stalls\\n\\n**What happens**: When a branch is mispredicted, the pipeline must flush and restart. During this time, no useful work is done.\\n\\n**Impact**: In tight loops with unpredictable branches, you might spend 10-20% of CPU time on misprediction penalties instead of actual computation.\\n\\n**Example**: A loop that processes 1 million items with a 50/50 unpredictable branch might waste 2-3ms just on branch mispredictions. In a function that should complete in 10ms, this is a 20-30% overhead.\\n\\n### Speculative Execution Waste\\n\\n**What is speculative execution?** When the CPU predicts a branch will be taken, it speculatively executes instructions from that path before confirming the prediction.\\n\\n**The waste**: If prediction is wrong, all speculatively executed instructions are discarded:\\n- Decoded instructions: wasted\\n- Executed operations: wasted (unless side-effect free)\\n- Cache loads: might still help (prefetched data)\\n- Memory bandwidth: partially wasted\\n\\n**Impact**: Not just the misprediction penalty, but also wasted work and resources.\\n\\n### Compounding Effects\\n\\n**Multiple branches in sequence**:\\n- If Branch A is mispredicted, Branch B might be evaluated on the wrong path\\n- When the pipeline corrects, Branch B must be re-evaluated\\n- Cascading waste from multiple mispredictions\\n\\n**Nested branches**:\\n- Outer branch misprediction causes inner branches to be evaluated speculatively on wrong path\\n- More instructions wasted, larger penalty\\n\\n**Impact**: Complex control flow with unpredictable branches can amplify the problem.\\n\\n### Cache and Memory Effects\\n\\n**Instruction cache misses**: When a branch is mispredicted, the CPU might load instructions from the wrong path into the instruction cache. When it corrects, it must load the correct instructions, potentially causing cache misses.\\n\\n**Data prefetching**: Speculative execution might prefetch data from the wrong path, wasting memory bandwidth.\\n\\n**Impact**: Additional penalties beyond the direct misprediction cost.\\n\\n---\\n\\n## When to Use This Approach\\n\\n**Hot paths in performance-critical code**: Code that executes frequently (millions of times per second) and is on the critical path for latency or throughput.\\n\\n**Loops with conditions**: Especially tight loops with branches inside. The branch is evaluated many times, so mispredictions compound.\\n\\n**After profiling confirms branch mispredictions**: Use profiling tools (perf, VTune) to identify branches with high misprediction rates. Optimize those, not all branches.\\n\\n**Code with predictable patterns**: When you can make branches more predictable (e.g., process common cases first, sort data to make comparisons predictable).\\n\\n**Latency-sensitive applications**: Where consistent, low latency matters (game engines, trading systems, real-time systems).\\n\\n**High-throughput processing**: Where processing speed directly impacts business metrics (data processing, request handling).\\n\\n**Why these scenarios**: Branch optimization only matters when branches are frequently executed. In cold code, the optimization cost (readability, maintenance) isn\'t worth it.\\n\\n---\\n\\n## When Not to Use It\\n\\n**Cold code**: Code that executes rarely. The optimization cost (readability) isn\'t worth the negligible performance benefit.\\n\\n**Already predictable branches**: If profiling shows branches are already well-predicted (low misprediction rate), optimization won\'t help.\\n\\n**One-time initialization**: Code that runs once at startup. Branch mispredictions here don\'t matter.\\n\\n**Readability is more important**: When code clarity and maintainability are priorities over micro-optimizations.\\n\\n**Compiler already optimizes it**: Modern compilers do branch optimization. Manual optimization might be redundant or conflict with compiler decisions.\\n\\n**No profiling data**: Don\'t optimize branches without data showing they\'re a problem. You might optimize the wrong thing.\\n\\n**Complex optimization for minimal gain**: If the optimization makes code much more complex for a 1-2% gain, it\'s probably not worth it.\\n\\n**Why avoid these**: Branch optimization has costs (readability, maintenance). Only apply when benefits clearly outweigh costs.\\n\\n---\\n\\n## How to Measure and Validate\\n\\n### Profiling Tools\\n\\n**perf (Linux)**:\\n```bash\\n# Measure branch mispredictions\\nperf stat -e branches,branch-misses ./your_application\\n\\n# Detailed branch analysis\\nperf record -e branch-misses ./your_application\\nperf report\\n```\\n\\n**Intel VTune**: \\n- \\\"Microarchitecture Exploration\\\" analysis\\n- Shows branch misprediction hotspots\\n- Visual representation of branch efficiency\\n\\n**Visual Studio Profiler (Windows)**:\\n- \\\"CPU Usage\\\" profiling\\n- Can show branch misprediction events\\n- Timeline view of performance issues\\n\\n### Key Metrics\\n\\n**Branch misprediction rate**: \\n- Formula: `(branch-misses / branches)  100%`\\n- Target: < 5% for hot code\\n- Action: If > 10%, investigate and optimize\\n\\n**Cycles lost to mispredictions**:\\n- Calculate: `branch-misses  misprediction-penalty`\\n- Compare to total cycles to see impact\\n\\n**Instruction-per-cycle (IPC)**:\\n- Higher is better (more work per cycle)\\n- Branch mispredictions reduce IPC\\n- Monitor IPC before/after optimization\\n\\n### Detection Strategies\\n\\n1. **Profile your hot paths**: Use profiling tools on code that executes frequently\\n2. **Look for high misprediction rates**: Branches with >10% misprediction rate are candidates\\n3. **Identify unpredictable patterns**: Look for branches that alternate unpredictably\\n4. **Measure before/after**: Profile, optimize, profile again to verify improvement\\n\\n---\\n\\n## Optimization Techniques\\n\\n### Technique 1: Common Case First\\n\\n**Principle**: Put the most likely path first in if/else statements.\\n\\n**Why it works**: CPU predictors often favor the first path (static prediction) or learn that the first path is more common.\\n\\n```csharp\\n//  Bad: Rare case first\\nif (errorOccurred) {  // Rare case\\n    HandleError();\\n} else {  // Common case\\n    ProcessNormal();\\n}\\n\\n//  Good: Common case first\\nif (!errorOccurred) {  // Common case\\n    ProcessNormal();\\n} else {  // Rare case\\n    HandleError();\\n}\\n```\\n\\n**Benefit**: Predictor learns the common path faster, fewer mispredictions.\\n\\n### Technique 2: Separate Unpredictable Branches\\n\\n**Principle**: If you have a loop with an unpredictable branch, separate the filtering from the processing.\\n\\n```csharp\\n//  Bad: Unpredictable branch in hot loop\\nint count = 0;\\nforeach (var item in items) {\\n    if (item.IsValid && item.Value > threshold) {  // Unpredictable\\n        Process(item);\\n        count++;\\n    }\\n}\\n\\n//  Good: Separate filtering (branch once per item)\\nvar validItems = items\\n    .Where(i => i.IsValid && i.Value > threshold)\\n    .ToList();  // Branch here, but only once per item\\n\\nforeach (var item in validItems) {  // No branches in hot loop!\\n    Process(item);\\n}\\n```\\n\\n**Benefit**: Branch happens during filtering (once), not in the hot processing loop (many times).\\n\\n### Technique 3: Branchless Operations\\n\\n**Principle**: Use arithmetic operations instead of branches when possible.\\n\\n```csharp\\n//  Bad: Branch in hot loop\\nint count = 0;\\nforeach (var value in values) {\\n    if (value > threshold) {\\n        count++;\\n    }\\n}\\n\\n//  Good: Branchless\\nint count = 0;\\nforeach (var value in values) {\\n    count += (value > threshold) ? 1 : 0;  // No branch, just arithmetic\\n}\\n\\n// Or using LINQ (compiler may optimize)\\nint count = values.Count(v => v > threshold);\\n```\\n\\n**Why it works**: Conditional expressions can be compiled to branchless code (conditional moves, bitwise operations). No branch = no misprediction.\\n\\n**Trade-off**: Might be slightly less readable. Use when profiling shows the branch is a problem.\\n\\n### Technique 4: Sort Data for Predictable Comparisons\\n\\n**Principle**: When comparing values in a loop, sorted data makes branches predictable.\\n\\n```csharp\\n// Unsorted data: comparisons are unpredictable\\nforeach (var item in unsortedItems) {\\n    if (item.Value > threshold) {  // 50/50 unpredictable\\n        Process(item);\\n    }\\n}\\n\\n// Sorted data: comparisons are predictable\\nArray.Sort(items, (a, b) => a.Value.CompareTo(b.Value));\\nforeach (var item in items) {\\n    if (item.Value > threshold) {  // First all false, then all true\\n        Process(item);  // Predictable pattern!\\n    }\\n}\\n```\\n\\n**Why it works**: With sorted data, comparisons follow a pattern (all false, then all true). Predictors learn this quickly.\\n\\n**Trade-off**: Sorting has a cost. Only worth it if you process the data multiple times or if sorting is cheap.\\n\\n### Technique 5: Use Lookup Tables Instead of Switches\\n\\n**Principle**: For small, dense switch statements, lookup tables can avoid branches.\\n\\n```csharp\\n//  Many branches (switch)\\nint result;\\nswitch (value) {\\n    case 0: result = Function0(); break;\\n    case 1: result = Function1(); break;\\n    case 2: result = Function2(); break;\\n    // ... many cases\\n}\\n\\n//  Lookup table (no branches if predictable)\\nvar functions = new Func<int>[] { Function0, Function1, Function2, ... };\\nint result = functions[value]();  // Direct jump, no branches\\n```\\n\\n**Why it works**: Direct array access and function call, no conditional branches to predict.\\n\\n**Trade-off**: Only works for dense, small ranges. Sparse switches might be better as switches.\\n\\n### Technique 6: Loop Unrolling for Predictable Patterns\\n\\n**Principle**: Reduce the number of loop branches by processing multiple items per iteration.\\n\\n```csharp\\n// Standard loop: branch every iteration\\nfor (int i = 0; i < array.Length; i++) {\\n    Process(array[i]);\\n}\\n\\n// Unrolled: branch every 4 iterations\\nfor (int i = 0; i < array.Length - 3; i += 4) {\\n    Process(array[i]);\\n    Process(array[i + 1]);\\n    Process(array[i + 2]);\\n    Process(array[i + 3]);\\n}\\n// Handle remainder...\\n\\n// Or let the compiler do it (modern compilers auto-unroll when beneficial)\\n```\\n\\n**Why it works**: Fewer loop branches = fewer misprediction opportunities.\\n\\n**Trade-off**: More code, might hurt instruction cache. Modern compilers often do this automatically.\\n\\n---\\n\\n## Example Scenarios\\n\\n### Scenario 1: Processing Items with Filters\\n\\n**Problem**: Loop with unpredictable filtering condition.\\n\\n**Solution**: Separate filtering from processing.\\n\\n```csharp\\n// Before: Unpredictable branch in hot loop\\npublic int ProcessValidItems(List<Item> items, int threshold) {\\n    int count = 0;\\n    foreach (var item in items) {\\n        if (item.IsValid && item.Value > threshold) {  // Unpredictable\\n            ProcessItem(item);\\n            count++;\\n        }\\n    }\\n    return count;\\n}\\n\\n// After: Predictable (or no branches in hot loop)\\npublic int ProcessValidItems(List<Item> items, int threshold) {\\n    var validItems = items\\n        .Where(i => i.IsValid && i.Value > threshold)\\n        .ToList();\\n    \\n    foreach (var item in validItems) {  // No branches!\\n        ProcessItem(item);\\n    }\\n    return validItems.Count;\\n}\\n```\\n\\n**Performance**: 10-20% improvement when the branch was unpredictable.\\n\\n### Scenario 2: Error Handling\\n\\n**Problem**: Error checks in hot path, but errors are rare.\\n\\n**Solution**: Common case first, or extract error handling.\\n\\n```csharp\\n// Before: Rare case first\\npublic void ProcessRequest(Request req) {\\n    if (req == null || !req.IsValid) {  // Rare\\n        throw new ArgumentException();\\n    }\\n    // Common case...\\n}\\n\\n// After: Common case first, or extract\\npublic void ProcessRequest(Request req) {\\n    if (req != null && req.IsValid) {  // Common case first\\n        // Process...\\n    } else {\\n        throw new ArgumentException();\\n    }\\n}\\n```\\n\\n**Performance**: 5-10% improvement in hot request processing path.\\n\\n### Scenario 3: Counting with Conditions\\n\\n**Problem**: Branch in counting loop.\\n\\n**Solution**: Branchless counting.\\n\\n```csharp\\n// Before: Branch\\nint count = 0;\\nforeach (var value in values) {\\n    if (value > threshold) {\\n        count++;\\n    }\\n}\\n\\n// After: Branchless\\nint count = 0;\\nforeach (var value in values) {\\n    count += (value > threshold) ? 1 : 0;\\n}\\n\\n// Or LINQ (compiler optimizes)\\nint count = values.Count(v => v > threshold);\\n```\\n\\n**Performance**: 15-25% improvement in tight counting loops.\"}],\"direction\":null,\"format\":\"\",\"indent\":0,\"type\":\"root\",\"version\":1}}',1767133236695,'2025-12-30 22:20:36','694f2665c3e44e650f0e5dd8','Optimize Branch Prediction for Better CPU Pipeline Utilization','published','initial_revision',NULL,NULL,NULL,NULL),('6956d518ebcfaae683eec9d9','6956d503ebcfaae683eec9c9','{\"root\":{\"children\":[{\"type\":\"markdown\",\"version\":1,\"markdown\":\"**Replace CPU-wasting busy-wait loops with appropriate synchronization mechanisms to free CPU cycles for useful work and improve system efficiency.**\\n\\n---\\n\\n## Executive Summary (TL;DR)\\n\\nBusy-wait loops continuously consume CPU cycles while waiting for a condition to become true, without yielding control to the operating system scheduler. This wastes CPU resources, increases power consumption, prevents other threads from running, and can degrade overall system performance. The solution is to use proper synchronization primitives (events, semaphores, condition variables) that allow the thread to sleep until the condition is met, or use async/await for non-blocking waits. For very short waits (microseconds), `SpinWait` provides an optimized alternative. Eliminating busy-wait loops can free 10-100% of CPU depending on the situation. Always prefer blocking synchronization or async patterns over busy-waiting, except in extremely latency-critical scenarios with guaranteed short wait times.\\n\\n---\\n\\n## Problem Context\\n\\n**What is a busy-wait loop?** A loop that repeatedly checks a condition in a tight cycle, consuming CPU continuously while waiting. The thread never gives up the CPU, so it burns cycles doing nothing useful.\\n\\n**Example of busy-wait**:\\n```csharp\\nwhile (!condition) {\\n    // Empty loop - just checking condition over and over\\n    // CPU is at 100% doing nothing useful!\\n}\\n```\\n\\n**The problem**: While the thread is busy-waiting, it:\\n- Consumes CPU cycles that could be used by other threads\\n- Wastes power (especially important in mobile/embedded systems)\\n- Prevents the operating system from scheduling other work\\n- Can cause thermal issues (CPU heating up from constant work)\\n- May starve other threads of CPU time\\n\\n### Key Terms Explained\\n**Context Switch**: When the OS scheduler stops one thread and starts another. This has overhead (saving/restoring thread state), but allows the CPU to do useful work instead of busy-waiting.\\n\\n**Yield**: When a thread voluntarily gives up the CPU, allowing the scheduler to run other threads. Threads can yield explicitly or by calling blocking operations.\\n\\n**Blocking**: When a thread waits for something (I/O, synchronization event) and is suspended by the OS. The thread doesn\'t consume CPU while blocked.\\n\\n### Why Naive Solutions Fail\\n\\n**Adding `Thread.Sleep()` to busy-wait loops**\\n```csharp\\nwhile (!condition) {\\n    Thread.Sleep(1);  // Sleep 1ms\\n}\\n```\\n- **Why it\'s better but still wrong**: Reduces CPU usage but introduces unnecessary latency. The thread wakes up every millisecond to check, even if the condition changed immediately after sleeping. Still not ideal.\\n\\n**Using very short sleeps**\\n```csharp\\nwhile (!condition) {\\n    Thread.Sleep(0);  // Yield to other threads\\n}\\n```\\n- **Why it\'s better**: Yields to other threads, but still has overhead. Better than pure busy-wait, but proper synchronization is cleaner and more efficient.\\n\\n**Assuming the problem will fix itself**\\n- **Why it fails**: Busy-wait doesn\'t fix itself. It continues wasting resources until you fix the code.\\n\\n---\\n\\n## How It Works\\n\\n### Understanding Thread Scheduling\\n\\n**What happens when a thread runs**: The OS scheduler assigns a CPU core to the thread. The thread executes instructions until:\\n- It completes its work\\n- It yields the CPU (explicitly or by blocking)\\n- The scheduler preempts it (time slice expires, higher priority thread needs CPU)\\n\\n**What happens during busy-wait**: The thread never yields. It executes the loop instructions continuously:\\n1. Check condition (read memory/register)\\n2. Compare with expected value\\n3. Branch back to step 1 if false\\n4. Repeat millions of times per second\\n\\n**CPU perspective**: The CPU executes these instructions at full speed (billions per second). Even though the thread isn\'t doing useful work, the CPU is working hard, consuming power and generating heat.\\n\\n### Operating System Behavior\\n\\n**When a thread blocks** (waits on a synchronization primitive):\\n1. Thread calls a blocking operation (e.g., `Wait()`, `Sleep()`)\\n2. OS scheduler removes thread from \\\"runnable\\\" queue\\n3. Thread is marked as \\\"waiting\\\" and doesn\'t consume CPU\\n4. OS schedules other threads to use the CPU\\n5. When the condition is met, OS moves thread back to \\\"runnable\\\" queue\\n6. Thread eventually gets scheduled and continues execution\\n\\n**Context switch overhead**: Switching threads takes ~1-10 microseconds on modern systems. This is minimal compared to wasting thousands of cycles in busy-wait.\\n\\n**Wake-up latency**: Modern OS kernels can wake waiting threads very quickly (often microseconds). Proper synchronization primitives use efficient kernel mechanisms (events, futexes) for fast wake-up.\\n\\n### Synchronization Primitives Explained\\n\\n**Event/Semaphore/Mutex**: These are OS-provided synchronization mechanisms that:\\n- Allow threads to wait without consuming CPU\\n- Wake threads efficiently when conditions change\\n- Use kernel mechanisms for fast signaling\\n\\n**How they work**:\\n1. Thread calls `Wait()`  OS suspends thread, doesn\'t consume CPU\\n2. Another thread calls `Set()`/`Signal()`  OS wakes waiting thread(s)\\n3. Woken thread resumes execution\\n\\n**Performance**: Wake-up is typically microseconds. The overhead is negligible compared to busy-wait waste.\\n\\n---\\n\\n## Why This Becomes a Bottleneck\\n\\n### CPU Starvation\\n\\n**What happens**: Busy-wait threads consume CPU cores, preventing other threads from running. If you have N CPU cores and M threads busy-waiting (where M > N), some threads can\'t run at all.\\n\\n**Impact**: System throughput decreases. Actual work threads compete for fewer available cores, causing slowdowns.\\n\\n**Example**: 8-core server, 10 threads doing real work, 5 threads busy-waiting. The 5 busy-wait threads consume 5 cores continuously. Only 3 cores available for the 10 work threads  severe contention and poor performance.\\n\\n### Power Consumption\\n\\n**What happens**: CPUs consume more power when executing instructions. Busy-wait loops execute instructions continuously, keeping the CPU active and consuming power.\\n\\n**Impact**: \\n- Higher electricity costs in data centers\\n- Reduced battery life in mobile devices\\n- Thermal issues (CPU heating up)\\n- Need for better cooling systems\\n\\n**Why it\'s significant**: In a data center with thousands of servers, busy-wait loops can significantly increase power consumption and cooling costs.\\n\\n### Thermal Throttling\\n\\n**What happens**: When CPUs get too hot, they reduce clock frequency to cool down (thermal throttling). Busy-wait loops generate heat, potentially triggering throttling.\\n\\n**Impact**: Reduced CPU performance system-wide. Even threads doing useful work slow down because the CPU is throttled.\\n\\n### Scheduler Inefficiency\\n\\n**What happens**: The OS scheduler can\'t make optimal scheduling decisions when threads don\'t yield. It can\'t balance load effectively or prioritize important work.\\n\\n**Impact**: Suboptimal resource utilization, longer response times for important work, poor system responsiveness.\\n\\n### False Parallelism\\n\\n**What happens**: Multiple threads busy-waiting appear to be \\\"running\\\" (they consume CPU), but they\'re not doing useful work. This creates an illusion of parallelism without actual progress.\\n\\n**Impact**: System appears busy but throughput is low. Monitoring tools show high CPU usage but low actual work completion.\\n\\n---\\n\\n## Optimization Techniques\\n\\n### Technique 1: Use Events/Semaphores for Coordination\\n\\n**When**: Threads need to wait for conditions or events from other threads.\\n\\n```csharp\\n//  Bad: Busy-wait\\npublic class BadWait {\\n    private bool _flag = false;\\n    \\n    public void WaitForFlag() {\\n        while (!_flag) {\\n            // Wasting CPU!\\n        }\\n    }\\n    \\n    public void SetFlag() {\\n        _flag = true;\\n    }\\n}\\n\\n//  Good: Use ManualResetEventSlim\\npublic class GoodWait {\\n    private readonly ManualResetEventSlim _event = new ManualResetEventSlim(false);\\n    \\n    public void WaitForFlag() {\\n        _event.Wait();  // Blocks, doesn\'t consume CPU\\n    }\\n    \\n    public void SetFlag() {\\n        _event.Set();  // Wakes waiting thread\\n    }\\n}\\n```\\n\\n**Why it works**: `ManualResetEventSlim` uses efficient OS mechanisms. Thread blocks (doesn\'t consume CPU) until `Set()` is called, then wakes quickly (microseconds).\\n\\n**Performance**: Eliminates CPU waste. Wake-up latency is microseconds, negligible for most scenarios.\\n\\n### Technique 2: Use async/await for I/O\\n\\n**When**: Waiting for I/O operations (file, network, database).\\n\\n```csharp\\n//  Bad: Busy-wait for I/O\\npublic void ProcessFile(string path) {\\n    while (!File.Exists(path)) {\\n        // Wasting CPU waiting for file!\\n    }\\n    // Process file...\\n}\\n\\n//  Good: Use async/await\\npublic async Task ProcessFileAsync(string path) {\\n    // Wait for file without blocking thread\\n    while (!File.Exists(path)) {\\n        await Task.Delay(100);  // Yield to other work\\n    }\\n    // Process file...\\n}\\n\\n//  Better: Use FileSystemWatcher or proper async I/O\\npublic async Task ProcessFileAsync(string path) {\\n    using var fileStream = File.OpenRead(path);\\n    // Use async I/O methods\\n    var buffer = new byte[4096];\\n    await fileStream.ReadAsync(buffer, 0, buffer.Length);\\n}\\n```\\n\\n**Why it works**: `async/await` doesn\'t block threads. The thread can do other work while waiting for I/O. When I/O completes, execution resumes.\\n\\n**Performance**: Doesn\'t waste threads on I/O waits. Allows more concurrent operations.\\n\\n### Technique 3: Use SpinWait for Very Short Waits\\n\\n**When**: Wait time is guaranteed to be extremely short (nanoseconds to microseconds).\\n\\n```csharp\\n//  For very short waits, use SpinWait\\npublic class OptimizedWait {\\n    private volatile bool _flag = false;\\n    \\n    public void WaitForFlag() {\\n        var spinWait = new SpinWait();\\n        while (!_flag) {\\n            spinWait.SpinOnce();  // Optimized for short waits\\n            // After some spins, yields to other threads\\n        }\\n    }\\n}\\n```\\n\\n**Why it works**: `SpinWait` uses CPU hints and gradually increases back-off. For very short waits, it avoids context switch overhead. For longer waits, it yields to prevent waste.\\n\\n**When to use**: Only when you\'re absolutely certain the wait will be nanoseconds/microseconds. For longer waits, use blocking primitives.\\n\\n**Trade-off**: Still consumes some CPU, but optimized. Use sparingly.\\n\\n### Technique 4: Use TaskCompletionSource for Async Coordination\\n\\n**When**: Coordinating async operations without blocking threads.\\n\\n```csharp\\n//  Good: TaskCompletionSource for async coordination\\npublic class AsyncWait {\\n    private readonly TaskCompletionSource<bool> _tcs = new TaskCompletionSource<bool>();\\n    \\n    public async Task WaitForFlagAsync() {\\n        await _tcs.Task;  // Non-blocking wait\\n    }\\n    \\n    public void SetFlag() {\\n        _tcs.SetResult(true);  // Completes the task\\n    }\\n}\\n```\\n\\n**Why it works**: `TaskCompletionSource` creates a task that completes when `SetResult()` is called. `await` doesn\'t block threadsit schedules continuation when the task completes.\\n\\n**Performance**: No thread blocking, efficient scheduling, allows high concurrency.\\n\\n### Technique 5: Use Producer-Consumer Patterns\\n\\n**When**: Threads need to wait for work items.\\n\\n```csharp\\n//  Good: Use BlockingCollection for producer-consumer\\npublic class WorkQueue {\\n    private readonly BlockingCollection<WorkItem> _queue = new BlockingCollection<WorkItem>();\\n    \\n    // Producer\\n    public void AddWork(WorkItem item) {\\n        _queue.Add(item);  // Wakes waiting consumers\\n    }\\n    \\n    // Consumer\\n    public WorkItem GetWork() {\\n        return _queue.Take();  // Blocks until work available\\n    }\\n}\\n```\\n\\n**Why it works**: `BlockingCollection` uses efficient blocking. Consumers block (no CPU waste) until producers add work, then wake immediately.\\n\\n**Performance**: Efficient coordination without busy-wait.\\n\\n---\\n\\n## Example Scenarios\\n\\n### Scenario 1: Waiting for a Flag\\n\\n**Problem**: Thread needs to wait for a boolean flag to become true.\\n\\n**Bad solution**: Busy-wait loop.\\n\\n**Good solution**: Use `ManualResetEventSlim` or `TaskCompletionSource`.\\n\\n```csharp\\n//  Recommended\\nprivate readonly ManualResetEventSlim _ready = new ManualResetEventSlim(false);\\n\\npublic void WaitForReady() {\\n    _ready.Wait();  // Blocks efficiently\\n}\\n\\npublic void SetReady() {\\n    _ready.Set();  // Wakes waiting threads\\n}\\n```\\n\\n**Performance impact**: Eliminates CPU waste. Thread blocks until ready, then wakes in microseconds.\\n\\n### Scenario 2: Waiting for I/O\\n\\n**Problem**: Waiting for file to be created or network response.\\n\\n**Bad solution**: Busy-wait checking file existence or response.\\n\\n**Good solution**: Use async/await with proper async I/O.\\n\\n```csharp\\n//  Recommended\\npublic async Task<string> ReadFileAsync(string path) {\\n    using var reader = new StreamReader(path);\\n    return await reader.ReadToEndAsync();  // Non-blocking I/O\\n}\\n```\\n\\n**Performance impact**: Doesn\'t block threads. Allows thousands of concurrent I/O operations.\\n\\n### Scenario 3: Producer-Consumer Pattern\\n\\n**Problem**: Consumer threads need to wait for work items.\\n\\n**Bad solution**: Busy-wait checking if queue has items.\\n\\n**Good solution**: Use `BlockingCollection` or channels.\\n\\n```csharp\\n//  Recommended\\nprivate readonly BlockingCollection<WorkItem> _workQueue = new BlockingCollection<WorkItem>();\\n\\npublic void ProcessWork() {\\n    foreach (var item in _workQueue.GetConsumingEnumerable()) {\\n        Process(item);  // Blocks until work available\\n    }\\n}\\n```\\n\\n**Performance impact**: Efficient coordination. Consumers block until work available, no CPU waste.\\n\\n---\\n\\n## Summary and Key Takeaways\\n\\nBusy-wait loops continuously consume CPU cycles while waiting for conditions, wasting resources and preventing other threads from running. Replace them with proper synchronization primitives (events, semaphores, async/await) that allow threads to block efficiently until conditions are met.\\n\\n**Core Principle**: Never busy-wait unless you\'re absolutely certain the wait will be nanoseconds. Use blocking synchronization or async patterns for all other cases.\\n\\n**Main Trade-off**: Tiny wake-up latency (microseconds) for blocking vs. massive CPU waste for busy-wait. The trade-off almost always favors blocking.\"}],\"direction\":null,\"format\":\"\",\"indent\":0,\"type\":\"root\",\"version\":1}}',1767298328644,'2026-01-01 20:12:08','694f2665c3e44e650f0e5dd8','Avoid Busy-Wait Loops: Use Proper Synchronization Primitives','published','initial_revision',NULL,NULL,NULL,NULL);
/*!40000 ALTER TABLE `post_revisions` ENABLE KEYS */;
UNLOCK TABLES;

--
-- Table structure for table `posts`
--

DROP TABLE IF EXISTS `posts`;
/*!40101 SET @saved_cs_client     = @@character_set_client */;
/*!50503 SET character_set_client = utf8mb4 */;
CREATE TABLE `posts` (
  `id` varchar(24) NOT NULL,
  `uuid` varchar(36) NOT NULL,
  `title` varchar(2000) NOT NULL,
  `slug` varchar(191) NOT NULL,
  `mobiledoc` longtext,
  `lexical` longtext,
  `html` longtext,
  `comment_id` varchar(50) DEFAULT NULL,
  `plaintext` longtext,
  `feature_image` varchar(2000) DEFAULT NULL,
  `featured` tinyint(1) NOT NULL DEFAULT '0',
  `type` varchar(50) NOT NULL DEFAULT 'post',
  `status` varchar(50) NOT NULL DEFAULT 'draft',
  `locale` varchar(6) DEFAULT NULL,
  `visibility` varchar(50) NOT NULL DEFAULT 'public',
  `email_recipient_filter` text NOT NULL,
  `created_at` datetime NOT NULL,
  `updated_at` datetime DEFAULT NULL,
  `published_at` datetime DEFAULT NULL,
  `published_by` varchar(24) DEFAULT NULL,
  `custom_excerpt` varchar(2000) DEFAULT NULL,
  `codeinjection_head` text,
  `codeinjection_foot` text,
  `custom_template` varchar(100) DEFAULT NULL,
  `canonical_url` text,
  `newsletter_id` varchar(24) DEFAULT NULL,
  `show_title_and_feature_image` tinyint(1) NOT NULL DEFAULT '1',
  PRIMARY KEY (`id`),
  UNIQUE KEY `posts_slug_type_unique` (`slug`,`type`),
  KEY `posts_uuid_index` (`uuid`),
  KEY `posts_updated_at_index` (`updated_at`),
  KEY `posts_published_at_index` (`published_at`),
  KEY `posts_newsletter_id_foreign` (`newsletter_id`),
  KEY `posts_type_status_updated_at_index` (`type`,`status`,`updated_at`),
  CONSTRAINT `posts_newsletter_id_foreign` FOREIGN KEY (`newsletter_id`) REFERENCES `newsletters` (`id`)
) ENGINE=InnoDB DEFAULT CHARSET=utf8mb4 COLLATE=utf8mb4_0900_ai_ci;
/*!40101 SET character_set_client = @saved_cs_client */;

--
-- Dumping data for table `posts`
--

LOCK TABLES `posts` WRITE;
/*!40000 ALTER TABLE `posts` DISABLE KEYS */;
INSERT INTO `posts` VALUES ('694f2666c3e44e650f0e5e6a','3e774f8f-2f5c-44ee-9312-8c334c68d78d','About this site','about','{\"version\":\"0.3.1\",\"atoms\":[],\"cards\":[[\"hr\",{}]],\"markups\":[[\"a\",[\"href\",\"https://ghost.org\"]]],\"sections\":[[1,\"p\",[[0,[],0,\"Performance Engineering is an independent publication launched in December 2025 by Manuel A Rodriguez. If you subscribe today, you\'ll get full access to the website as well as email newsletters about new content when it\'s available. Your subscription makes this site possible, and allows Performance Engineering to continue to exist. Thank you!\"]]],[1,\"h3\",[[0,[],0,\"Access all areas\"]]],[1,\"p\",[[0,[],0,\"By signing up, you\'ll get access to the full archive of everything that\'s been published before and everything that\'s still to come. Your very own private library.\"]]],[1,\"h3\",[[0,[],0,\"Fresh content, delivered\"]]],[1,\"p\",[[0,[],0,\"Stay up to date with new content sent straight to your inbox! No more worrying about whether you missed something because of a pesky algorithm or news feed.\"]]],[1,\"h3\",[[0,[],0,\"Meet people like you\"]]],[1,\"p\",[[0,[],0,\"Join a community of other subscribers who share the same interests.\"]]],[10,0],[1,\"h3\",[[0,[],0,\"Start your own thing\"]]],[1,\"p\",[[0,[],0,\"Enjoying the experience? Get started for free and set up your very own subscription business using \"],[0,[0],1,\"Ghost\"],[0,[],0,\", the same platform that powers this website.\"]]]],\"ghostVersion\":\"4.0\"}',NULL,'<p>Performance Engineering is an independent publication launched in December 2025 by Manuel A Rodriguez. If you subscribe today, you\'ll get full access to the website as well as email newsletters about new content when it\'s available. Your subscription makes this site possible, and allows Performance Engineering to continue to exist. Thank you!</p><h3 id=\"access-all-areas\">Access all areas</h3><p>By signing up, you\'ll get access to the full archive of everything that\'s been published before and everything that\'s still to come. Your very own private library.</p><h3 id=\"fresh-content-delivered\">Fresh content, delivered</h3><p>Stay up to date with new content sent straight to your inbox! No more worrying about whether you missed something because of a pesky algorithm or news feed.</p><h3 id=\"meet-people-like-you\">Meet people like you</h3><p>Join a community of other subscribers who share the same interests.</p><hr><h3 id=\"start-your-own-thing\">Start your own thing</h3><p>Enjoying the experience? Get started for free and set up your very own subscription business using <a href=\"https://ghost.org\">Ghost</a>, the same platform that powers this website.</p>','694f2666c3e44e650f0e5e6a','Performance Engineering is an independent publication launched in December 2025 by Manuel A Rodriguez. If you subscribe today, you\'ll get full access to the website as well as email newsletters about new content when it\'s available. Your subscription makes this site possible, and allows Performance Engineering to continue to exist. Thank you!\n\n\nAccess all areas\n\nBy signing up, you\'ll get access to the full archive of everything that\'s been published before and everything that\'s still to come. Your very own private library.\n\n\nFresh content, delivered\n\nStay up to date with new content sent straight to your inbox! No more worrying about whether you missed something because of a pesky algorithm or news feed.\n\n\nMeet people like you\n\nJoin a community of other subscribers who share the same interests.\n\n\nStart your own thing\n\nEnjoying the experience? Get started for free and set up your very own subscription business using Ghost, the same platform that powers this website.',NULL,0,'page','published',NULL,'public','all','2025-12-27 00:20:54','2025-12-27 00:21:16','2025-12-27 00:20:54','694f2665c3e44e650f0e5dd8',NULL,NULL,NULL,NULL,NULL,NULL,1),('694ff07e19c94fae00e37345','3328ed27-0ee2-4895-97f2-36b54be47229','Prefer Fewer Fast CPU Cores Over Many Slow Ones for Latency-Sensitive Workloads','prefer-fewer-fast-cpu-cores-over-many-slow-ones-for-latency-sensitive-workloads',NULL,'{\"root\":{\"children\":[{\"type\":\"markdown\",\"markdown\":\"# Prefer Fewer Fast CPU Cores Over Many Slow Ones for Latency-Sensitive Workloads\\n\\n## Subtitle\\nUnderstanding the trade-off between CPU core count and single-thread performance is critical for optimizing latency-sensitive applications where sequential execution matters more than raw parallelism.\\n\\n## Executive Summary (TL;DR)\\nChoosing fewer fast CPU cores over many slow cores is a performance principle that prioritizes single-thread performance and instruction-level parallelism (IPC) over core count. This approach delivers 20-40% better single-threaded performance, significantly lower latency for critical operations (often reducing response times from 50-100ms to 20-40ms), and improved throughput for applications with sequential dependencies. However, it sacrifices overall parallel throughput and scales poorly for highly concurrent workloads. Use this approach when latency matters more than throughput, for single-threaded applications, or when Amdahl\'s Law limits parallel speedup.\\n\\n## Problem Context\\nA common misconception in performance optimization is that more CPU cores always translate to better performance. Engineers often scale systems horizontally by adding cores without considering single-thread performance characteristics. This leads to scenarios where applications with sequential bottlenecks or latency-sensitive operations underperform despite high core counts.\\n\\n**Example: API REST Server Performance Issue**\\n\\nConsider a production API server handling authentication and data processing requests. A team migrates from an 8-core fast CPU (Intel Xeon @ 3.8 GHz) to a 32-core server CPU (AMD EPYC @ 2.5 GHz), expecting 4x better throughput. However, they observe the opposite: average response time increases from 35ms to 100ms, and P99 latency jumps from 65ms to 180ms.\\n\\nThe root cause: each API request has 8ms of sequential processing (input validation, authentication token verification, database connection setup, response serialization). With 32 slow cores, this sequential portion takes 15ms. With 8 fast cores, it takes 3ms. Even though the parallelizable database query runs slightly faster on more cores (12ms vs 15ms), the sequential bottleneck dominates: **32 slow cores = 15ms sequential + 12ms parallel = 27ms per request, but context switching overhead pushes it to 100ms. 8 fast cores = 3ms sequential + 15ms parallel = 18ms per request, with minimal overhead.**\\n\\nMany production systems suffer from this because modern CPUs designed for high core counts (such as some server-class processors) prioritize parallelism over single-thread speed. These processors trade higher clock frequencies and aggressive out-of-order execution for more cores, resulting in lower IPC (Instructions Per Cycle) and slower individual core performance.\\n\\nNaive scaling attempts often fail because:\\n- Sequential dependencies prevent effective parallelization (Amdahl\'s Law)\\n- Synchronization overhead increases with more cores\\n- Cache coherency traffic scales non-linearly with core count\\n- Critical paths remain bounded by single-thread performance\\n\\n## How It Works\\nCPU performance is determined by three key factors: clock frequency, IPC (Instructions Per Cycle), and core count. Fast cores achieve higher performance through:\\n\\n**Clock Frequency**: Higher clock rates allow more instructions to execute per second. Modern fast cores (e.g., Intel Core series, AMD Ryzen high-frequency variants) run at 4-6 GHz, while many-core processors often operate at 2-3 GHz.\\n\\n**Real-World Comparison**:\\n- **Many-core CPU**: AMD EPYC 7543 (32 cores @ 2.8 GHz base, IPC ~1.8)\\n- **Fast-core CPU**: Intel Core i9-12900K (8 performance cores @ 5.0 GHz, IPC ~3.2)\\n\\n**Performance Calculation**:\\nA single fast core at 5.0 GHz with IPC 3.2 can execute approximately **16 billion instructions per second** (5.0  10 Hz  3.2 IPC = 16  10 instructions/sec). A single slow core at 2.8 GHz with IPC 1.8 executes approximately **5 billion instructions per second** (2.8  10 Hz  1.8 IPC = 5.04  10 instructions/sec).\\n\\nFor a sequential algorithm (e.g., parsing JSON, computing a hash, traversing a linked list), the fast core completes in **3.1 seconds** what the slow core takes **10 seconds**. Even with 3 slow cores working in parallel, the fast core still wins because the work cannot be effectively parallelized.\\n\\n**Instruction-Level Parallelism (IPC)**: Fast cores employ deeper pipelines, wider execution units, larger instruction windows, and more aggressive branch prediction. This allows them to extract more parallelism from sequential code, executing multiple independent instructions simultaneously within a single thread.\\n\\n**Example**: Consider this sequential code:\\n```javascript\\nlet sum = 0;\\nfor (let i = 0; i < array.length; i++) {\\n    sum += array[i] * 2;\\n}\\n```\\n\\nA fast core with wide execution units can execute `array[i] * 2` and prepare the next iteration\'s memory fetch simultaneously, achieving IPC of 2.5-3.0. A slow core might achieve only 1.2-1.5 IPC on the same code, taking twice as long despite similar clock speeds.\\n\\n**Cache Hierarchy**: Fast cores typically feature larger, faster L1 and L2 caches per core, reducing memory latency for single-threaded workloads. They also have better prefetching logic that predicts memory access patterns.\\n\\n**Out-of-Order Execution**: Advanced out-of-order execution engines can reorder and parallelize independent instructions within a single thread, effectively creating instruction-level parallelism without explicit multi-threading.\\n\\nWhen a workload has sequential dependencies or Amdahl\'s Law limits parallel speedup, a single fast core can outperform multiple slow cores by completing the critical path faster, even if total theoretical throughput is lower.\\n\\n## Why This Becomes a Bottleneck\\nPerformance degrades when core speed is sacrificed for core count because overhead increases faster than useful work. Here\'s why fast cores outperform many slow cores:\\n\\n**1 Sequential Bottlenecks (Amdahl\'s Law)**\\n\\nEvery parallel algorithm has sequential portions that cannot be parallelized (data distribution, result aggregation, synchronization, I/O).\\n\\n**Why this matters**: Amdahl\'s Law shows that even 5% sequential code limits speedup to 20x regardless of core count. The sequential portion becomes the bottleneck.\\n\\n**Slow cores**:\\n- Execute sequential code slowly (e.g., 5ms for validation, logging, serialization)\\n- Sequential work dominates total latency\\n- More cores don\'t helpsequential work can\'t be parallelized\\n\\n**Fast cores**:\\n- Execute sequential code 2-3x faster (e.g., 2ms for the same validation work)\\n- Sequential bottleneck is reduced, improving overall latency\\n\\n**Example**: A web server processing requests has 5ms sequential work (input validation, authentication, response formatting) and 2ms parallelizable work (database query). With 32 slow cores: 5ms sequential + 2ms parallel = **7ms total** (plus 3-5ms overhead = 10-12ms). With 8 fast cores: 2ms sequential + 2ms parallel = **4ms total** (minimal overhead). Fast cores reduce sequential time by 60%, resulting in 50-60% better overall latency.\\n\\n**2 Context Switching Overhead**\\n\\nWhen a thread is paused, the OS must save registers, stack, and CPU state. This work produces no useful output.\\n\\n**Slow cores**:\\n- Fewer instructions per second\\n- Context switches take longer in real time\\n- More time is wasted switching than doing real work\\n- With many threads, context switching overhead dominates (can consume 15-25% of CPU time)\\n\\n**Fast cores**:\\n- Switch faster (same number of cycles, but cycles execute faster)\\n- Return to useful execution sooner\\n- Overhead becomes a smaller percentage of total time (5-10% overhead)\\n\\n**Example**: A server with 64 threads on 32 slow cores spends 20% of CPU time context switching. The same workload on 8 fast cores spends 8% of CPU time context switching, allowing 12% more CPU time for actual work.\\n\\n**3 Lock / Mutex Contention**\\n\\nShared resources require locks. Only one thread can enter the critical section at a time.\\n\\n**Slow cores**:\\n- Hold locks longer (execute critical section code slowly)\\n- Other threads wait idle\\n- Thread queues grow quickly\\n- Lock contention escalates exponentially with more cores\\n\\n**Fast cores**:\\n- Enter, execute, and release locks quickly\\n- Less waiting time for other threads\\n- Higher real throughput despite fewer cores\\n\\n**Example**: A database connection pool with 32 threads competing for 10 connections. On 32 slow cores, threads spend **85% of their time waiting** for locks (average wait time 15ms per operation because slow cores hold locks longer). On 8 fast cores with the same 32 threads, threads spend **15% of their time waiting** (average wait time 2.5ms because fast cores release locks 3x faster).\\n\\n**4 Cache Misses (Critical Factor)**\\n\\nCPU cache is much faster than RAM (L1 cache: ~1ns, RAM: ~100ns). Cache misses occur when needed data is not in cache.\\n\\n**Why cache misses happen**:\\n- Context switching evicts cache lines\\n- Other threads overwrite cache\\n- Multiple cores invalidate each other\'s cache (cache coherence protocol)\\n- Data working set does not fit in cache\\n\\n**Slow cores**:\\n- Execute fewer instructions before being preempted\\n- Lose cache more often (context switches evict cache lines)\\n- Cache misses dominate execution time (each miss costs 100-300ns)\\n- Higher cache miss rates (e.g., 25% miss rate with many cores)\\n\\n**Fast cores**:\\n- Finish work before cache eviction (complete tasks faster)\\n- Better cache locality (fewer threads = less cache contention)\\n- Memory latency is amortized over more useful work\\n- Lower cache miss rates (e.g., 8% miss rate with fewer cores)\\n\\n**Example**: Processing a shared data structure across cores. With 32 cores, cache miss rate is **25%** (cores frequently invalidate each other\'s cache lines via MESI protocol). With 8 cores, cache miss rate drops to **8%** (less contention, better locality). Each cache miss adds 100-300ns latency. The 32-core system spends 25-75ns per operation waiting for memory; the 8-core system spends 8-24ns per operation.\\n\\n**5 Cache Coherence & Core Synchronization**\\n\\nMulti-core CPUs must keep caches consistent (MESI protocol). Cache lines are invalidated across cores when data is modified.\\n\\n**Slow cores**:\\n- React slowly to invalidations (take longer to process coherence messages)\\n- Stall other cores waiting for synchronization\\n- Amplify synchronization delays (slow cores hold cache lines longer)\\n- Cache ping-pong effect: data bounces between cores\' caches\\n\\n**Fast cores**:\\n- Synchronize quickly (process coherence messages faster)\\n- Reduce global latency (faster invalidation propagation)\\n- Less cache ping-pong (fewer cores = fewer invalidations needed)\\n\\n**Example**: A shared counter incremented by multiple threads. With 32 slow cores, each increment triggers cache invalidations across all cores, causing ping-pong. Each increment takes 150ns due to coherence overhead. With 8 fast cores, fewer invalidations are needed, and each increment takes 40ns**3.75x faster**.\\n\\n**6 Blocking I/O (Network, Disk, DB)**\\n\\nMany applications are I/O-bound, not CPU-bound. Threads wait for external resources (network responses, disk reads, database queries).\\n\\n**Slow cores**:\\n- Process I/O results slowly when data arrives\\n- Hold buffers and locks longer while processing\\n- Increase request latency (I/O wait time + slow processing time)\\n\\n**Fast cores**:\\n- Process responses immediately when I/O completes\\n- Free resources quickly (buffers, locks, connections)\\n- Lower end-to-end latency (I/O wait time + fast processing time)\\n\\n**Example**: An e-commerce API processes orders with 99% parallel work (payment processing, inventory checks) and 1% sequential work (final order commit to database). The sequential database write takes 50ms on slow cores vs 20ms on fast cores. Even though 99% of work completes in 10ms on both systems, P99 latency is **60ms on slow cores vs 30ms on fast cores**the sequential I/O bottleneck dominates tail latency.\\n\\n**7 Net Effect**\\n\\n**More cores  more performance when overhead dominates**\\n\\n- Overhead increases with core count (context switching, lock contention, cache coherence)\\n- Slow cores amplify overhead (each overhead event takes longer)\\n- Fast cores minimize overhead (overhead events complete faster)\\n- Sequential work can\'t be parallelizedfast cores are the only solution\\n- Real throughput = Useful work / (Useful work + Overhead). Fast cores reduce both numerator (faster work) and denominator (less overhead)\\n\\n**Example Summary**: A web server handling 10,000 requests/second. With 32 slow cores: 7ms useful work + 5ms overhead = **12ms per request**. With 8 fast cores: 4ms useful work + 1ms overhead = **5ms per request**. Fast cores deliver **2.4x better latency** with 4x fewer cores.\\n\\n## Advantages\\n- **Superior Single-Thread Performance**: 20-40% better performance on single-threaded workloads compared to many-core processors with lower IPC\\n- **Reduced Latency**: Critical operations complete 50-100% faster (e.g., reducing response times from 50-100ms to 20-40ms in observed benchmarks)\\n- **Lower Synchronization Overhead**: Fewer cores mean fewer lock contentions, atomic operations, and cache coherency messages\\n- **Better Cache Locality**: Larger per-core caches and fewer threads reduce cache misses and improve memory access patterns\\n- **Simpler Architecture**: Less need for complex thread pools, work-stealing schedulers, and parallel algorithms\\n- **Predictable Performance**: More deterministic latency characteristics, crucial for real-time systems\\n- **Higher Throughput for Sequential Workloads**: Applications that cannot be parallelized achieve 15-30% better throughput even with fewer total cores\\n\\n## Disadvantages and Trade-offs\\n- **Limited Parallel Throughput**: Cannot match the total compute capacity of many-core systems for embarrassingly parallel workloads\\n- **Higher Per-Core Cost**: Fast cores with advanced microarchitecture features are more expensive to manufacture\\n- **Poor Scalability for Concurrent Workloads**: Request-per-thread server models (like traditional web servers) benefit less when request count exceeds fast core count\\n- **Lower Total Compute Capacity**: Cannot execute as many independent threads simultaneously\\n- **Underutilization Risk**: If workloads can be fully parallelized, fast cores may sit idle while waiting for I/O, wasting resources\\n- **Memory Bandwidth Limits**: Fewer cores mean fewer memory controllers, potentially limiting memory bandwidth for data-parallel workloads\\n\\n## When to Use This Approach\\n- **Latency-Sensitive Applications**: Real-time systems, trading platforms, gaming servers, interactive applications where P99 latency matters more than throughput\\n  \\n  **Example - Trading Platform**: A high-frequency trading system processing order execution. With 32 slow cores, P99 latency is **85ms** (order validation, risk checks, market data processing are sequential). With 8 fast cores, P99 latency drops to **35ms**a **58% improvement**. This directly impacts profitability, as faster execution captures better prices.\\n\\n- **Sequential Workloads**: Applications with Amdahl\'s Law limits, legacy code that cannot be parallelized, algorithms with strong data dependencies\\n\\n- **Single-Threaded Applications**: Node.js event loops, Python GIL-bound code, JavaScript engines, database query execution (single query optimization)\\n  \\n  **Example - Node.js API**: A REST API built with Node.js handling 5,000 requests/second. Node.js runs on a single event loop thread per process. With 16 slow cores running 4 Node.js processes (4 cores each), CPU utilization is **95%** and average latency is 45ms. With 8 fast cores running 4 Node.js processes (2 cores each), CPU utilization is **65%** and average latency is 22ms**51% faster response times** with lower resource usage.\\n\\n- **Cache-Sensitive Workloads**: Applications where cache locality matters more than parallelism (many scientific computing kernels, graph algorithms)\\n\\n- **I/O-Bound Systems with Critical Paths**: Systems where CPU work per request is minimal but latency is critical (API gateways, load balancers with simple routing logic)\\n\\n- **Database Query Execution**: Single complex queries that cannot be parallelized effectively.\\n  \\n  **Example - Database Query**: A complex SQL query with multiple joins and aggregations executes on a single thread. On 32 slow cores, the query takes **450ms** (single-threaded execution doesn\'t benefit from extra cores). On 8 fast cores, the same query completes in **280ms**a **38% improvement** because the single query thread runs faster.\\n\\n- **Development and Testing**: Faster iteration cycles when single-thread performance improves compile times and test execution speed\\n\\n## When Not to Use It\\n- **Highly Parallel Workloads**: Data processing pipelines, batch jobs, scientific simulations with perfect parallelism (no shared state)\\n\\n- **Embarrassingly Parallel Problems**: Image processing, video encoding, Monte Carlo simulations where each unit of work is independent\\n  \\n  **Example - Video Encoding**: Encoding a 4K video (38402160, 60fps, 10 minutes) using H.264 encoding. With 32 slow cores, encoding completes in **15 minutes** (workload perfectly parallelizes across frames). With 8 fast cores, encoding takes **42 minutes****2.8x slower** because the workload benefits from core count, not core speed.\\n\\n  **Example - Batch Image Processing**: Processing 1 million images (resizing, format conversion, thumbnail generation). With 32 slow cores, the batch completes in **2 hours** (31,250 images per hour per core  32 cores). With 8 fast cores, it takes **8 hours** (125,000 images per hour per core  8 cores, but each core processes faster, still net slower than 32 cores).\\n\\n- **Request-Per-Thread Servers**: Traditional threaded web servers handling thousands of concurrent connections (more cores allow more simultaneous request processing)\\n\\n- **Cost-Optimized Deployments**: When total compute capacity per dollar is the primary metric and latency requirements are relaxed\\n\\n- **Cloud Environments with Auto-Scaling**: When horizontal scaling is cheaper than vertical scaling and workload can be distributed\\n\\n- **Containers with Thread Pools**: Applications using thread pools larger than available fast cores, where additional slow cores provide better resource utilization\\n\\n## Performance Impact\\nReal-world observations show measurable improvements across different workload types:\\n\\n**Comparative Performance Table**:\\n\\n| Caso de Uso | Configuracin | Mtrica Clave | Resultado | Mejora |\\n|-------------|---------------|---------------|-----------|--------|\\n| Trading System | 32 cores lentos vs 8 cores rpidos | P99 Latency | 85ms  35ms | 58% |\\n| Node.js REST API | 16 cores lentos vs 8 cores rpidos | Avg Latency | 45ms  22ms | 51% |\\n| Database Query (Single-threaded) | 32 cores vs 8 cores rpidos | Query Time | 450ms  280ms | 38% |\\n| Microservices Gateway | 16 cores vs 8 cores rpidos | P95 Latency | 120ms  55ms | 54% |\\n| JSON Parsing Service | 32 cores vs 8 cores rpidos | Throughput | 8K ops/sec  12K ops/sec | 50% |\\n| WebSocket Server | 24 cores vs 8 cores rpidos | Connection Latency | 25ms  12ms | 52% |\\n\\n**Latency Improvements**: P50 latency reductions of 30-50% and P99 latency improvements of 40-60% in latency-sensitive applications. For example, a trading system reduced order processing latency from 85ms to 35ms by switching from 32 slow cores to 8 fast cores.\\n\\n**Single-Thread Throughput**: 20-40% improvement in single-threaded benchmarks (SPEC CPU benchmarks show this consistently). JavaScript V8 benchmarks show 25-35% improvements on fast-core architectures. A concrete example: parsing a 10MB JSON file takes 280ms on a slow core vs 180ms on a fast core**36% faster**.\\n\\n**Sequential Workload Throughput**: Even with fewer total cores, applications with 10-20% sequential code show 15-30% better overall throughput because the critical path completes faster.\\n\\n**Example - Web Application**: An e-commerce application with 15% sequential code (authentication, session management, order finalization) processes orders. With 32 slow cores: 15ms sequential + 85ms parallel = 100ms per order, throughput of 320 orders/second. With 8 fast cores: 6ms sequential + 85ms parallel = 91ms per order, throughput of 88 orders/second per process. Running 4 processes (total 32 cores equivalent), throughput is **352 orders/second****10% better** despite the same core count, because the sequential bottleneck is reduced.\\n\\n**Resource Utilization**: Lower CPU utilization (50-70% vs 80-95%) but better response time characteristics, indicating that cores are not the bottleneck but rather single-thread speed.\\n\\n**Energy Efficiency**: Better performance per watt for single-threaded workloads, though total system power may be lower with fewer cores.\\n\\n## Common Mistakes\\n- **Assuming More Cores Always Help**: Adding cores to applications with sequential bottlenecks without profiling to identify the actual bottleneck\\n  \\n  **Example**: A Node.js API server handling 3,000 requests/second on 8 cores shows 85% CPU utilization. The team adds 8 more cores (total 16), expecting 50% CPU utilization and better throughput. Result: CPU utilization drops to 60%, but latency **increases by 15%** (from 25ms to 29ms average) due to increased context switching overhead. The bottleneck was single-thread performance in the event loop, not lack of cores. Solution: Switch to 8 faster cores instead of adding more slow cores.\\n\\n- **Ignoring Amdahl\'s Law**: Failing to calculate theoretical speedup limits based on sequential code percentage\\n  \\n  **Example**: An application has 10% sequential code and 90% parallelizable code. The team calculates: \\\"With 32 cores, we should get 32x speedup on the parallel part, so overall speedup should be close to 32x.\\\" Reality: Amdahl\'s Law shows maximum speedup is 1 / (0.1 + 0.9/32) = **7.6x**, not 32x. Adding more cores beyond 8-16 provides diminishing returns. The sequential 10% becomes the bottleneck.\\n\\n- **Over-Parallelization**: Creating excessive threads that contend for fast cores, leading to context switching overhead that negates single-thread advantages\\n  \\n  **Example**: A Java application running on 8 fast cores creates a thread pool with 64 threads (8 threads per core). Each thread competes for CPU time, causing frequent context switches. Result: CPU spends 20% of time context switching instead of executing code. Latency is 40ms vs 18ms with an 8-thread pool (matching core count). The extra threads negate the fast-core advantage.\\n\\n- **Mismatched Architecture Patterns**: Using request-per-thread models (many threads) with fast-core architectures instead of event-driven or async models\\n  \\n  **Example**: A C++ web server uses one thread per request (traditional Apache-style). On 32 slow cores, it handles 32 concurrent requests efficiently. When migrated to 8 fast cores, it can only handle 8 concurrent requests per process, requiring multiple processes and load balancing. The architecture doesn\'t leverage fast cores effectively. Better approach: Use async I/O (epoll/kqueue) to handle thousands of concurrent requests on 8 fast cores.\\n\\n- **Not Profiling Single-Thread Performance**: Optimizing for multi-threaded scenarios without measuring whether single-thread speed is the actual constraint\\n  \\n  **Example**: A team observes high CPU utilization (90%) and assumes they need more cores. They add cores, but latency doesn\'t improve. Profiling reveals: single request processing takes 50ms, but only 5ms is CPU-bound (the rest is I/O wait). The bottleneck is I/O, not CPU cores. Adding fast cores won\'t help; optimizing I/O (async operations, connection pooling) is the solution.\\n\\n- **Cache-Unaware Algorithms**: Implementing algorithms that ignore cache locality, wasting the cache advantages of fast cores\\n  \\n  **Example**: A graph traversal algorithm uses a linked list data structure, causing random memory access patterns. On fast cores with good cache prefetching, cache miss rate is still 18% due to poor locality. Reimplementing with an array-based structure improves cache locality: cache miss rate drops to 4%, and traversal speed improves by 3x. Fast cores\' cache advantages are wasted without cache-aware algorithms.\\n\\n- **Benchmarking Synthetic Loads**: Testing with perfectly parallel synthetic workloads instead of realistic production traffic patterns\\n  \\n  **Example**: A team benchmarks their application with a synthetic workload that processes independent tasks in parallel (100% parallelizable). Results show 32 slow cores outperform 8 fast cores by 4x. They choose 32 slow cores. In production, the real workload has 20% sequential code (authentication, logging, serialization). Result: 8 fast cores actually perform 25% better than 32 slow cores because the sequential bottleneck wasn\'t captured in the synthetic benchmark.\\n\\n## How to Measure and Validate\\n**Profiling Tools**:\\n- Use `perf` (Linux) to measure CPI (Cycles Per Instruction), cache misses, and branch mispredictions\\n- Profile with tools like `vtune` or `perf top` to identify if single-thread performance or parallelism is the bottleneck\\n- Measure IPC metrics: instructions retired per cycle should be higher on fast cores (typically 2-4 IPC vs 1-2 IPC on slow cores)\\n\\n**Example - Using `perf` to Measure CPI**:\\n```bash\\n# Profile a single-threaded application\\nperf stat -e cycles,instructions,cache-references,cache-misses ./my-app\\n\\n# Example output on slow core:\\n# 1,250,000,000 cycles\\n# 2,187,500,000 instructions\\n# CPI = 1,250,000,000 / 2,187,500,000 = 0.57 (this is actually IPC, CPI would be 1.75)\\n# IPC = 2,187,500,000 / 1,250,000,000 = 1.75 instructions per cycle\\n\\n# Example output on fast core (same workload):\\n# 625,000,000 cycles\\n# 2,187,500,000 instructions\\n# IPC = 2,187,500,000 / 625,000,000 = 3.5 instructions per cycle\\n```\\n\\n**Interpreting Results**: If IPC < 2.0 on single-threaded workloads, the CPU is likely a bottleneck. Fast cores typically achieve IPC of 2.5-4.0 on optimized code. If CPI > 2.0 (IPC < 0.5), consider cores with better single-thread performance.\\n\\n**Key Metrics**:\\n- **Latency Percentiles**: Track P50, P95, P99 latency - fast cores should show lower tail latencies\\n  \\n  **Example Benchmark Results**:\\n  - Slow cores: P50 = 45ms, P95 = 120ms, P99 = 180ms\\n  - Fast cores: P50 = 22ms, P95 = 55ms, P99 = 85ms\\n  - The P99 improvement (53% faster) indicates sequential bottlenecks are being resolved.\\n\\n- **Single-Thread Throughput**: Benchmark single-threaded execution time for critical paths\\n  \\n  **Example**: Measure time to process 1,000 database records sequentially:\\n  - Slow core: 12.5 seconds (12.5ms per record)\\n  - Fast core: 7.8 seconds (7.8ms per record)\\n  - **38% faster** indicates the workload benefits from fast cores.\\n\\n- **CPU Utilization**: Lower utilization with better performance indicates single-thread speedup\\n  \\n  **Example**: API server handling 5,000 req/sec:\\n  - 32 slow cores: 95% CPU utilization, 45ms avg latency\\n  - 8 fast cores: 65% CPU utilization, 22ms avg latency\\n  - Lower utilization + better performance = single-thread speed is the constraint, not parallelism.\\n\\n- **Context Switch Rate**: Fewer context switches per request with fast cores\\n  \\n  **Example**: `vmstat 1` shows context switches:\\n  - Slow cores: 15,000 context switches/second, 3.0 switches per request\\n  - Fast cores: 8,000 context switches/second, 1.6 switches per request\\n  - Fewer switches mean less overhead and better cache locality.\\n\\n- **Cache Hit Rates**: Monitor L1/L2/L3 cache hit ratios - fast cores should show better locality\\n  \\n  **Example**: Using `perf` to measure cache performance:\\n  ```bash\\n  perf stat -e L1-dcache-loads,L1-dcache-load-misses,L2-cache-loads,L2-cache-load-misses ./app\\n  ```\\n  - Slow cores: L1 miss rate 12%, L2 miss rate 8%\\n  - Fast cores: L1 miss rate 5%, L2 miss rate 3%\\n  - Lower miss rates indicate better cache locality and prefetching.\\n\\n**Benchmarking Strategy**:\\n1. Run single-threaded benchmarks (SPEC CPU, single-threaded application tests)\\n   \\n   **Example**: SPEC CPU2017 single-threaded benchmark:\\n   - Slow core: 35 points (normalized score)\\n   - Fast core: 52 points (normalized score)\\n   - **49% improvement** in single-thread performance.\\n\\n2. Measure critical path latency under production load\\n3. Compare same workload on many-core vs few-core-fast systems\\n4. Use realistic load patterns, not synthetic parallel workloads\\n5. Measure tail latencies, not just averages\\n\\n**Production Validation**:\\n- A/B test with canary deployments comparing core configurations\\n  \\n  **Example Deployment Strategy**:\\n  1. Deploy 10% of traffic to fast-core servers\\n  2. Monitor for 24-48 hours\\n  3. Compare metrics: latency (P50, P95, P99), error rates, throughput\\n  4. If fast cores show 30%+ latency improvement with stable error rates, gradually migrate traffic\\n\\n- Monitor application-level metrics (request latency, transaction completion time)\\n- Track system metrics (CPU utilization, context switches, cache performance)\\n- Validate that improvements translate to business metrics (user experience, revenue)\\n\\n## Summary and Key Takeaways\\nThe core principle: **Fewer fast CPU cores outperform many slow cores when single-thread performance matters more than total parallel throughput**. This trade-off is fundamental in CPU architecture and should guide hardware selection and system design.\\n\\nThe main trade-off is between latency/sequential performance and total parallel capacity. Fast cores excel at reducing critical path latency and improving single-thread execution, while many-core systems excel at total throughput for parallelizable workloads.\\n\\n**Decision Guideline**: Choose fast cores when (1) latency requirements are strict (P99 < 100ms), (2) workloads have sequential dependencies (Amdahl\'s Law limits apply), (3) single-thread performance is the bottleneck (profiling confirms), or (4) cache locality matters more than parallelism. Choose many cores when (1) workloads are embarrassingly parallel, (2) total throughput is the primary metric, (3) cost per compute unit is critical, or (4) request-per-thread models handle massive concurrency.\\n\\nAlways profile before deciding: measure IPC, latency percentiles, and identify whether the bottleneck is sequential execution or parallel capacity.\"}],\"direction\":null,\"format\":\"\",\"indent\":0,\"type\":\"root\",\"version\":1}}','<!--kg-card-begin: markdown--><h1 id=\"prefer-fewer-fast-cpu-cores-over-many-slow-ones-for-latency-sensitive-workloads\">Prefer Fewer Fast CPU Cores Over Many Slow Ones for Latency-Sensitive Workloads</h1>\n<h2 id=\"subtitle\">Subtitle</h2>\n<p>Understanding the trade-off between CPU core count and single-thread performance is critical for optimizing latency-sensitive applications where sequential execution matters more than raw parallelism.</p>\n<h2 id=\"executive-summary-tldr\">Executive Summary (TL;DR)</h2>\n<p>Choosing fewer fast CPU cores over many slow cores is a performance principle that prioritizes single-thread performance and instruction-level parallelism (IPC) over core count. This approach delivers 20-40% better single-threaded performance, significantly lower latency for critical operations (often reducing response times from 50-100ms to 20-40ms), and improved throughput for applications with sequential dependencies. However, it sacrifices overall parallel throughput and scales poorly for highly concurrent workloads. Use this approach when latency matters more than throughput, for single-threaded applications, or when Amdahl\'s Law limits parallel speedup.</p>\n<h2 id=\"problem-context\">Problem Context</h2>\n<p>A common misconception in performance optimization is that more CPU cores always translate to better performance. Engineers often scale systems horizontally by adding cores without considering single-thread performance characteristics. This leads to scenarios where applications with sequential bottlenecks or latency-sensitive operations underperform despite high core counts.</p>\n<p><strong>Example: API REST Server Performance Issue</strong></p>\n<p>Consider a production API server handling authentication and data processing requests. A team migrates from an 8-core fast CPU (Intel Xeon @ 3.8 GHz) to a 32-core server CPU (AMD EPYC @ 2.5 GHz), expecting 4x better throughput. However, they observe the opposite: average response time increases from 35ms to 100ms, and P99 latency jumps from 65ms to 180ms.</p>\n<p>The root cause: each API request has 8ms of sequential processing (input validation, authentication token verification, database connection setup, response serialization). With 32 slow cores, this sequential portion takes 15ms. With 8 fast cores, it takes 3ms. Even though the parallelizable database query runs slightly faster on more cores (12ms vs 15ms), the sequential bottleneck dominates: <strong>32 slow cores = 15ms sequential + 12ms parallel = 27ms per request, but context switching overhead pushes it to 100ms. 8 fast cores = 3ms sequential + 15ms parallel = 18ms per request, with minimal overhead.</strong></p>\n<p>Many production systems suffer from this because modern CPUs designed for high core counts (such as some server-class processors) prioritize parallelism over single-thread speed. These processors trade higher clock frequencies and aggressive out-of-order execution for more cores, resulting in lower IPC (Instructions Per Cycle) and slower individual core performance.</p>\n<p>Naive scaling attempts often fail because:</p>\n<ul>\n<li>Sequential dependencies prevent effective parallelization (Amdahl\'s Law)</li>\n<li>Synchronization overhead increases with more cores</li>\n<li>Cache coherency traffic scales non-linearly with core count</li>\n<li>Critical paths remain bounded by single-thread performance</li>\n</ul>\n<h2 id=\"how-it-works\">How It Works</h2>\n<p>CPU performance is determined by three key factors: clock frequency, IPC (Instructions Per Cycle), and core count. Fast cores achieve higher performance through:</p>\n<p><strong>Clock Frequency</strong>: Higher clock rates allow more instructions to execute per second. Modern fast cores (e.g., Intel Core series, AMD Ryzen high-frequency variants) run at 4-6 GHz, while many-core processors often operate at 2-3 GHz.</p>\n<p><strong>Real-World Comparison</strong>:</p>\n<ul>\n<li><strong>Many-core CPU</strong>: AMD EPYC 7543 (32 cores @ 2.8 GHz base, IPC ~1.8)</li>\n<li><strong>Fast-core CPU</strong>: Intel Core i9-12900K (8 performance cores @ 5.0 GHz, IPC ~3.2)</li>\n</ul>\n<p><strong>Performance Calculation</strong>:<br>\nA single fast core at 5.0 GHz with IPC 3.2 can execute approximately <strong>16 billion instructions per second</strong> (5.0  10 Hz  3.2 IPC = 16  10 instructions/sec). A single slow core at 2.8 GHz with IPC 1.8 executes approximately <strong>5 billion instructions per second</strong> (2.8  10 Hz  1.8 IPC = 5.04  10 instructions/sec).</p>\n<p>For a sequential algorithm (e.g., parsing JSON, computing a hash, traversing a linked list), the fast core completes in <strong>3.1 seconds</strong> what the slow core takes <strong>10 seconds</strong>. Even with 3 slow cores working in parallel, the fast core still wins because the work cannot be effectively parallelized.</p>\n<p><strong>Instruction-Level Parallelism (IPC)</strong>: Fast cores employ deeper pipelines, wider execution units, larger instruction windows, and more aggressive branch prediction. This allows them to extract more parallelism from sequential code, executing multiple independent instructions simultaneously within a single thread.</p>\n<p><strong>Example</strong>: Consider this sequential code:</p>\n<pre><code class=\"language-javascript\">let sum = 0;\nfor (let i = 0; i &lt; array.length; i++) {\n    sum += array[i] * 2;\n}\n</code></pre>\n<p>A fast core with wide execution units can execute <code>array[i] * 2</code> and prepare the next iteration\'s memory fetch simultaneously, achieving IPC of 2.5-3.0. A slow core might achieve only 1.2-1.5 IPC on the same code, taking twice as long despite similar clock speeds.</p>\n<p><strong>Cache Hierarchy</strong>: Fast cores typically feature larger, faster L1 and L2 caches per core, reducing memory latency for single-threaded workloads. They also have better prefetching logic that predicts memory access patterns.</p>\n<p><strong>Out-of-Order Execution</strong>: Advanced out-of-order execution engines can reorder and parallelize independent instructions within a single thread, effectively creating instruction-level parallelism without explicit multi-threading.</p>\n<p>When a workload has sequential dependencies or Amdahl\'s Law limits parallel speedup, a single fast core can outperform multiple slow cores by completing the critical path faster, even if total theoretical throughput is lower.</p>\n<h2 id=\"why-this-becomes-a-bottleneck\">Why This Becomes a Bottleneck</h2>\n<p>Performance degrades when core speed is sacrificed for core count because overhead increases faster than useful work. Here\'s why fast cores outperform many slow cores:</p>\n<p><strong>1 Sequential Bottlenecks (Amdahl\'s Law)</strong></p>\n<p>Every parallel algorithm has sequential portions that cannot be parallelized (data distribution, result aggregation, synchronization, I/O).</p>\n<p><strong>Why this matters</strong>: Amdahl\'s Law shows that even 5% sequential code limits speedup to 20x regardless of core count. The sequential portion becomes the bottleneck.</p>\n<p><strong>Slow cores</strong>:</p>\n<ul>\n<li>Execute sequential code slowly (e.g., 5ms for validation, logging, serialization)</li>\n<li>Sequential work dominates total latency</li>\n<li>More cores don\'t helpsequential work can\'t be parallelized</li>\n</ul>\n<p><strong>Fast cores</strong>:</p>\n<ul>\n<li>Execute sequential code 2-3x faster (e.g., 2ms for the same validation work)</li>\n<li>Sequential bottleneck is reduced, improving overall latency</li>\n</ul>\n<p><strong>Example</strong>: A web server processing requests has 5ms sequential work (input validation, authentication, response formatting) and 2ms parallelizable work (database query). With 32 slow cores: 5ms sequential + 2ms parallel = <strong>7ms total</strong> (plus 3-5ms overhead = 10-12ms). With 8 fast cores: 2ms sequential + 2ms parallel = <strong>4ms total</strong> (minimal overhead). Fast cores reduce sequential time by 60%, resulting in 50-60% better overall latency.</p>\n<p><strong>2 Context Switching Overhead</strong></p>\n<p>When a thread is paused, the OS must save registers, stack, and CPU state. This work produces no useful output.</p>\n<p><strong>Slow cores</strong>:</p>\n<ul>\n<li>Fewer instructions per second</li>\n<li>Context switches take longer in real time</li>\n<li>More time is wasted switching than doing real work</li>\n<li>With many threads, context switching overhead dominates (can consume 15-25% of CPU time)</li>\n</ul>\n<p><strong>Fast cores</strong>:</p>\n<ul>\n<li>Switch faster (same number of cycles, but cycles execute faster)</li>\n<li>Return to useful execution sooner</li>\n<li>Overhead becomes a smaller percentage of total time (5-10% overhead)</li>\n</ul>\n<p><strong>Example</strong>: A server with 64 threads on 32 slow cores spends 20% of CPU time context switching. The same workload on 8 fast cores spends 8% of CPU time context switching, allowing 12% more CPU time for actual work.</p>\n<p><strong>3 Lock / Mutex Contention</strong></p>\n<p>Shared resources require locks. Only one thread can enter the critical section at a time.</p>\n<p><strong>Slow cores</strong>:</p>\n<ul>\n<li>Hold locks longer (execute critical section code slowly)</li>\n<li>Other threads wait idle</li>\n<li>Thread queues grow quickly</li>\n<li>Lock contention escalates exponentially with more cores</li>\n</ul>\n<p><strong>Fast cores</strong>:</p>\n<ul>\n<li>Enter, execute, and release locks quickly</li>\n<li>Less waiting time for other threads</li>\n<li>Higher real throughput despite fewer cores</li>\n</ul>\n<p><strong>Example</strong>: A database connection pool with 32 threads competing for 10 connections. On 32 slow cores, threads spend <strong>85% of their time waiting</strong> for locks (average wait time 15ms per operation because slow cores hold locks longer). On 8 fast cores with the same 32 threads, threads spend <strong>15% of their time waiting</strong> (average wait time 2.5ms because fast cores release locks 3x faster).</p>\n<p><strong>4 Cache Misses (Critical Factor)</strong></p>\n<p>CPU cache is much faster than RAM (L1 cache: ~1ns, RAM: ~100ns). Cache misses occur when needed data is not in cache.</p>\n<p><strong>Why cache misses happen</strong>:</p>\n<ul>\n<li>Context switching evicts cache lines</li>\n<li>Other threads overwrite cache</li>\n<li>Multiple cores invalidate each other\'s cache (cache coherence protocol)</li>\n<li>Data working set does not fit in cache</li>\n</ul>\n<p><strong>Slow cores</strong>:</p>\n<ul>\n<li>Execute fewer instructions before being preempted</li>\n<li>Lose cache more often (context switches evict cache lines)</li>\n<li>Cache misses dominate execution time (each miss costs 100-300ns)</li>\n<li>Higher cache miss rates (e.g., 25% miss rate with many cores)</li>\n</ul>\n<p><strong>Fast cores</strong>:</p>\n<ul>\n<li>Finish work before cache eviction (complete tasks faster)</li>\n<li>Better cache locality (fewer threads = less cache contention)</li>\n<li>Memory latency is amortized over more useful work</li>\n<li>Lower cache miss rates (e.g., 8% miss rate with fewer cores)</li>\n</ul>\n<p><strong>Example</strong>: Processing a shared data structure across cores. With 32 cores, cache miss rate is <strong>25%</strong> (cores frequently invalidate each other\'s cache lines via MESI protocol). With 8 cores, cache miss rate drops to <strong>8%</strong> (less contention, better locality). Each cache miss adds 100-300ns latency. The 32-core system spends 25-75ns per operation waiting for memory; the 8-core system spends 8-24ns per operation.</p>\n<p><strong>5 Cache Coherence &amp; Core Synchronization</strong></p>\n<p>Multi-core CPUs must keep caches consistent (MESI protocol). Cache lines are invalidated across cores when data is modified.</p>\n<p><strong>Slow cores</strong>:</p>\n<ul>\n<li>React slowly to invalidations (take longer to process coherence messages)</li>\n<li>Stall other cores waiting for synchronization</li>\n<li>Amplify synchronization delays (slow cores hold cache lines longer)</li>\n<li>Cache ping-pong effect: data bounces between cores\' caches</li>\n</ul>\n<p><strong>Fast cores</strong>:</p>\n<ul>\n<li>Synchronize quickly (process coherence messages faster)</li>\n<li>Reduce global latency (faster invalidation propagation)</li>\n<li>Less cache ping-pong (fewer cores = fewer invalidations needed)</li>\n</ul>\n<p><strong>Example</strong>: A shared counter incremented by multiple threads. With 32 slow cores, each increment triggers cache invalidations across all cores, causing ping-pong. Each increment takes 150ns due to coherence overhead. With 8 fast cores, fewer invalidations are needed, and each increment takes 40ns<strong>3.75x faster</strong>.</p>\n<p><strong>6 Blocking I/O (Network, Disk, DB)</strong></p>\n<p>Many applications are I/O-bound, not CPU-bound. Threads wait for external resources (network responses, disk reads, database queries).</p>\n<p><strong>Slow cores</strong>:</p>\n<ul>\n<li>Process I/O results slowly when data arrives</li>\n<li>Hold buffers and locks longer while processing</li>\n<li>Increase request latency (I/O wait time + slow processing time)</li>\n</ul>\n<p><strong>Fast cores</strong>:</p>\n<ul>\n<li>Process responses immediately when I/O completes</li>\n<li>Free resources quickly (buffers, locks, connections)</li>\n<li>Lower end-to-end latency (I/O wait time + fast processing time)</li>\n</ul>\n<p><strong>Example</strong>: An e-commerce API processes orders with 99% parallel work (payment processing, inventory checks) and 1% sequential work (final order commit to database). The sequential database write takes 50ms on slow cores vs 20ms on fast cores. Even though 99% of work completes in 10ms on both systems, P99 latency is <strong>60ms on slow cores vs 30ms on fast cores</strong>the sequential I/O bottleneck dominates tail latency.</p>\n<p><strong>7 Net Effect</strong></p>\n<p><strong>More cores  more performance when overhead dominates</strong></p>\n<ul>\n<li>Overhead increases with core count (context switching, lock contention, cache coherence)</li>\n<li>Slow cores amplify overhead (each overhead event takes longer)</li>\n<li>Fast cores minimize overhead (overhead events complete faster)</li>\n<li>Sequential work can\'t be parallelizedfast cores are the only solution</li>\n<li>Real throughput = Useful work / (Useful work + Overhead). Fast cores reduce both numerator (faster work) and denominator (less overhead)</li>\n</ul>\n<p><strong>Example Summary</strong>: A web server handling 10,000 requests/second. With 32 slow cores: 7ms useful work + 5ms overhead = <strong>12ms per request</strong>. With 8 fast cores: 4ms useful work + 1ms overhead = <strong>5ms per request</strong>. Fast cores deliver <strong>2.4x better latency</strong> with 4x fewer cores.</p>\n<h2 id=\"advantages\">Advantages</h2>\n<ul>\n<li><strong>Superior Single-Thread Performance</strong>: 20-40% better performance on single-threaded workloads compared to many-core processors with lower IPC</li>\n<li><strong>Reduced Latency</strong>: Critical operations complete 50-100% faster (e.g., reducing response times from 50-100ms to 20-40ms in observed benchmarks)</li>\n<li><strong>Lower Synchronization Overhead</strong>: Fewer cores mean fewer lock contentions, atomic operations, and cache coherency messages</li>\n<li><strong>Better Cache Locality</strong>: Larger per-core caches and fewer threads reduce cache misses and improve memory access patterns</li>\n<li><strong>Simpler Architecture</strong>: Less need for complex thread pools, work-stealing schedulers, and parallel algorithms</li>\n<li><strong>Predictable Performance</strong>: More deterministic latency characteristics, crucial for real-time systems</li>\n<li><strong>Higher Throughput for Sequential Workloads</strong>: Applications that cannot be parallelized achieve 15-30% better throughput even with fewer total cores</li>\n</ul>\n<h2 id=\"disadvantages-and-trade-offs\">Disadvantages and Trade-offs</h2>\n<ul>\n<li><strong>Limited Parallel Throughput</strong>: Cannot match the total compute capacity of many-core systems for embarrassingly parallel workloads</li>\n<li><strong>Higher Per-Core Cost</strong>: Fast cores with advanced microarchitecture features are more expensive to manufacture</li>\n<li><strong>Poor Scalability for Concurrent Workloads</strong>: Request-per-thread server models (like traditional web servers) benefit less when request count exceeds fast core count</li>\n<li><strong>Lower Total Compute Capacity</strong>: Cannot execute as many independent threads simultaneously</li>\n<li><strong>Underutilization Risk</strong>: If workloads can be fully parallelized, fast cores may sit idle while waiting for I/O, wasting resources</li>\n<li><strong>Memory Bandwidth Limits</strong>: Fewer cores mean fewer memory controllers, potentially limiting memory bandwidth for data-parallel workloads</li>\n</ul>\n<h2 id=\"when-to-use-this-approach\">When to Use This Approach</h2>\n<ul>\n<li>\n<p><strong>Latency-Sensitive Applications</strong>: Real-time systems, trading platforms, gaming servers, interactive applications where P99 latency matters more than throughput</p>\n<p><strong>Example - Trading Platform</strong>: A high-frequency trading system processing order execution. With 32 slow cores, P99 latency is <strong>85ms</strong> (order validation, risk checks, market data processing are sequential). With 8 fast cores, P99 latency drops to <strong>35ms</strong>a <strong>58% improvement</strong>. This directly impacts profitability, as faster execution captures better prices.</p>\n</li>\n<li>\n<p><strong>Sequential Workloads</strong>: Applications with Amdahl\'s Law limits, legacy code that cannot be parallelized, algorithms with strong data dependencies</p>\n</li>\n<li>\n<p><strong>Single-Threaded Applications</strong>: Node.js event loops, Python GIL-bound code, JavaScript engines, database query execution (single query optimization)</p>\n<p><strong>Example - Node.js API</strong>: A REST API built with Node.js handling 5,000 requests/second. Node.js runs on a single event loop thread per process. With 16 slow cores running 4 Node.js processes (4 cores each), CPU utilization is <strong>95%</strong> and average latency is 45ms. With 8 fast cores running 4 Node.js processes (2 cores each), CPU utilization is <strong>65%</strong> and average latency is 22ms<strong>51% faster response times</strong> with lower resource usage.</p>\n</li>\n<li>\n<p><strong>Cache-Sensitive Workloads</strong>: Applications where cache locality matters more than parallelism (many scientific computing kernels, graph algorithms)</p>\n</li>\n<li>\n<p><strong>I/O-Bound Systems with Critical Paths</strong>: Systems where CPU work per request is minimal but latency is critical (API gateways, load balancers with simple routing logic)</p>\n</li>\n<li>\n<p><strong>Database Query Execution</strong>: Single complex queries that cannot be parallelized effectively.</p>\n<p><strong>Example - Database Query</strong>: A complex SQL query with multiple joins and aggregations executes on a single thread. On 32 slow cores, the query takes <strong>450ms</strong> (single-threaded execution doesn\'t benefit from extra cores). On 8 fast cores, the same query completes in <strong>280ms</strong>a <strong>38% improvement</strong> because the single query thread runs faster.</p>\n</li>\n<li>\n<p><strong>Development and Testing</strong>: Faster iteration cycles when single-thread performance improves compile times and test execution speed</p>\n</li>\n</ul>\n<h2 id=\"when-not-to-use-it\">When Not to Use It</h2>\n<ul>\n<li>\n<p><strong>Highly Parallel Workloads</strong>: Data processing pipelines, batch jobs, scientific simulations with perfect parallelism (no shared state)</p>\n</li>\n<li>\n<p><strong>Embarrassingly Parallel Problems</strong>: Image processing, video encoding, Monte Carlo simulations where each unit of work is independent</p>\n<p><strong>Example - Video Encoding</strong>: Encoding a 4K video (38402160, 60fps, 10 minutes) using H.264 encoding. With 32 slow cores, encoding completes in <strong>15 minutes</strong> (workload perfectly parallelizes across frames). With 8 fast cores, encoding takes <strong>42 minutes</strong><strong>2.8x slower</strong> because the workload benefits from core count, not core speed.</p>\n<p><strong>Example - Batch Image Processing</strong>: Processing 1 million images (resizing, format conversion, thumbnail generation). With 32 slow cores, the batch completes in <strong>2 hours</strong> (31,250 images per hour per core  32 cores). With 8 fast cores, it takes <strong>8 hours</strong> (125,000 images per hour per core  8 cores, but each core processes faster, still net slower than 32 cores).</p>\n</li>\n<li>\n<p><strong>Request-Per-Thread Servers</strong>: Traditional threaded web servers handling thousands of concurrent connections (more cores allow more simultaneous request processing)</p>\n</li>\n<li>\n<p><strong>Cost-Optimized Deployments</strong>: When total compute capacity per dollar is the primary metric and latency requirements are relaxed</p>\n</li>\n<li>\n<p><strong>Cloud Environments with Auto-Scaling</strong>: When horizontal scaling is cheaper than vertical scaling and workload can be distributed</p>\n</li>\n<li>\n<p><strong>Containers with Thread Pools</strong>: Applications using thread pools larger than available fast cores, where additional slow cores provide better resource utilization</p>\n</li>\n</ul>\n<h2 id=\"performance-impact\">Performance Impact</h2>\n<p>Real-world observations show measurable improvements across different workload types:</p>\n<p><strong>Comparative Performance Table</strong>:</p>\n<table>\n<thead>\n<tr>\n<th>Caso de Uso</th>\n<th>Configuracin</th>\n<th>Mtrica Clave</th>\n<th>Resultado</th>\n<th>Mejora</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td>Trading System</td>\n<td>32 cores lentos vs 8 cores rpidos</td>\n<td>P99 Latency</td>\n<td>85ms  35ms</td>\n<td>58%</td>\n</tr>\n<tr>\n<td>Node.js REST API</td>\n<td>16 cores lentos vs 8 cores rpidos</td>\n<td>Avg Latency</td>\n<td>45ms  22ms</td>\n<td>51%</td>\n</tr>\n<tr>\n<td>Database Query (Single-threaded)</td>\n<td>32 cores vs 8 cores rpidos</td>\n<td>Query Time</td>\n<td>450ms  280ms</td>\n<td>38%</td>\n</tr>\n<tr>\n<td>Microservices Gateway</td>\n<td>16 cores vs 8 cores rpidos</td>\n<td>P95 Latency</td>\n<td>120ms  55ms</td>\n<td>54%</td>\n</tr>\n<tr>\n<td>JSON Parsing Service</td>\n<td>32 cores vs 8 cores rpidos</td>\n<td>Throughput</td>\n<td>8K ops/sec  12K ops/sec</td>\n<td>50%</td>\n</tr>\n<tr>\n<td>WebSocket Server</td>\n<td>24 cores vs 8 cores rpidos</td>\n<td>Connection Latency</td>\n<td>25ms  12ms</td>\n<td>52%</td>\n</tr>\n</tbody>\n</table>\n<p><strong>Latency Improvements</strong>: P50 latency reductions of 30-50% and P99 latency improvements of 40-60% in latency-sensitive applications. For example, a trading system reduced order processing latency from 85ms to 35ms by switching from 32 slow cores to 8 fast cores.</p>\n<p><strong>Single-Thread Throughput</strong>: 20-40% improvement in single-threaded benchmarks (SPEC CPU benchmarks show this consistently). JavaScript V8 benchmarks show 25-35% improvements on fast-core architectures. A concrete example: parsing a 10MB JSON file takes 280ms on a slow core vs 180ms on a fast core<strong>36% faster</strong>.</p>\n<p><strong>Sequential Workload Throughput</strong>: Even with fewer total cores, applications with 10-20% sequential code show 15-30% better overall throughput because the critical path completes faster.</p>\n<p><strong>Example - Web Application</strong>: An e-commerce application with 15% sequential code (authentication, session management, order finalization) processes orders. With 32 slow cores: 15ms sequential + 85ms parallel = 100ms per order, throughput of 320 orders/second. With 8 fast cores: 6ms sequential + 85ms parallel = 91ms per order, throughput of 88 orders/second per process. Running 4 processes (total 32 cores equivalent), throughput is <strong>352 orders/second</strong><strong>10% better</strong> despite the same core count, because the sequential bottleneck is reduced.</p>\n<p><strong>Resource Utilization</strong>: Lower CPU utilization (50-70% vs 80-95%) but better response time characteristics, indicating that cores are not the bottleneck but rather single-thread speed.</p>\n<p><strong>Energy Efficiency</strong>: Better performance per watt for single-threaded workloads, though total system power may be lower with fewer cores.</p>\n<h2 id=\"common-mistakes\">Common Mistakes</h2>\n<ul>\n<li>\n<p><strong>Assuming More Cores Always Help</strong>: Adding cores to applications with sequential bottlenecks without profiling to identify the actual bottleneck</p>\n<p><strong>Example</strong>: A Node.js API server handling 3,000 requests/second on 8 cores shows 85% CPU utilization. The team adds 8 more cores (total 16), expecting 50% CPU utilization and better throughput. Result: CPU utilization drops to 60%, but latency <strong>increases by 15%</strong> (from 25ms to 29ms average) due to increased context switching overhead. The bottleneck was single-thread performance in the event loop, not lack of cores. Solution: Switch to 8 faster cores instead of adding more slow cores.</p>\n</li>\n<li>\n<p><strong>Ignoring Amdahl\'s Law</strong>: Failing to calculate theoretical speedup limits based on sequential code percentage</p>\n<p><strong>Example</strong>: An application has 10% sequential code and 90% parallelizable code. The team calculates: &quot;With 32 cores, we should get 32x speedup on the parallel part, so overall speedup should be close to 32x.&quot; Reality: Amdahl\'s Law shows maximum speedup is 1 / (0.1 + 0.9/32) = <strong>7.6x</strong>, not 32x. Adding more cores beyond 8-16 provides diminishing returns. The sequential 10% becomes the bottleneck.</p>\n</li>\n<li>\n<p><strong>Over-Parallelization</strong>: Creating excessive threads that contend for fast cores, leading to context switching overhead that negates single-thread advantages</p>\n<p><strong>Example</strong>: A Java application running on 8 fast cores creates a thread pool with 64 threads (8 threads per core). Each thread competes for CPU time, causing frequent context switches. Result: CPU spends 20% of time context switching instead of executing code. Latency is 40ms vs 18ms with an 8-thread pool (matching core count). The extra threads negate the fast-core advantage.</p>\n</li>\n<li>\n<p><strong>Mismatched Architecture Patterns</strong>: Using request-per-thread models (many threads) with fast-core architectures instead of event-driven or async models</p>\n<p><strong>Example</strong>: A C++ web server uses one thread per request (traditional Apache-style). On 32 slow cores, it handles 32 concurrent requests efficiently. When migrated to 8 fast cores, it can only handle 8 concurrent requests per process, requiring multiple processes and load balancing. The architecture doesn\'t leverage fast cores effectively. Better approach: Use async I/O (epoll/kqueue) to handle thousands of concurrent requests on 8 fast cores.</p>\n</li>\n<li>\n<p><strong>Not Profiling Single-Thread Performance</strong>: Optimizing for multi-threaded scenarios without measuring whether single-thread speed is the actual constraint</p>\n<p><strong>Example</strong>: A team observes high CPU utilization (90%) and assumes they need more cores. They add cores, but latency doesn\'t improve. Profiling reveals: single request processing takes 50ms, but only 5ms is CPU-bound (the rest is I/O wait). The bottleneck is I/O, not CPU cores. Adding fast cores won\'t help; optimizing I/O (async operations, connection pooling) is the solution.</p>\n</li>\n<li>\n<p><strong>Cache-Unaware Algorithms</strong>: Implementing algorithms that ignore cache locality, wasting the cache advantages of fast cores</p>\n<p><strong>Example</strong>: A graph traversal algorithm uses a linked list data structure, causing random memory access patterns. On fast cores with good cache prefetching, cache miss rate is still 18% due to poor locality. Reimplementing with an array-based structure improves cache locality: cache miss rate drops to 4%, and traversal speed improves by 3x. Fast cores\' cache advantages are wasted without cache-aware algorithms.</p>\n</li>\n<li>\n<p><strong>Benchmarking Synthetic Loads</strong>: Testing with perfectly parallel synthetic workloads instead of realistic production traffic patterns</p>\n<p><strong>Example</strong>: A team benchmarks their application with a synthetic workload that processes independent tasks in parallel (100% parallelizable). Results show 32 slow cores outperform 8 fast cores by 4x. They choose 32 slow cores. In production, the real workload has 20% sequential code (authentication, logging, serialization). Result: 8 fast cores actually perform 25% better than 32 slow cores because the sequential bottleneck wasn\'t captured in the synthetic benchmark.</p>\n</li>\n</ul>\n<h2 id=\"how-to-measure-and-validate\">How to Measure and Validate</h2>\n<p><strong>Profiling Tools</strong>:</p>\n<ul>\n<li>Use <code>perf</code> (Linux) to measure CPI (Cycles Per Instruction), cache misses, and branch mispredictions</li>\n<li>Profile with tools like <code>vtune</code> or <code>perf top</code> to identify if single-thread performance or parallelism is the bottleneck</li>\n<li>Measure IPC metrics: instructions retired per cycle should be higher on fast cores (typically 2-4 IPC vs 1-2 IPC on slow cores)</li>\n</ul>\n<p><strong>Example - Using <code>perf</code> to Measure CPI</strong>:</p>\n<pre><code class=\"language-bash\"># Profile a single-threaded application\nperf stat -e cycles,instructions,cache-references,cache-misses ./my-app\n\n# Example output on slow core:\n# 1,250,000,000 cycles\n# 2,187,500,000 instructions\n# CPI = 1,250,000,000 / 2,187,500,000 = 0.57 (this is actually IPC, CPI would be 1.75)\n# IPC = 2,187,500,000 / 1,250,000,000 = 1.75 instructions per cycle\n\n# Example output on fast core (same workload):\n# 625,000,000 cycles\n# 2,187,500,000 instructions\n# IPC = 2,187,500,000 / 625,000,000 = 3.5 instructions per cycle\n</code></pre>\n<p><strong>Interpreting Results</strong>: If IPC &lt; 2.0 on single-threaded workloads, the CPU is likely a bottleneck. Fast cores typically achieve IPC of 2.5-4.0 on optimized code. If CPI &gt; 2.0 (IPC &lt; 0.5), consider cores with better single-thread performance.</p>\n<p><strong>Key Metrics</strong>:</p>\n<ul>\n<li>\n<p><strong>Latency Percentiles</strong>: Track P50, P95, P99 latency - fast cores should show lower tail latencies</p>\n<p><strong>Example Benchmark Results</strong>:</p>\n<ul>\n<li>Slow cores: P50 = 45ms, P95 = 120ms, P99 = 180ms</li>\n<li>Fast cores: P50 = 22ms, P95 = 55ms, P99 = 85ms</li>\n<li>The P99 improvement (53% faster) indicates sequential bottlenecks are being resolved.</li>\n</ul>\n</li>\n<li>\n<p><strong>Single-Thread Throughput</strong>: Benchmark single-threaded execution time for critical paths</p>\n<p><strong>Example</strong>: Measure time to process 1,000 database records sequentially:</p>\n<ul>\n<li>Slow core: 12.5 seconds (12.5ms per record)</li>\n<li>Fast core: 7.8 seconds (7.8ms per record)</li>\n<li><strong>38% faster</strong> indicates the workload benefits from fast cores.</li>\n</ul>\n</li>\n<li>\n<p><strong>CPU Utilization</strong>: Lower utilization with better performance indicates single-thread speedup</p>\n<p><strong>Example</strong>: API server handling 5,000 req/sec:</p>\n<ul>\n<li>32 slow cores: 95% CPU utilization, 45ms avg latency</li>\n<li>8 fast cores: 65% CPU utilization, 22ms avg latency</li>\n<li>Lower utilization + better performance = single-thread speed is the constraint, not parallelism.</li>\n</ul>\n</li>\n<li>\n<p><strong>Context Switch Rate</strong>: Fewer context switches per request with fast cores</p>\n<p><strong>Example</strong>: <code>vmstat 1</code> shows context switches:</p>\n<ul>\n<li>Slow cores: 15,000 context switches/second, 3.0 switches per request</li>\n<li>Fast cores: 8,000 context switches/second, 1.6 switches per request</li>\n<li>Fewer switches mean less overhead and better cache locality.</li>\n</ul>\n</li>\n<li>\n<p><strong>Cache Hit Rates</strong>: Monitor L1/L2/L3 cache hit ratios - fast cores should show better locality</p>\n<p><strong>Example</strong>: Using <code>perf</code> to measure cache performance:</p>\n<pre><code class=\"language-bash\">perf stat -e L1-dcache-loads,L1-dcache-load-misses,L2-cache-loads,L2-cache-load-misses ./app\n</code></pre>\n<ul>\n<li>Slow cores: L1 miss rate 12%, L2 miss rate 8%</li>\n<li>Fast cores: L1 miss rate 5%, L2 miss rate 3%</li>\n<li>Lower miss rates indicate better cache locality and prefetching.</li>\n</ul>\n</li>\n</ul>\n<p><strong>Benchmarking Strategy</strong>:</p>\n<ol>\n<li>\n<p>Run single-threaded benchmarks (SPEC CPU, single-threaded application tests)</p>\n<p><strong>Example</strong>: SPEC CPU2017 single-threaded benchmark:</p>\n<ul>\n<li>Slow core: 35 points (normalized score)</li>\n<li>Fast core: 52 points (normalized score)</li>\n<li><strong>49% improvement</strong> in single-thread performance.</li>\n</ul>\n</li>\n<li>\n<p>Measure critical path latency under production load</p>\n</li>\n<li>\n<p>Compare same workload on many-core vs few-core-fast systems</p>\n</li>\n<li>\n<p>Use realistic load patterns, not synthetic parallel workloads</p>\n</li>\n<li>\n<p>Measure tail latencies, not just averages</p>\n</li>\n</ol>\n<p><strong>Production Validation</strong>:</p>\n<ul>\n<li>\n<p>A/B test with canary deployments comparing core configurations</p>\n<p><strong>Example Deployment Strategy</strong>:</p>\n<ol>\n<li>Deploy 10% of traffic to fast-core servers</li>\n<li>Monitor for 24-48 hours</li>\n<li>Compare metrics: latency (P50, P95, P99), error rates, throughput</li>\n<li>If fast cores show 30%+ latency improvement with stable error rates, gradually migrate traffic</li>\n</ol>\n</li>\n<li>\n<p>Monitor application-level metrics (request latency, transaction completion time)</p>\n</li>\n<li>\n<p>Track system metrics (CPU utilization, context switches, cache performance)</p>\n</li>\n<li>\n<p>Validate that improvements translate to business metrics (user experience, revenue)</p>\n</li>\n</ul>\n<h2 id=\"summary-and-key-takeaways\">Summary and Key Takeaways</h2>\n<p>The core principle: <strong>Fewer fast CPU cores outperform many slow cores when single-thread performance matters more than total parallel throughput</strong>. This trade-off is fundamental in CPU architecture and should guide hardware selection and system design.</p>\n<p>The main trade-off is between latency/sequential performance and total parallel capacity. Fast cores excel at reducing critical path latency and improving single-thread execution, while many-core systems excel at total throughput for parallelizable workloads.</p>\n<p><strong>Decision Guideline</strong>: Choose fast cores when (1) latency requirements are strict (P99 &lt; 100ms), (2) workloads have sequential dependencies (Amdahl\'s Law limits apply), (3) single-thread performance is the bottleneck (profiling confirms), or (4) cache locality matters more than parallelism. Choose many cores when (1) workloads are embarrassingly parallel, (2) total throughput is the primary metric, (3) cost per compute unit is critical, or (4) request-per-thread models handle massive concurrency.</p>\n<p>Always profile before deciding: measure IPC, latency percentiles, and identify whether the bottleneck is sequential execution or parallel capacity.</p>\n<!--kg-card-end: markdown-->','694ff07e19c94fae00e37345','Prefer Fewer Fast CPU Cores Over Many Slow Ones for Latency-Sensitive Workloads\n\n\n\nSubtitle\n\n\nUnderstanding the trade-off between CPU core count and single-thread performance is critical for optimizing latency-sensitive applications where sequential execution matters more than raw parallelism.\n\n\n\nExecutive Summary (TL;DR)\n\n\nChoosing fewer fast CPU cores over many slow cores is a performance principle that prioritizes single-thread performance and instruction-level parallelism (IPC) over core count. This approach delivers 20-40% better single-threaded performance, significantly lower latency for critical operations (often reducing response times from 50-100ms to 20-40ms), and improved throughput for applications with sequential dependencies. However, it sacrifices overall parallel throughput and scales poorly for highly concurrent workloads. Use this approach when latency matters more than throughput, for single-threaded applications, or when Amdahl\'s Law limits parallel speedup.\n\n\n\nProblem Context\n\n\nA common misconception in performance optimization is that more CPU cores always translate to better performance. Engineers often scale systems horizontally by adding cores without considering single-thread performance characteristics. This leads to scenarios where applications with sequential bottlenecks or latency-sensitive operations underperform despite high core counts.\n\n\nExample: API REST Server Performance Issue\n\n\nConsider a production API server handling authentication and data processing requests. A team migrates from an 8-core fast CPU (Intel Xeon @ 3.8 GHz) to a 32-core server CPU (AMD EPYC @ 2.5 GHz), expecting 4x better throughput. However, they observe the opposite: average response time increases from 35ms to 100ms, and P99 latency jumps from 65ms to 180ms.\n\n\nThe root cause: each API request has 8ms of sequential processing (input validation, authentication token verification, database connection setup, response serialization). With 32 slow cores, this sequential portion takes 15ms. With 8 fast cores, it takes 3ms. Even though the parallelizable database query runs slightly faster on more cores (12ms vs 15ms), the sequential bottleneck dominates: 32 slow cores = 15ms sequential + 12ms parallel = 27ms per request, but context switching overhead pushes it to 100ms. 8 fast cores = 3ms sequential + 15ms parallel = 18ms per request, with minimal overhead.\n\n\nMany production systems suffer from this because modern CPUs designed for high core counts (such as some server-class processors) prioritize parallelism over single-thread speed. These processors trade higher clock frequencies and aggressive out-of-order execution for more cores, resulting in lower IPC (Instructions Per Cycle) and slower individual core performance.\n\n\nNaive scaling attempts often fail because:\n\n\n * Sequential dependencies prevent effective parallelization (Amdahl\'s Law)\n * Synchronization overhead increases with more cores\n * Cache coherency traffic scales non-linearly with core count\n * Critical paths remain bounded by single-thread performance\n\n\n\nHow It Works\n\n\nCPU performance is determined by three key factors: clock frequency, IPC (Instructions Per Cycle), and core count. Fast cores achieve higher performance through:\n\n\nClock Frequency: Higher clock rates allow more instructions to execute per second. Modern fast cores (e.g., Intel Core series, AMD Ryzen high-frequency variants) run at 4-6 GHz, while many-core processors often operate at 2-3 GHz.\n\n\nReal-World Comparison:\n\n\n * Many-core CPU: AMD EPYC 7543 (32 cores @ 2.8 GHz base, IPC ~1.8)\n * Fast-core CPU: Intel Core i9-12900K (8 performance cores @ 5.0 GHz, IPC ~3.2)\n\n\nPerformance Calculation:\n\nA single fast core at 5.0 GHz with IPC 3.2 can execute approximately 16 billion instructions per second (5.0  10 Hz  3.2 IPC = 16  10 instructions/sec). A single slow core at 2.8 GHz with IPC 1.8 executes approximately 5 billion instructions per second (2.8  10 Hz  1.8 IPC = 5.04  10 instructions/sec).\n\n\nFor a sequential algorithm (e.g., parsing JSON, computing a hash, traversing a linked list), the fast core completes in 3.1 seconds what the slow core takes 10 seconds. Even with 3 slow cores working in parallel, the fast core still wins because the work cannot be effectively parallelized.\n\n\nInstruction-Level Parallelism (IPC): Fast cores employ deeper pipelines, wider execution units, larger instruction windows, and more aggressive branch prediction. This allows them to extract more parallelism from sequential code, executing multiple independent instructions simultaneously within a single thread.\n\n\nExample: Consider this sequential code:\n\n\nlet sum = 0;\nfor (let i = 0; i < array.length; i++) {\n    sum += array[i] * 2;\n}\n\n\n\nA fast core with wide execution units can execute array[i] * 2 and prepare the next iteration\'s memory fetch simultaneously, achieving IPC of 2.5-3.0. A slow core might achieve only 1.2-1.5 IPC on the same code, taking twice as long despite similar clock speeds.\n\n\nCache Hierarchy: Fast cores typically feature larger, faster L1 and L2 caches per core, reducing memory latency for single-threaded workloads. They also have better prefetching logic that predicts memory access patterns.\n\n\nOut-of-Order Execution: Advanced out-of-order execution engines can reorder and parallelize independent instructions within a single thread, effectively creating instruction-level parallelism without explicit multi-threading.\n\n\nWhen a workload has sequential dependencies or Amdahl\'s Law limits parallel speedup, a single fast core can outperform multiple slow cores by completing the critical path faster, even if total theoretical throughput is lower.\n\n\n\nWhy This Becomes a Bottleneck\n\n\nPerformance degrades when core speed is sacrificed for core count because overhead increases faster than useful work. Here\'s why fast cores outperform many slow cores:\n\n\n1 Sequential Bottlenecks (Amdahl\'s Law)\n\n\nEvery parallel algorithm has sequential portions that cannot be parallelized (data distribution, result aggregation, synchronization, I/O).\n\n\nWhy this matters: Amdahl\'s Law shows that even 5% sequential code limits speedup to 20x regardless of core count. The sequential portion becomes the bottleneck.\n\n\nSlow cores:\n\n\n * Execute sequential code slowly (e.g., 5ms for validation, logging, serialization)\n * Sequential work dominates total latency\n * More cores don\'t helpsequential work can\'t be parallelized\n\n\nFast cores:\n\n\n * Execute sequential code 2-3x faster (e.g., 2ms for the same validation work)\n * Sequential bottleneck is reduced, improving overall latency\n\n\nExample: A web server processing requests has 5ms sequential work (input validation, authentication, response formatting) and 2ms parallelizable work (database query). With 32 slow cores: 5ms sequential + 2ms parallel = 7ms total (plus 3-5ms overhead = 10-12ms). With 8 fast cores: 2ms sequential + 2ms parallel = 4ms total (minimal overhead). Fast cores reduce sequential time by 60%, resulting in 50-60% better overall latency.\n\n\n2 Context Switching Overhead\n\n\nWhen a thread is paused, the OS must save registers, stack, and CPU state. This work produces no useful output.\n\n\nSlow cores:\n\n\n * Fewer instructions per second\n * Context switches take longer in real time\n * More time is wasted switching than doing real work\n * With many threads, context switching overhead dominates (can consume 15-25% of CPU time)\n\n\nFast cores:\n\n\n * Switch faster (same number of cycles, but cycles execute faster)\n * Return to useful execution sooner\n * Overhead becomes a smaller percentage of total time (5-10% overhead)\n\n\nExample: A server with 64 threads on 32 slow cores spends 20% of CPU time context switching. The same workload on 8 fast cores spends 8% of CPU time context switching, allowing 12% more CPU time for actual work.\n\n\n3 Lock / Mutex Contention\n\n\nShared resources require locks. Only one thread can enter the critical section at a time.\n\n\nSlow cores:\n\n\n * Hold locks longer (execute critical section code slowly)\n * Other threads wait idle\n * Thread queues grow quickly\n * Lock contention escalates exponentially with more cores\n\n\nFast cores:\n\n\n * Enter, execute, and release locks quickly\n * Less waiting time for other threads\n * Higher real throughput despite fewer cores\n\n\nExample: A database connection pool with 32 threads competing for 10 connections. On 32 slow cores, threads spend 85% of their time waiting for locks (average wait time 15ms per operation because slow cores hold locks longer). On 8 fast cores with the same 32 threads, threads spend 15% of their time waiting (average wait time 2.5ms because fast cores release locks 3x faster).\n\n\n4 Cache Misses (Critical Factor)\n\n\nCPU cache is much faster than RAM (L1 cache: ~1ns, RAM: ~100ns). Cache misses occur when needed data is not in cache.\n\n\nWhy cache misses happen:\n\n\n * Context switching evicts cache lines\n * Other threads overwrite cache\n * Multiple cores invalidate each other\'s cache (cache coherence protocol)\n * Data working set does not fit in cache\n\n\nSlow cores:\n\n\n * Execute fewer instructions before being preempted\n * Lose cache more often (context switches evict cache lines)\n * Cache misses dominate execution time (each miss costs 100-300ns)\n * Higher cache miss rates (e.g., 25% miss rate with many cores)\n\n\nFast cores:\n\n\n * Finish work before cache eviction (complete tasks faster)\n * Better cache locality (fewer threads = less cache contention)\n * Memory latency is amortized over more useful work\n * Lower cache miss rates (e.g., 8% miss rate with fewer cores)\n\n\nExample: Processing a shared data structure across cores. With 32 cores, cache miss rate is 25% (cores frequently invalidate each other\'s cache lines via MESI protocol). With 8 cores, cache miss rate drops to 8% (less contention, better locality). Each cache miss adds 100-300ns latency. The 32-core system spends 25-75ns per operation waiting for memory; the 8-core system spends 8-24ns per operation.\n\n\n5 Cache Coherence & Core Synchronization\n\n\nMulti-core CPUs must keep caches consistent (MESI protocol). Cache lines are invalidated across cores when data is modified.\n\n\nSlow cores:\n\n\n * React slowly to invalidations (take longer to process coherence messages)\n * Stall other cores waiting for synchronization\n * Amplify synchronization delays (slow cores hold cache lines longer)\n * Cache ping-pong effect: data bounces between cores\' caches\n\n\nFast cores:\n\n\n * Synchronize quickly (process coherence messages faster)\n * Reduce global latency (faster invalidation propagation)\n * Less cache ping-pong (fewer cores = fewer invalidations needed)\n\n\nExample: A shared counter incremented by multiple threads. With 32 slow cores, each increment triggers cache invalidations across all cores, causing ping-pong. Each increment takes 150ns due to coherence overhead. With 8 fast cores, fewer invalidations are needed, and each increment takes 40ns3.75x faster.\n\n\n6 Blocking I/O (Network, Disk, DB)\n\n\nMany applications are I/O-bound, not CPU-bound. Threads wait for external resources (network responses, disk reads, database queries).\n\n\nSlow cores:\n\n\n * Process I/O results slowly when data arrives\n * Hold buffers and locks longer while processing\n * Increase request latency (I/O wait time + slow processing time)\n\n\nFast cores:\n\n\n * Process responses immediately when I/O completes\n * Free resources quickly (buffers, locks, connections)\n * Lower end-to-end latency (I/O wait time + fast processing time)\n\n\nExample: An e-commerce API processes orders with 99% parallel work (payment processing, inventory checks) and 1% sequential work (final order commit to database). The sequential database write takes 50ms on slow cores vs 20ms on fast cores. Even though 99% of work completes in 10ms on both systems, P99 latency is 60ms on slow cores vs 30ms on fast coresthe sequential I/O bottleneck dominates tail latency.\n\n\n7 Net Effect\n\n\nMore cores  more performance when overhead dominates\n\n\n * Overhead increases with core count (context switching, lock contention, cache coherence)\n * Slow cores amplify overhead (each overhead event takes longer)\n * Fast cores minimize overhead (overhead events complete faster)\n * Sequential work can\'t be parallelizedfast cores are the only solution\n * Real throughput = Useful work / (Useful work + Overhead). Fast cores reduce both numerator (faster work) and denominator (less overhead)\n\n\nExample Summary: A web server handling 10,000 requests/second. With 32 slow cores: 7ms useful work + 5ms overhead = 12ms per request. With 8 fast cores: 4ms useful work + 1ms overhead = 5ms per request. Fast cores deliver 2.4x better latency with 4x fewer cores.\n\n\n\nAdvantages\n\n\n * Superior Single-Thread Performance: 20-40% better performance on single-threaded workloads compared to many-core processors with lower IPC\n * Reduced Latency: Critical operations complete 50-100% faster (e.g., reducing response times from 50-100ms to 20-40ms in observed benchmarks)\n * Lower Synchronization Overhead: Fewer cores mean fewer lock contentions, atomic operations, and cache coherency messages\n * Better Cache Locality: Larger per-core caches and fewer threads reduce cache misses and improve memory access patterns\n * Simpler Architecture: Less need for complex thread pools, work-stealing schedulers, and parallel algorithms\n * Predictable Performance: More deterministic latency characteristics, crucial for real-time systems\n * Higher Throughput for Sequential Workloads: Applications that cannot be parallelized achieve 15-30% better throughput even with fewer total cores\n\n\n\nDisadvantages and Trade-offs\n\n\n * Limited Parallel Throughput: Cannot match the total compute capacity of many-core systems for embarrassingly parallel workloads\n * Higher Per-Core Cost: Fast cores with advanced microarchitecture features are more expensive to manufacture\n * Poor Scalability for Concurrent Workloads: Request-per-thread server models (like traditional web servers) benefit less when request count exceeds fast core count\n * Lower Total Compute Capacity: Cannot execute as many independent threads simultaneously\n * Underutilization Risk: If workloads can be fully parallelized, fast cores may sit idle while waiting for I/O, wasting resources\n * Memory Bandwidth Limits: Fewer cores mean fewer memory controllers, potentially limiting memory bandwidth for data-parallel workloads\n\n\n\nWhen to Use This Approach\n\n\n * \n   \n   \n   Latency-Sensitive Applications: Real-time systems, trading platforms, gaming servers, interactive applications where P99 latency matters more than throughput\n   \n   \n   Example - Trading Platform: A high-frequency trading system processing order execution. With 32 slow cores, P99 latency is 85ms (order validation, risk checks, market data processing are sequential). With 8 fast cores, P99 latency drops to 35msa 58% improvement. This directly impacts profitability, as faster execution captures better prices.\n   \n\n * \n   \n   \n   Sequential Workloads: Applications with Amdahl\'s Law limits, legacy code that cannot be parallelized, algorithms with strong data dependencies\n   \n\n * \n   \n   \n   Single-Threaded Applications: Node.js event loops, Python GIL-bound code, JavaScript engines, database query execution (single query optimization)\n   \n   \n   Example - Node.js API: A REST API built with Node.js handling 5,000 requests/second. Node.js runs on a single event loop thread per process. With 16 slow cores running 4 Node.js processes (4 cores each), CPU utilization is 95% and average latency is 45ms. With 8 fast cores running 4 Node.js processes (2 cores each), CPU utilization is 65% and average latency is 22ms51% faster response times with lower resource usage.\n   \n\n * \n   \n   \n   Cache-Sensitive Workloads: Applications where cache locality matters more than parallelism (many scientific computing kernels, graph algorithms)\n   \n\n * \n   \n   \n   I/O-Bound Systems with Critical Paths: Systems where CPU work per request is minimal but latency is critical (API gateways, load balancers with simple routing logic)\n   \n\n * \n   \n   \n   Database Query Execution: Single complex queries that cannot be parallelized effectively.\n   \n   \n   Example - Database Query: A complex SQL query with multiple joins and aggregations executes on a single thread. On 32 slow cores, the query takes 450ms (single-threaded execution doesn\'t benefit from extra cores). On 8 fast cores, the same query completes in 280msa 38% improvement because the single query thread runs faster.\n   \n\n * \n   \n   \n   Development and Testing: Faster iteration cycles when single-thread performance improves compile times and test execution speed\n   \n\n\n\nWhen Not to Use It\n\n\n * \n   \n   \n   Highly Parallel Workloads: Data processing pipelines, batch jobs, scientific simulations with perfect parallelism (no shared state)\n   \n\n * \n   \n   \n   Embarrassingly Parallel Problems: Image processing, video encoding, Monte Carlo simulations where each unit of work is independent\n   \n   \n   Example - Video Encoding: Encoding a 4K video (38402160, 60fps, 10 minutes) using H.264 encoding. With 32 slow cores, encoding completes in 15 minutes (workload perfectly parallelizes across frames). With 8 fast cores, encoding takes 42 minutes2.8x slower because the workload benefits from core count, not core speed.\n   \n   \n   Example - Batch Image Processing: Processing 1 million images (resizing, format conversion, thumbnail generation). With 32 slow cores, the batch completes in 2 hours (31,250 images per hour per core  32 cores). With 8 fast cores, it takes 8 hours (125,000 images per hour per core  8 cores, but each core processes faster, still net slower than 32 cores).\n   \n\n * \n   \n   \n   Request-Per-Thread Servers: Traditional threaded web servers handling thousands of concurrent connections (more cores allow more simultaneous request processing)\n   \n\n * \n   \n   \n   Cost-Optimized Deployments: When total compute capacity per dollar is the primary metric and latency requirements are relaxed\n   \n\n * \n   \n   \n   Cloud Environments with Auto-Scaling: When horizontal scaling is cheaper than vertical scaling and workload can be distributed\n   \n\n * \n   \n   \n   Containers with Thread Pools: Applications using thread pools larger than available fast cores, where additional slow cores provide better resource utilization\n   \n\n\n\nPerformance Impact\n\n\nReal-world observations show measurable improvements across different workload types:\n\n\nComparative Performance Table:\n\n\n\n\n\nCaso de Uso\nConfiguracin\nMtrica Clave\nResultado\nMejora\n\n\n\n\nTrading System\n32 cores lentos vs 8 cores rpidos\nP99 Latency\n85ms  35ms\n58%\n\n\nNode.js REST API\n16 cores lentos vs 8 cores rpidos\nAvg Latency\n45ms  22ms\n51%\n\n\nDatabase Query (Single-threaded)\n32 cores vs 8 cores rpidos\nQuery Time\n450ms  280ms\n38%\n\n\nMicroservices Gateway\n16 cores vs 8 cores rpidos\nP95 Latency\n120ms  55ms\n54%\n\n\nJSON Parsing Service\n32 cores vs 8 cores rpidos\nThroughput\n8K ops/sec  12K ops/sec\n50%\n\n\nWebSocket Server\n24 cores vs 8 cores rpidos\nConnection Latency\n25ms  12ms\n52%\n\n\n\n\n\nLatency Improvements: P50 latency reductions of 30-50% and P99 latency improvements of 40-60% in latency-sensitive applications. For example, a trading system reduced order processing latency from 85ms to 35ms by switching from 32 slow cores to 8 fast cores.\n\n\nSingle-Thread Throughput: 20-40% improvement in single-threaded benchmarks (SPEC CPU benchmarks show this consistently). JavaScript V8 benchmarks show 25-35% improvements on fast-core architectures. A concrete example: parsing a 10MB JSON file takes 280ms on a slow core vs 180ms on a fast core36% faster.\n\n\nSequential Workload Throughput: Even with fewer total cores, applications with 10-20% sequential code show 15-30% better overall throughput because the critical path completes faster.\n\n\nExample - Web Application: An e-commerce application with 15% sequential code (authentication, session management, order finalization) processes orders. With 32 slow cores: 15ms sequential + 85ms parallel = 100ms per order, throughput of 320 orders/second. With 8 fast cores: 6ms sequential + 85ms parallel = 91ms per order, throughput of 88 orders/second per process. Running 4 processes (total 32 cores equivalent), throughput is 352 orders/second10% better despite the same core count, because the sequential bottleneck is reduced.\n\n\nResource Utilization: Lower CPU utilization (50-70% vs 80-95%) but better response time characteristics, indicating that cores are not the bottleneck but rather single-thread speed.\n\n\nEnergy Efficiency: Better performance per watt for single-threaded workloads, though total system power may be lower with fewer cores.\n\n\n\nCommon Mistakes\n\n\n * \n   \n   \n   Assuming More Cores Always Help: Adding cores to applications with sequential bottlenecks without profiling to identify the actual bottleneck\n   \n   \n   Example: A Node.js API server handling 3,000 requests/second on 8 cores shows 85% CPU utilization. The team adds 8 more cores (total 16), expecting 50% CPU utilization and better throughput. Result: CPU utilization drops to 60%, but latency increases by 15% (from 25ms to 29ms average) due to increased context switching overhead. The bottleneck was single-thread performance in the event loop, not lack of cores. Solution: Switch to 8 faster cores instead of adding more slow cores.\n   \n\n * \n   \n   \n   Ignoring Amdahl\'s Law: Failing to calculate theoretical speedup limits based on sequential code percentage\n   \n   \n   Example: An application has 10% sequential code and 90% parallelizable code. The team calculates: \"With 32 cores, we should get 32x speedup on the parallel part, so overall speedup should be close to 32x.\" Reality: Amdahl\'s Law shows maximum speedup is 1 / (0.1 + 0.9/32) = 7.6x, not 32x. Adding more cores beyond 8-16 provides diminishing returns. The sequential 10% becomes the bottleneck.\n   \n\n * \n   \n   \n   Over-Parallelization: Creating excessive threads that contend for fast cores, leading to context switching overhead that negates single-thread advantages\n   \n   \n   Example: A Java application running on 8 fast cores creates a thread pool with 64 threads (8 threads per core). Each thread competes for CPU time, causing frequent context switches. Result: CPU spends 20% of time context switching instead of executing code. Latency is 40ms vs 18ms with an 8-thread pool (matching core count). The extra threads negate the fast-core advantage.\n   \n\n * \n   \n   \n   Mismatched Architecture Patterns: Using request-per-thread models (many threads) with fast-core architectures instead of event-driven or async models\n   \n   \n   Example: A C++ web server uses one thread per request (traditional Apache-style). On 32 slow cores, it handles 32 concurrent requests efficiently. When migrated to 8 fast cores, it can only handle 8 concurrent requests per process, requiring multiple processes and load balancing. The architecture doesn\'t leverage fast cores effectively. Better approach: Use async I/O (epoll/kqueue) to handle thousands of concurrent requests on 8 fast cores.\n   \n\n * \n   \n   \n   Not Profiling Single-Thread Performance: Optimizing for multi-threaded scenarios without measuring whether single-thread speed is the actual constraint\n   \n   \n   Example: A team observes high CPU utilization (90%) and assumes they need more cores. They add cores, but latency doesn\'t improve. Profiling reveals: single request processing takes 50ms, but only 5ms is CPU-bound (the rest is I/O wait). The bottleneck is I/O, not CPU cores. Adding fast cores won\'t help; optimizing I/O (async operations, connection pooling) is the solution.\n   \n\n * \n   \n   \n   Cache-Unaware Algorithms: Implementing algorithms that ignore cache locality, wasting the cache advantages of fast cores\n   \n   \n   Example: A graph traversal algorithm uses a linked list data structure, causing random memory access patterns. On fast cores with good cache prefetching, cache miss rate is still 18% due to poor locality. Reimplementing with an array-based structure improves cache locality: cache miss rate drops to 4%, and traversal speed improves by 3x. Fast cores\' cache advantages are wasted without cache-aware algorithms.\n   \n\n * \n   \n   \n   Benchmarking Synthetic Loads: Testing with perfectly parallel synthetic workloads instead of realistic production traffic patterns\n   \n   \n   Example: A team benchmarks their application with a synthetic workload that processes independent tasks in parallel (100% parallelizable). Results show 32 slow cores outperform 8 fast cores by 4x. They choose 32 slow cores. In production, the real workload has 20% sequential code (authentication, logging, serialization). Result: 8 fast cores actually perform 25% better than 32 slow cores because the sequential bottleneck wasn\'t captured in the synthetic benchmark.\n   \n\n\n\nHow to Measure and Validate\n\n\nProfiling Tools:\n\n\n * Use perf (Linux) to measure CPI (Cycles Per Instruction), cache misses, and branch mispredictions\n * Profile with tools like vtune or perf top to identify if single-thread performance or parallelism is the bottleneck\n * Measure IPC metrics: instructions retired per cycle should be higher on fast cores (typically 2-4 IPC vs 1-2 IPC on slow cores)\n\n\nExample - Using perf to Measure CPI:\n\n\n# Profile a single-threaded application\nperf stat -e cycles,instructions,cache-references,cache-misses ./my-app\n\n# Example output on slow core:\n# 1,250,000,000 cycles\n# 2,187,500,000 instructions\n# CPI = 1,250,000,000 / 2,187,500,000 = 0.57 (this is actually IPC, CPI would be 1.75)\n# IPC = 2,187,500,000 / 1,250,000,000 = 1.75 instructions per cycle\n\n# Example output on fast core (same workload):\n# 625,000,000 cycles\n# 2,187,500,000 instructions\n# IPC = 2,187,500,000 / 625,000,000 = 3.5 instructions per cycle\n\n\n\nInterpreting Results: If IPC < 2.0 on single-threaded workloads, the CPU is likely a bottleneck. Fast cores typically achieve IPC of 2.5-4.0 on optimized code. If CPI > 2.0 (IPC < 0.5), consider cores with better single-thread performance.\n\n\nKey Metrics:\n\n\n * \n   \n   \n   Latency Percentiles: Track P50, P95, P99 latency - fast cores should show lower tail latencies\n   \n   \n   Example Benchmark Results:\n   \n   \n   * Slow cores: P50 = 45ms, P95 = 120ms, P99 = 180ms\n   * Fast cores: P50 = 22ms, P95 = 55ms, P99 = 85ms\n   * The P99 improvement (53% faster) indicates sequential bottlenecks are being resolved.\n   \n * \n   \n   \n   Single-Thread Throughput: Benchmark single-threaded execution time for critical paths\n   \n   \n   Example: Measure time to process 1,000 database records sequentially:\n   \n   \n   * Slow core: 12.5 seconds (12.5ms per record)\n   * Fast core: 7.8 seconds (7.8ms per record)\n   * 38% faster indicates the workload benefits from fast cores.\n   \n * \n   \n   \n   CPU Utilization: Lower utilization with better performance indicates single-thread speedup\n   \n   \n   Example: API server handling 5,000 req/sec:\n   \n   \n   * 32 slow cores: 95% CPU utilization, 45ms avg latency\n   * 8 fast cores: 65% CPU utilization, 22ms avg latency\n   * Lower utilization + better performance = single-thread speed is the constraint, not parallelism.\n   \n * \n   \n   \n   Context Switch Rate: Fewer context switches per request with fast cores\n   \n   \n   Example: vmstat 1 shows context switches:\n   \n   \n   * Slow cores: 15,000 context switches/second, 3.0 switches per request\n   * Fast cores: 8,000 context switches/second, 1.6 switches per request\n   * Fewer switches mean less overhead and better cache locality.\n   \n * \n   \n   \n   Cache Hit Rates: Monitor L1/L2/L3 cache hit ratios - fast cores should show better locality\n   \n   \n   Example: Using perf to measure cache performance:\n   \n   \n   perf stat -e L1-dcache-loads,L1-dcache-load-misses,L2-cache-loads,L2-cache-load-misses ./app\n   \n   \n   \n   * Slow cores: L1 miss rate 12%, L2 miss rate 8%\n   * Fast cores: L1 miss rate 5%, L2 miss rate 3%\n   * Lower miss rates indicate better cache locality and prefetching.\n   \n\n\nBenchmarking Strategy:\n\n\n 1. \n    \n    \n    Run single-threaded benchmarks (SPEC CPU, single-threaded application tests)\n    \n    \n    Example: SPEC CPU2017 single-threaded benchmark:\n    \n    \n    * Slow core: 35 points (normalized score)\n    * Fast core: 52 points (normalized score)\n    * 49% improvement in single-thread performance.\n    \n 2. \n    \n    \n    Measure critical path latency under production load\n    \n\n 3. \n    \n    \n    Compare same workload on many-core vs few-core-fast systems\n    \n\n 4. \n    \n    \n    Use realistic load patterns, not synthetic parallel workloads\n    \n\n 5. \n    \n    \n    Measure tail latencies, not just averages\n    \n\n\nProduction Validation:\n\n\n * \n   \n   \n   A/B test with canary deployments comparing core configurations\n   \n   \n   Example Deployment Strategy:\n   \n   \n   1. Deploy 10% of traffic to fast-core servers\n   2. Monitor for 24-48 hours\n   3. Compare metrics: latency (P50, P95, P99), error rates, throughput\n   4. If fast cores show 30%+ latency improvement with stable error rates, gradually migrate traffic\n   \n * \n   \n   \n   Monitor application-level metrics (request latency, transaction completion time)\n   \n\n * \n   \n   \n   Track system metrics (CPU utilization, context switches, cache performance)\n   \n\n * \n   \n   \n   Validate that improvements translate to business metrics (user experience, revenue)\n   \n\n\n\nSummary and Key Takeaways\n\n\nThe core principle: Fewer fast CPU cores outperform many slow cores when single-thread performance matters more than total parallel throughput. This trade-off is fundamental in CPU architecture and should guide hardware selection and system design.\n\n\nThe main trade-off is between latency/sequential performance and total parallel capacity. Fast cores excel at reducing critical path latency and improving single-thread execution, while many-core systems excel at total throughput for parallelizable workloads.\n\n\nDecision Guideline: Choose fast cores when (1) latency requirements are strict (P99 < 100ms), (2) workloads have sequential dependencies (Amdahl\'s Law limits apply), (3) single-thread performance is the bottleneck (profiling confirms), or (4) cache locality matters more than parallelism. Choose many cores when (1) workloads are embarrassingly parallel, (2) total throughput is the primary metric, (3) cost per compute unit is critical, or (4) request-per-thread models handle massive concurrency.\n\n\nAlways profile before deciding: measure IPC, latency percentiles, and identify whether the bottleneck is sequential execution or parallel capacity.\n',NULL,0,'post','published',NULL,'public','all','2025-12-27 14:43:10','2025-12-27 14:43:47','2025-12-27 14:43:11','694f2665c3e44e650f0e5dd8',NULL,NULL,NULL,NULL,NULL,NULL,1),('6950348e19c94fae00e37362','f78a677c-3867-40fb-b62d-f513cd5919d0','Reduce Context Switching','reduce-context-switching',NULL,'{\"root\":{\"children\":[{\"type\":\"markdown\",\"version\":1,\"markdown\":\"## Subtitle\\nContext switching overhead becomes a measurable performance bottleneck in high-throughput systems when too many threads or processes compete for limited CPU resources.\\n\\n---\\n\\n## Executive Summary (TL;DR)\\nContext switching happens when the operating system stops one thread and starts another. Each switch requires coordination work and often forces the CPU to repeat work it was already doing efficiently. When context switching happens too often, the CPU spends more time coordinating execution than doing useful work. Reducing unnecessary context switches improves throughput, lowers latency (especially worst-case latency), and makes performance more predictable. This optimization matters most in CPU-bound workloads where thread count exceeds CPU core count.\\n\\n---\\n\\n## Problem Context\\nA common misconception is that increasing the number of threads automatically improves performance. This leads to designs where a new thread is created for every request or task. While this can work for I/O-bound workloads (where threads mostly wait), it becomes harmful for CPU-bound workloads.\\n\\nA CPU has a fixed number of cores, and each core can only execute one thread at a time. When an application creates more threads than available cores, the operating system must constantly stop and resume threads so they can take turns using the CPU. This constant stopping and starting introduces overhead and reduces efficiency.\\n\\nIn real systems, this often appears as:\\n- High CPU usage but low throughput\\n- Requests becoming slower under load\\n- Performance degrading as more threads are added\\n\\nTypical mistakes include:\\n- Creating thousands of threads\\n- Blocking threads on I/O\\n- Misconfigured thread pools\\n- Ignoring CPU core limits\\n- Mixing CPU-bound and I/O-bound work in the same execution model\\n\\n---\\n\\n## How Context Switching Works\\nA **context switch** occurs when the operating system:\\n1. Stops the currently running thread\\n2. Saves its execution state\\n3. Loads the execution state of another thread\\n4. Resumes execution of that thread\\n\\nEven though modern systems are fast, this process still takes time. More importantly, it interrupts the natural flow of execution. When this interruption happens repeatedly, the CPU becomes less efficient because it cannot build momentum executing useful work.\\n\\n---\\n\\n## Voluntary vs Involuntary Context Switching\\n\\n### Voluntary Context Switching\\nOccurs when a thread cannot continue and explicitly stops:\\n- Waiting for disk or network I/O\\n- Waiting for a lock\\n- Calling `sleep` or `yield`\\n\\n### Involuntary Context Switching\\nOccurs when the operating system interrupts a running thread:\\n- To give CPU time to another thread\\n- To enforce fairness when many threads compete\\n\\n**Key idea**  \\nVoluntary switches come from waiting.  \\nInvoluntary switches come from too many runnable threads.\\n\\n---\\n\\n## Why This Becomes a Bottleneck\\n\\nContext switching becomes a bottleneck when threads are interrupted so frequently that useful work is broken into very small pieces.\\n\\nA CPU core can only run one thread at a time. If more threads want to run than there are cores, the operating system must constantly rotate between them. Each rotation introduces overhead.\\n\\nWhen the number of threads is small, this overhead is negligible. When the number grows, interruptions become frequent and expensive.\\n\\nFrequent interruption slows execution. Slower execution means threads stay active longer. Threads that stay active longer increase contention. Increased contention causes even more interruptions.\\n\\nBlocking behavior worsens the situation. Threads that wait for I/O or locks stop and later resume. Each stop and resume adds more switching and more coordination work.\\n\\nMovement of threads between CPU cores also hurts efficiency. When a thread resumes execution on a different core, it must re-establish execution context, which slows progress and increases the likelihood of further interruption.\\n\\nAll these effects reinforce each other:\\n- More threads  more interruptions  \\n- More interruptions  slower progress  \\n- Slower progress  more contention  \\n- More contention  even more interruptions  \\n\\nThis feedback loop explains why systems with excessive threading often show high CPU usage, low throughput, poor scalability, and unstable latency.\\n\\n---\\n\\n## Time Quantum and Scheduling Behavior\\n\\nA **time quantum** (also called *time slice*) is the amount of CPU time a runnable thread is allowed to execute before the operating system scheduler may preempt it and switch to another thread.\\n\\n### Is the time quantum fixed?\\nNo. The time quantum is **not a fixed, universal duration**.\\n\\nIts effective length varies depending on:\\n- The operating system scheduler implementation\\n- The scheduling policy in use\\n- The number of runnable threads in the run queue\\n- Thread priorities\\n- Overall system load\\n\\nModern schedulers do not aim to give each thread a fixed time slice. Instead, they aim for **fairness** across all runnable threads.\\n\\n### How the quantum behaves in practice\\n- When **few threads** are runnable, each thread runs longer without interruption.\\n- When **many threads** are runnable, the scheduler reduces how long each thread can run.\\n- As the run queue grows, uninterrupted execution time per thread shrinks.\\n\\nIn effect:\\n- More runnable threads  shorter execution windows\\n- Shorter execution windows  more preemption\\n- More preemption  more involuntary context switches\\n\\nThis is why oversubscribing the CPU with runnable threads directly increases context switching overhead.\\n\\n---\\n\\n## What Causes a Thread to Context Switch\\n\\nA thread context switch occurs whenever the operating system stops one thread and allows another to run. This can happen for several reasons.\\n\\n### 1. Time Quantum Expiration (Preemption)\\nIf a thread exhausts its allowed execution window, the scheduler may preempt it to give another runnable thread CPU time.\\n\\nThis is the most common cause of **involuntary context switching**, especially when:\\n- There are more runnable threads than CPU cores\\n- Threads are CPU-bound and do not block naturally\\n\\n---\\n\\n### 2. Blocking Operations (Voluntary Switching)\\nA thread voluntarily stops executing when it cannot continue:\\n- Waiting for disk or network I/O\\n- Waiting to acquire a lock\\n- Waiting on a condition variable\\n- Calling `sleep` or `yield`\\n\\nThe scheduler then switches to another runnable thread.\\n\\n---\\n\\n### 3. Lock Contention\\nWhen a thread attempts to acquire a lock that is already held:\\n- The thread blocks\\n- The scheduler switches to another thread\\n- The blocked thread later resumes when the lock becomes available\\n\\nHigh lock contention causes frequent stop-and-resume cycles and increases context switching.\\n\\n---\\n\\n### 4. Higher-Priority Threads Becoming Runnable\\nIf a higher-priority thread becomes runnable:\\n- The scheduler may immediately preempt the currently running thread\\n- A context switch occurs even if the current thread is making progress\\n\\n---\\n\\n### 5. Thread Migration Between CPU Cores\\nThreads may resume execution on a different CPU core than where they previously ran.\\n\\nWhile this does not always cause an immediate context switch by itself, it:\\n- Slows execution\\n- Increases execution time\\n- Increases the likelihood of further preemption\\n\\nIndirectly, this leads to more context switching.\\n\\n---\\n\\n### 6. Operating System Interrupts\\nHardware interrupts (network, disk, timers) temporarily stop user threads so the kernel can handle events.\\n\\nAfter interrupt handling completes, the scheduler decides which thread runs next, which may result in a context switch.\\n\\n---\\n\\n## Why Fewer Context Switches Improve Overall Performance\\n\\nReducing context switching improves performance not because context switches are inherently bad, but because **every context switch interrupts multiple layers of work that are already operating efficiently**. When switches happen too often, those layers never reach a stable, optimized state.\\n\\nBelow are the most important performance relationships affected by context switching, explained without diving into hardware internals.\\n\\n---\\n\\n### 1. Longer Uninterrupted Execution Means Less Rework\\n\\nWhen a thread runs continuously, it makes steady progress. When it is interrupted frequently, its work is fragmented.\\n\\nFragmented work often requires:\\n- Re-checking conditions\\n- Re-entering code paths\\n- Re-establishing execution flow\\n\\nEven on fast CPUs, restarting work repeatedly is slower than continuing smoothly.\\n\\n**Fewer context switches  longer uninterrupted execution  less repeated work**\\n\\n---\\n\\n### 2. Better Reuse of Data and State\\n\\nWhile a thread is running, the system naturally keeps the data it uses readily accessible. When execution switches to another thread, different data is accessed and the previous data becomes less immediately available.\\n\\nWhen the original thread resumes, accessing its data again takes longer than if it had continued running.\\n\\nYou dont need to know hardware details to see the effect:\\n- Continuous execution keeps data close\\n- Frequent switching pushes data farther away\\n\\n**Fewer context switches  better data reuse  faster execution**\\n\\n---\\n\\n### 3. Less Time Spent Coordinating, More Time Doing Work\\n\\nEvery context switch requires the operating system to:\\n- Decide which thread should run\\n- Stop one execution\\n- Start another execution\\n\\nThis coordination work is necessary, but it produces no business value. It does not process requests, compute results, or serve users.\\n\\nReducing context switches reduces this pure overhead.\\n\\n**Fewer context switches  less coordination  more CPU time for real work**\\n\\n---\\n\\n### 4. Fewer Artificial Execution Interruptions\\n\\nContext switches often interrupt threads that are actively making progress. These interruptions break the natural flow of execution and reduce efficiency.\\n\\nAllowing threads to run longer reduces artificial pauses and improves overall execution efficiency.\\n\\n**Fewer context switches  smoother execution  higher efficiency**\\n\\n---\\n\\n### 5. Faster Completion of Tasks\\n\\nWhen threads are interrupted frequently, tasks take longer to complete because they are repeatedly paused and resumed.\\n\\nLonger task durations create secondary problems:\\n- Resources are held longer\\n- More threads remain active\\n- System pressure increases\\n\\nReducing context switching shortens task lifetimes and lowers overall contention.\\n\\n**Fewer context switches  faster task completion  less system pressure**\\n\\n---\\n\\n### 6. More Predictable Performance and Latency\\n\\nFrequent context switching introduces randomness:\\n- Some threads are interrupted many times\\n- Others run longer\\n- Latency becomes inconsistent\\n\\nReducing context switching stabilizes execution timing and improves predictability, especially for worst-case latency.\\n\\n**Fewer context switches  less variability  more predictable performance**\\n\\n---\\n\\n### 7. Better Behavior Under Load\\n\\nUnder load, negative effects amplify:\\n- Slower execution increases contention\\n- Increased contention increases switching\\n- Increased switching slows execution further\\n\\nReducing context switching weakens this negative feedback loop and allows the system to degrade more gracefully under stress.\\n\\n**Fewer context switches  weaker feedback loops  better scalability**\\n\\n---\\n\\n## How to Reduce Context Switching\\n\\nReducing context switching means **reducing how often threads are stopped and restarted**. The goal is not to eliminate concurrency, but to avoid unnecessary interruptions.\\n\\n---\\n\\n### 1. Use Thread Pools Instead of Creating Threads\\n\\n**What goes wrong**  \\nCreating a new thread for each task quickly leads to a large number of threads that all want CPU time. Since the CPU can only run a limited number of threads at once, the operating system must constantly stop one thread and start another so they can take turns.\\n\\n**What changes**  \\nThread pools create a fixed number of threads ahead of time and reuse them for many tasks. New work waits in a queue instead of creating new threads.\\n\\n**Why it helps**  \\nBecause the number of threads is limited, fewer threads compete for the CPU at the same time. This reduces how often the operating system needs to interrupt threads, which directly reduces context switching.\\n\\n---\\n\\n### 2. Limit Concurrency to CPU Cores for CPU-Bound Work\\n\\n**What goes wrong**  \\nCPU-bound tasks always want to run and do not naturally pause. If there are more CPU-bound threads than CPU cores, the operating system must constantly rotate between them, interrupting each one even while it is making progress.\\n\\n**What changes**  \\nConcurrency is limited so that only as many CPU-bound tasks run at the same time as there are CPU cores.\\n\\n**Why it helps**  \\nWhen the number of running threads matches the number of cores, threads can run longer without being interrupted. Fewer forced interruptions mean fewer involuntary context switches.\\n\\n---\\n\\n### 3. Use Async / Non-Blocking I/O Instead of Blocking I/O\\n\\n**What goes wrong**  \\nWith blocking I/O, a thread starts an operation (like a network or disk call) and then stops doing anything while waiting for the result. That thread must later be restarted when the operation completes, causing additional context switches.\\n\\n**What changes**  \\nWith async I/O, the thread starts the operation and immediately returns. The operating system notifies the application later when the I/O is complete, without stopping a thread just to wait.\\n\\n**Why it helps**  \\nThreads are no longer stopped and restarted simply to wait for I/O. Fewer threads move between running and waiting states, which significantly reduces context switching.\\n\\n---\\n\\n### 4. Minimize Lock Contention\\n\\n**What goes wrong**  \\nWhen multiple threads need the same lock, some threads must wait. Waiting threads stop running and later resume when the lock becomes available. Each wait and resume introduces context switches.\\n\\n**What changes**  \\nReducing shared state, using finer-grained locks, or redesigning code to avoid locks reduces how often threads must wait.\\n\\n**Why it helps**  \\nWhen threads wait less, they are stopped and restarted less often. This directly reduces both voluntary and involuntary context switches.\\n\\n---\\n\\n### 5. Reduce Thread Migration Between CPU Cores\\n\\n**What goes wrong**  \\nThreads may resume execution on a different CPU core than the one they previously ran on. When this happens, execution becomes less efficient and threads take longer to complete their work.\\n\\n**What changes**  \\nKeeping threads on the same CPU core when possible improves execution continuity.\\n\\n**Why it helps**  \\nThreads that run more efficiently finish their work faster and spend less time competing for CPU time, which indirectly reduces context switching.\\n\\n---\\n\\n### 6. Avoid `sleep`, `yield`, and Busy Waiting\\n\\n**What goes wrong**  \\nCalling `sleep` or `yield` explicitly tells the operating system to stop running the thread, even if no real reason exists. Busy waiting repeatedly checks for work and causes unnecessary scheduling activity.\\n\\n**What changes**  \\nEvent-driven or signal-based mechanisms allow threads to stop only when real work is unavailable and resume exactly when needed.\\n\\n**Why it helps**  \\nThreads are no longer interrupted artificially. They stop and resume only when necessary, reducing unnecessary context switches.\\n\\n---\\n\\n### 7. Batch Work Instead of Scheduling Tiny Tasks\\n\\n**What goes wrong**  \\nScheduling many very small tasks causes frequent thread scheduling and frequent interruptions. The overhead of stopping and starting threads can become comparable to the work itself.\\n\\n**What changes**  \\nMultiple small tasks are grouped together and processed in a single execution.\\n\\n**Why it helps**  \\nThreads do more useful work each time they run, which reduces how often they need to be interrupted and rescheduled.\\n\\n---\\n\\n### 8. Use Work-Stealing Schedulers\\n\\n**What goes wrong**  \\nSome threads become idle while others are overloaded. Idle threads may block while busy threads continue to accumulate work.\\n\\n**What changes**  \\nWork-stealing allows idle threads to take work from busy threads instead of blocking or creating new threads.\\n\\n**Why it helps**  \\nThreads remain productive without increasing the total number of threads or causing additional stop-and-restart cycles, reducing context switching overall.\\n\\n---\\n\\n## Summary and Key Takeaways\\nContext switching becomes harmful when threads are interrupted too often. The CPU performs best when threads can run long enough to make meaningful progress.\\n\\nThe goal is not fewer threads, but **fewer unnecessary interruptions**.\\n\\n**Rule of thumb:**  \\nIf adding more threads makes performance worse, excessive context switching is often the reason.\\n\\n---\\n\\n## Example in C#\\n\\n```csharp\\n//  Bad: create many threads\\nfor (int i = 0; i < 1000; i++)\\n{\\n    new Thread(() => DoWork()).Start();\\n}\\n\\n//  Good: bounded concurrency\\nParallel.For(0, 1000, i => DoWork());\\n\\n//  Best for I/O-bound work\\nawait Task.WhenAll(\\n    Enumerable.Range(0, 1000)\\n        .Select(_ => DoWorkAsync())\\n);\\n\"}],\"direction\":null,\"format\":\"\",\"indent\":0,\"type\":\"root\",\"version\":1}}','<h2 id=\"subtitle\">Subtitle</h2>\n<p>Context switching overhead becomes a measurable performance bottleneck in high-throughput systems when too many threads or processes compete for limited CPU resources.</p>\n<hr>\n<h2 id=\"executive-summary-tldr\">Executive Summary (TL;DR)</h2>\n<p>Context switching happens when the operating system stops one thread and starts another. Each switch requires coordination work and often forces the CPU to repeat work it was already doing efficiently. When context switching happens too often, the CPU spends more time coordinating execution than doing useful work. Reducing unnecessary context switches improves throughput, lowers latency (especially worst-case latency), and makes performance more predictable. This optimization matters most in CPU-bound workloads where thread count exceeds CPU core count.</p>\n<hr>\n<h2 id=\"problem-context\">Problem Context</h2>\n<p>A common misconception is that increasing the number of threads automatically improves performance. This leads to designs where a new thread is created for every request or task. While this can work for I/O-bound workloads (where threads mostly wait), it becomes harmful for CPU-bound workloads.</p>\n<p>A CPU has a fixed number of cores, and each core can only execute one thread at a time. When an application creates more threads than available cores, the operating system must constantly stop and resume threads so they can take turns using the CPU. This constant stopping and starting introduces overhead and reduces efficiency.</p>\n<p>In real systems, this often appears as:</p>\n<ul>\n<li>High CPU usage but low throughput</li>\n<li>Requests becoming slower under load</li>\n<li>Performance degrading as more threads are added</li>\n</ul>\n<p>Typical mistakes include:</p>\n<ul>\n<li>Creating thousands of threads</li>\n<li>Blocking threads on I/O</li>\n<li>Misconfigured thread pools</li>\n<li>Ignoring CPU core limits</li>\n<li>Mixing CPU-bound and I/O-bound work in the same execution model</li>\n</ul>\n<hr>\n<h2 id=\"how-context-switching-works\">How Context Switching Works</h2>\n<p>A <strong>context switch</strong> occurs when the operating system:</p>\n<ol>\n<li>Stops the currently running thread</li>\n<li>Saves its execution state</li>\n<li>Loads the execution state of another thread</li>\n<li>Resumes execution of that thread</li>\n</ol>\n<p>Even though modern systems are fast, this process still takes time. More importantly, it interrupts the natural flow of execution. When this interruption happens repeatedly, the CPU becomes less efficient because it cannot build momentum executing useful work.</p>\n<hr>\n<h2 id=\"voluntary-vs-involuntary-context-switching\">Voluntary vs Involuntary Context Switching</h2>\n<h3 id=\"voluntary-context-switching\">Voluntary Context Switching</h3>\n<p>Occurs when a thread cannot continue and explicitly stops:</p>\n<ul>\n<li>Waiting for disk or network I/O</li>\n<li>Waiting for a lock</li>\n<li>Calling <code>sleep</code> or <code>yield</code></li>\n</ul>\n<h3 id=\"involuntary-context-switching\">Involuntary Context Switching</h3>\n<p>Occurs when the operating system interrupts a running thread:</p>\n<ul>\n<li>To give CPU time to another thread</li>\n<li>To enforce fairness when many threads compete</li>\n</ul>\n<p><strong>Key idea</strong><br>\nVoluntary switches come from waiting.<br>\nInvoluntary switches come from too many runnable threads.</p>\n<hr>\n<h2 id=\"why-this-becomes-a-bottleneck\">Why This Becomes a Bottleneck</h2>\n<p>Context switching becomes a bottleneck when threads are interrupted so frequently that useful work is broken into very small pieces.</p>\n<p>A CPU core can only run one thread at a time. If more threads want to run than there are cores, the operating system must constantly rotate between them. Each rotation introduces overhead.</p>\n<p>When the number of threads is small, this overhead is negligible. When the number grows, interruptions become frequent and expensive.</p>\n<p>Frequent interruption slows execution. Slower execution means threads stay active longer. Threads that stay active longer increase contention. Increased contention causes even more interruptions.</p>\n<p>Blocking behavior worsens the situation. Threads that wait for I/O or locks stop and later resume. Each stop and resume adds more switching and more coordination work.</p>\n<p>Movement of threads between CPU cores also hurts efficiency. When a thread resumes execution on a different core, it must re-establish execution context, which slows progress and increases the likelihood of further interruption.</p>\n<p>All these effects reinforce each other:</p>\n<ul>\n<li>More threads  more interruptions</li>\n<li>More interruptions  slower progress</li>\n<li>Slower progress  more contention</li>\n<li>More contention  even more interruptions</li>\n</ul>\n<p>This feedback loop explains why systems with excessive threading often show high CPU usage, low throughput, poor scalability, and unstable latency.</p>\n<hr>\n<h2 id=\"time-quantum-and-scheduling-behavior\">Time Quantum and Scheduling Behavior</h2>\n<p>A <strong>time quantum</strong> (also called <em>time slice</em>) is the amount of CPU time a runnable thread is allowed to execute before the operating system scheduler may preempt it and switch to another thread.</p>\n<h3 id=\"is-the-time-quantum-fixed\">Is the time quantum fixed?</h3>\n<p>No. The time quantum is <strong>not a fixed, universal duration</strong>.</p>\n<p>Its effective length varies depending on:</p>\n<ul>\n<li>The operating system scheduler implementation</li>\n<li>The scheduling policy in use</li>\n<li>The number of runnable threads in the run queue</li>\n<li>Thread priorities</li>\n<li>Overall system load</li>\n</ul>\n<p>Modern schedulers do not aim to give each thread a fixed time slice. Instead, they aim for <strong>fairness</strong> across all runnable threads.</p>\n<h3 id=\"how-the-quantum-behaves-in-practice\">How the quantum behaves in practice</h3>\n<ul>\n<li>When <strong>few threads</strong> are runnable, each thread runs longer without interruption.</li>\n<li>When <strong>many threads</strong> are runnable, the scheduler reduces how long each thread can run.</li>\n<li>As the run queue grows, uninterrupted execution time per thread shrinks.</li>\n</ul>\n<p>In effect:</p>\n<ul>\n<li>More runnable threads  shorter execution windows</li>\n<li>Shorter execution windows  more preemption</li>\n<li>More preemption  more involuntary context switches</li>\n</ul>\n<p>This is why oversubscribing the CPU with runnable threads directly increases context switching overhead.</p>\n<hr>\n<h2 id=\"what-causes-a-thread-to-context-switch\">What Causes a Thread to Context Switch</h2>\n<p>A thread context switch occurs whenever the operating system stops one thread and allows another to run. This can happen for several reasons.</p>\n<h3 id=\"1-time-quantum-expiration-preemption\">1. Time Quantum Expiration (Preemption)</h3>\n<p>If a thread exhausts its allowed execution window, the scheduler may preempt it to give another runnable thread CPU time.</p>\n<p>This is the most common cause of <strong>involuntary context switching</strong>, especially when:</p>\n<ul>\n<li>There are more runnable threads than CPU cores</li>\n<li>Threads are CPU-bound and do not block naturally</li>\n</ul>\n<hr>\n<h3 id=\"2-blocking-operations-voluntary-switching\">2. Blocking Operations (Voluntary Switching)</h3>\n<p>A thread voluntarily stops executing when it cannot continue:</p>\n<ul>\n<li>Waiting for disk or network I/O</li>\n<li>Waiting to acquire a lock</li>\n<li>Waiting on a condition variable</li>\n<li>Calling <code>sleep</code> or <code>yield</code></li>\n</ul>\n<p>The scheduler then switches to another runnable thread.</p>\n<hr>\n<h3 id=\"3-lock-contention\">3. Lock Contention</h3>\n<p>When a thread attempts to acquire a lock that is already held:</p>\n<ul>\n<li>The thread blocks</li>\n<li>The scheduler switches to another thread</li>\n<li>The blocked thread later resumes when the lock becomes available</li>\n</ul>\n<p>High lock contention causes frequent stop-and-resume cycles and increases context switching.</p>\n<hr>\n<h3 id=\"4-higher-priority-threads-becoming-runnable\">4. Higher-Priority Threads Becoming Runnable</h3>\n<p>If a higher-priority thread becomes runnable:</p>\n<ul>\n<li>The scheduler may immediately preempt the currently running thread</li>\n<li>A context switch occurs even if the current thread is making progress</li>\n</ul>\n<hr>\n<h3 id=\"5-thread-migration-between-cpu-cores\">5. Thread Migration Between CPU Cores</h3>\n<p>Threads may resume execution on a different CPU core than where they previously ran.</p>\n<p>While this does not always cause an immediate context switch by itself, it:</p>\n<ul>\n<li>Slows execution</li>\n<li>Increases execution time</li>\n<li>Increases the likelihood of further preemption</li>\n</ul>\n<p>Indirectly, this leads to more context switching.</p>\n<hr>\n<h3 id=\"6-operating-system-interrupts\">6. Operating System Interrupts</h3>\n<p>Hardware interrupts (network, disk, timers) temporarily stop user threads so the kernel can handle events.</p>\n<p>After interrupt handling completes, the scheduler decides which thread runs next, which may result in a context switch.</p>\n<hr>\n<h2 id=\"why-fewer-context-switches-improve-overall-performance\">Why Fewer Context Switches Improve Overall Performance</h2>\n<p>Reducing context switching improves performance not because context switches are inherently bad, but because <strong>every context switch interrupts multiple layers of work that are already operating efficiently</strong>. When switches happen too often, those layers never reach a stable, optimized state.</p>\n<p>Below are the most important performance relationships affected by context switching, explained without diving into hardware internals.</p>\n<hr>\n<h3 id=\"1-longer-uninterrupted-execution-means-less-rework\">1. Longer Uninterrupted Execution Means Less Rework</h3>\n<p>When a thread runs continuously, it makes steady progress. When it is interrupted frequently, its work is fragmented.</p>\n<p>Fragmented work often requires:</p>\n<ul>\n<li>Re-checking conditions</li>\n<li>Re-entering code paths</li>\n<li>Re-establishing execution flow</li>\n</ul>\n<p>Even on fast CPUs, restarting work repeatedly is slower than continuing smoothly.</p>\n<p><strong>Fewer context switches  longer uninterrupted execution  less repeated work</strong></p>\n<hr>\n<h3 id=\"2-better-reuse-of-data-and-state\">2. Better Reuse of Data and State</h3>\n<p>While a thread is running, the system naturally keeps the data it uses readily accessible. When execution switches to another thread, different data is accessed and the previous data becomes less immediately available.</p>\n<p>When the original thread resumes, accessing its data again takes longer than if it had continued running.</p>\n<p>You dont need to know hardware details to see the effect:</p>\n<ul>\n<li>Continuous execution keeps data close</li>\n<li>Frequent switching pushes data farther away</li>\n</ul>\n<p><strong>Fewer context switches  better data reuse  faster execution</strong></p>\n<hr>\n<h3 id=\"3-less-time-spent-coordinating-more-time-doing-work\">3. Less Time Spent Coordinating, More Time Doing Work</h3>\n<p>Every context switch requires the operating system to:</p>\n<ul>\n<li>Decide which thread should run</li>\n<li>Stop one execution</li>\n<li>Start another execution</li>\n</ul>\n<p>This coordination work is necessary, but it produces no business value. It does not process requests, compute results, or serve users.</p>\n<p>Reducing context switches reduces this pure overhead.</p>\n<p><strong>Fewer context switches  less coordination  more CPU time for real work</strong></p>\n<hr>\n<h3 id=\"4-fewer-artificial-execution-interruptions\">4. Fewer Artificial Execution Interruptions</h3>\n<p>Context switches often interrupt threads that are actively making progress. These interruptions break the natural flow of execution and reduce efficiency.</p>\n<p>Allowing threads to run longer reduces artificial pauses and improves overall execution efficiency.</p>\n<p><strong>Fewer context switches  smoother execution  higher efficiency</strong></p>\n<hr>\n<h3 id=\"5-faster-completion-of-tasks\">5. Faster Completion of Tasks</h3>\n<p>When threads are interrupted frequently, tasks take longer to complete because they are repeatedly paused and resumed.</p>\n<p>Longer task durations create secondary problems:</p>\n<ul>\n<li>Resources are held longer</li>\n<li>More threads remain active</li>\n<li>System pressure increases</li>\n</ul>\n<p>Reducing context switching shortens task lifetimes and lowers overall contention.</p>\n<p><strong>Fewer context switches  faster task completion  less system pressure</strong></p>\n<hr>\n<h3 id=\"6-more-predictable-performance-and-latency\">6. More Predictable Performance and Latency</h3>\n<p>Frequent context switching introduces randomness:</p>\n<ul>\n<li>Some threads are interrupted many times</li>\n<li>Others run longer</li>\n<li>Latency becomes inconsistent</li>\n</ul>\n<p>Reducing context switching stabilizes execution timing and improves predictability, especially for worst-case latency.</p>\n<p><strong>Fewer context switches  less variability  more predictable performance</strong></p>\n<hr>\n<h3 id=\"7-better-behavior-under-load\">7. Better Behavior Under Load</h3>\n<p>Under load, negative effects amplify:</p>\n<ul>\n<li>Slower execution increases contention</li>\n<li>Increased contention increases switching</li>\n<li>Increased switching slows execution further</li>\n</ul>\n<p>Reducing context switching weakens this negative feedback loop and allows the system to degrade more gracefully under stress.</p>\n<p><strong>Fewer context switches  weaker feedback loops  better scalability</strong></p>\n<hr>\n<h2 id=\"how-to-reduce-context-switching\">How to Reduce Context Switching</h2>\n<p>Reducing context switching means <strong>reducing how often threads are stopped and restarted</strong>. The goal is not to eliminate concurrency, but to avoid unnecessary interruptions.</p>\n<hr>\n<h3 id=\"1-use-thread-pools-instead-of-creating-threads\">1. Use Thread Pools Instead of Creating Threads</h3>\n<p><strong>What goes wrong</strong><br>\nCreating a new thread for each task quickly leads to a large number of threads that all want CPU time. Since the CPU can only run a limited number of threads at once, the operating system must constantly stop one thread and start another so they can take turns.</p>\n<p><strong>What changes</strong><br>\nThread pools create a fixed number of threads ahead of time and reuse them for many tasks. New work waits in a queue instead of creating new threads.</p>\n<p><strong>Why it helps</strong><br>\nBecause the number of threads is limited, fewer threads compete for the CPU at the same time. This reduces how often the operating system needs to interrupt threads, which directly reduces context switching.</p>\n<hr>\n<h3 id=\"2-limit-concurrency-to-cpu-cores-for-cpu-bound-work\">2. Limit Concurrency to CPU Cores for CPU-Bound Work</h3>\n<p><strong>What goes wrong</strong><br>\nCPU-bound tasks always want to run and do not naturally pause. If there are more CPU-bound threads than CPU cores, the operating system must constantly rotate between them, interrupting each one even while it is making progress.</p>\n<p><strong>What changes</strong><br>\nConcurrency is limited so that only as many CPU-bound tasks run at the same time as there are CPU cores.</p>\n<p><strong>Why it helps</strong><br>\nWhen the number of running threads matches the number of cores, threads can run longer without being interrupted. Fewer forced interruptions mean fewer involuntary context switches.</p>\n<hr>\n<h3 id=\"3-use-async-non-blocking-io-instead-of-blocking-io\">3. Use Async / Non-Blocking I/O Instead of Blocking I/O</h3>\n<p><strong>What goes wrong</strong><br>\nWith blocking I/O, a thread starts an operation (like a network or disk call) and then stops doing anything while waiting for the result. That thread must later be restarted when the operation completes, causing additional context switches.</p>\n<p><strong>What changes</strong><br>\nWith async I/O, the thread starts the operation and immediately returns. The operating system notifies the application later when the I/O is complete, without stopping a thread just to wait.</p>\n<p><strong>Why it helps</strong><br>\nThreads are no longer stopped and restarted simply to wait for I/O. Fewer threads move between running and waiting states, which significantly reduces context switching.</p>\n<hr>\n<h3 id=\"4-minimize-lock-contention\">4. Minimize Lock Contention</h3>\n<p><strong>What goes wrong</strong><br>\nWhen multiple threads need the same lock, some threads must wait. Waiting threads stop running and later resume when the lock becomes available. Each wait and resume introduces context switches.</p>\n<p><strong>What changes</strong><br>\nReducing shared state, using finer-grained locks, or redesigning code to avoid locks reduces how often threads must wait.</p>\n<p><strong>Why it helps</strong><br>\nWhen threads wait less, they are stopped and restarted less often. This directly reduces both voluntary and involuntary context switches.</p>\n<hr>\n<h3 id=\"5-reduce-thread-migration-between-cpu-cores\">5. Reduce Thread Migration Between CPU Cores</h3>\n<p><strong>What goes wrong</strong><br>\nThreads may resume execution on a different CPU core than the one they previously ran on. When this happens, execution becomes less efficient and threads take longer to complete their work.</p>\n<p><strong>What changes</strong><br>\nKeeping threads on the same CPU core when possible improves execution continuity.</p>\n<p><strong>Why it helps</strong><br>\nThreads that run more efficiently finish their work faster and spend less time competing for CPU time, which indirectly reduces context switching.</p>\n<hr>\n<h3 id=\"6-avoid-sleep-yield-and-busy-waiting\">6. Avoid <code>sleep</code>, <code>yield</code>, and Busy Waiting</h3>\n<p><strong>What goes wrong</strong><br>\nCalling <code>sleep</code> or <code>yield</code> explicitly tells the operating system to stop running the thread, even if no real reason exists. Busy waiting repeatedly checks for work and causes unnecessary scheduling activity.</p>\n<p><strong>What changes</strong><br>\nEvent-driven or signal-based mechanisms allow threads to stop only when real work is unavailable and resume exactly when needed.</p>\n<p><strong>Why it helps</strong><br>\nThreads are no longer interrupted artificially. They stop and resume only when necessary, reducing unnecessary context switches.</p>\n<hr>\n<h3 id=\"7-batch-work-instead-of-scheduling-tiny-tasks\">7. Batch Work Instead of Scheduling Tiny Tasks</h3>\n<p><strong>What goes wrong</strong><br>\nScheduling many very small tasks causes frequent thread scheduling and frequent interruptions. The overhead of stopping and starting threads can become comparable to the work itself.</p>\n<p><strong>What changes</strong><br>\nMultiple small tasks are grouped together and processed in a single execution.</p>\n<p><strong>Why it helps</strong><br>\nThreads do more useful work each time they run, which reduces how often they need to be interrupted and rescheduled.</p>\n<hr>\n<h3 id=\"8-use-work-stealing-schedulers\">8. Use Work-Stealing Schedulers</h3>\n<p><strong>What goes wrong</strong><br>\nSome threads become idle while others are overloaded. Idle threads may block while busy threads continue to accumulate work.</p>\n<p><strong>What changes</strong><br>\nWork-stealing allows idle threads to take work from busy threads instead of blocking or creating new threads.</p>\n<p><strong>Why it helps</strong><br>\nThreads remain productive without increasing the total number of threads or causing additional stop-and-restart cycles, reducing context switching overall.</p>\n<hr>\n<h2 id=\"summary-and-key-takeaways\">Summary and Key Takeaways</h2>\n<p>Context switching becomes harmful when threads are interrupted too often. The CPU performs best when threads can run long enough to make meaningful progress.</p>\n<p>The goal is not fewer threads, but <strong>fewer unnecessary interruptions</strong>.</p>\n<p><strong>Rule of thumb:</strong><br>\nIf adding more threads makes performance worse, excessive context switching is often the reason.</p>\n<hr>\n<h2 id=\"example-in-c\">Example in C#</h2>\n<pre><code class=\"language-csharp\">//  Bad: create many threads\nfor (int i = 0; i &lt; 1000; i++)\n{\n    new Thread(() =&gt; DoWork()).Start();\n}\n\n//  Good: bounded concurrency\nParallel.For(0, 1000, i =&gt; DoWork());\n\n//  Best for I/O-bound work\nawait Task.WhenAll(\n    Enumerable.Range(0, 1000)\n        .Select(_ =&gt; DoWorkAsync())\n);\n</code></pre>\n','6950348e19c94fae00e37362','Subtitle\n\n\nContext switching overhead becomes a measurable performance bottleneck in high-throughput systems when too many threads or processes compete for limited CPU resources.\n\n\n\n\nExecutive Summary (TL;DR)\n\n\nContext switching happens when the operating system stops one thread and starts another. Each switch requires coordination work and often forces the CPU to repeat work it was already doing efficiently. When context switching happens too often, the CPU spends more time coordinating execution than doing useful work. Reducing unnecessary context switches improves throughput, lowers latency (especially worst-case latency), and makes performance more predictable. This optimization matters most in CPU-bound workloads where thread count exceeds CPU core count.\n\n\n\n\nProblem Context\n\n\nA common misconception is that increasing the number of threads automatically improves performance. This leads to designs where a new thread is created for every request or task. While this can work for I/O-bound workloads (where threads mostly wait), it becomes harmful for CPU-bound workloads.\n\n\nA CPU has a fixed number of cores, and each core can only execute one thread at a time. When an application creates more threads than available cores, the operating system must constantly stop and resume threads so they can take turns using the CPU. This constant stopping and starting introduces overhead and reduces efficiency.\n\n\nIn real systems, this often appears as:\n\n\n * High CPU usage but low throughput\n * Requests becoming slower under load\n * Performance degrading as more threads are added\n\n\nTypical mistakes include:\n\n\n * Creating thousands of threads\n * Blocking threads on I/O\n * Misconfigured thread pools\n * Ignoring CPU core limits\n * Mixing CPU-bound and I/O-bound work in the same execution model\n\n\n\n\nHow Context Switching Works\n\n\nA context switch occurs when the operating system:\n\n\n 1. Stops the currently running thread\n 2. Saves its execution state\n 3. Loads the execution state of another thread\n 4. Resumes execution of that thread\n\n\nEven though modern systems are fast, this process still takes time. More importantly, it interrupts the natural flow of execution. When this interruption happens repeatedly, the CPU becomes less efficient because it cannot build momentum executing useful work.\n\n\n\n\nVoluntary vs Involuntary Context Switching\n\n\n\nVoluntary Context Switching\n\n\nOccurs when a thread cannot continue and explicitly stops:\n\n\n * Waiting for disk or network I/O\n * Waiting for a lock\n * Calling sleep or yield\n\n\n\nInvoluntary Context Switching\n\n\nOccurs when the operating system interrupts a running thread:\n\n\n * To give CPU time to another thread\n * To enforce fairness when many threads compete\n\n\nKey idea\n\nVoluntary switches come from waiting.\n\nInvoluntary switches come from too many runnable threads.\n\n\n\n\nWhy This Becomes a Bottleneck\n\n\nContext switching becomes a bottleneck when threads are interrupted so frequently that useful work is broken into very small pieces.\n\n\nA CPU core can only run one thread at a time. If more threads want to run than there are cores, the operating system must constantly rotate between them. Each rotation introduces overhead.\n\n\nWhen the number of threads is small, this overhead is negligible. When the number grows, interruptions become frequent and expensive.\n\n\nFrequent interruption slows execution. Slower execution means threads stay active longer. Threads that stay active longer increase contention. Increased contention causes even more interruptions.\n\n\nBlocking behavior worsens the situation. Threads that wait for I/O or locks stop and later resume. Each stop and resume adds more switching and more coordination work.\n\n\nMovement of threads between CPU cores also hurts efficiency. When a thread resumes execution on a different core, it must re-establish execution context, which slows progress and increases the likelihood of further interruption.\n\n\nAll these effects reinforce each other:\n\n\n * More threads  more interruptions\n * More interruptions  slower progress\n * Slower progress  more contention\n * More contention  even more interruptions\n\n\nThis feedback loop explains why systems with excessive threading often show high CPU usage, low throughput, poor scalability, and unstable latency.\n\n\n\n\nTime Quantum and Scheduling Behavior\n\n\nA time quantum (also called time slice) is the amount of CPU time a runnable thread is allowed to execute before the operating system scheduler may preempt it and switch to another thread.\n\n\n\nIs the time quantum fixed?\n\n\nNo. The time quantum is not a fixed, universal duration.\n\n\nIts effective length varies depending on:\n\n\n * The operating system scheduler implementation\n * The scheduling policy in use\n * The number of runnable threads in the run queue\n * Thread priorities\n * Overall system load\n\n\nModern schedulers do not aim to give each thread a fixed time slice. Instead, they aim for fairness across all runnable threads.\n\n\n\nHow the quantum behaves in practice\n\n\n * When few threads are runnable, each thread runs longer without interruption.\n * When many threads are runnable, the scheduler reduces how long each thread can run.\n * As the run queue grows, uninterrupted execution time per thread shrinks.\n\n\nIn effect:\n\n\n * More runnable threads  shorter execution windows\n * Shorter execution windows  more preemption\n * More preemption  more involuntary context switches\n\n\nThis is why oversubscribing the CPU with runnable threads directly increases context switching overhead.\n\n\n\n\nWhat Causes a Thread to Context Switch\n\n\nA thread context switch occurs whenever the operating system stops one thread and allows another to run. This can happen for several reasons.\n\n\n\n1. Time Quantum Expiration (Preemption)\n\n\nIf a thread exhausts its allowed execution window, the scheduler may preempt it to give another runnable thread CPU time.\n\n\nThis is the most common cause of involuntary context switching, especially when:\n\n\n * There are more runnable threads than CPU cores\n * Threads are CPU-bound and do not block naturally\n\n\n\n\n2. Blocking Operations (Voluntary Switching)\n\n\nA thread voluntarily stops executing when it cannot continue:\n\n\n * Waiting for disk or network I/O\n * Waiting to acquire a lock\n * Waiting on a condition variable\n * Calling sleep or yield\n\n\nThe scheduler then switches to another runnable thread.\n\n\n\n\n3. Lock Contention\n\n\nWhen a thread attempts to acquire a lock that is already held:\n\n\n * The thread blocks\n * The scheduler switches to another thread\n * The blocked thread later resumes when the lock becomes available\n\n\nHigh lock contention causes frequent stop-and-resume cycles and increases context switching.\n\n\n\n\n4. Higher-Priority Threads Becoming Runnable\n\n\nIf a higher-priority thread becomes runnable:\n\n\n * The scheduler may immediately preempt the currently running thread\n * A context switch occurs even if the current thread is making progress\n\n\n\n\n5. Thread Migration Between CPU Cores\n\n\nThreads may resume execution on a different CPU core than where they previously ran.\n\n\nWhile this does not always cause an immediate context switch by itself, it:\n\n\n * Slows execution\n * Increases execution time\n * Increases the likelihood of further preemption\n\n\nIndirectly, this leads to more context switching.\n\n\n\n\n6. Operating System Interrupts\n\n\nHardware interrupts (network, disk, timers) temporarily stop user threads so the kernel can handle events.\n\n\nAfter interrupt handling completes, the scheduler decides which thread runs next, which may result in a context switch.\n\n\n\n\nWhy Fewer Context Switches Improve Overall Performance\n\n\nReducing context switching improves performance not because context switches are inherently bad, but because every context switch interrupts multiple layers of work that are already operating efficiently. When switches happen too often, those layers never reach a stable, optimized state.\n\n\nBelow are the most important performance relationships affected by context switching, explained without diving into hardware internals.\n\n\n\n\n1. Longer Uninterrupted Execution Means Less Rework\n\n\nWhen a thread runs continuously, it makes steady progress. When it is interrupted frequently, its work is fragmented.\n\n\nFragmented work often requires:\n\n\n * Re-checking conditions\n * Re-entering code paths\n * Re-establishing execution flow\n\n\nEven on fast CPUs, restarting work repeatedly is slower than continuing smoothly.\n\n\nFewer context switches  longer uninterrupted execution  less repeated work\n\n\n\n\n2. Better Reuse of Data and State\n\n\nWhile a thread is running, the system naturally keeps the data it uses readily accessible. When execution switches to another thread, different data is accessed and the previous data becomes less immediately available.\n\n\nWhen the original thread resumes, accessing its data again takes longer than if it had continued running.\n\n\nYou dont need to know hardware details to see the effect:\n\n\n * Continuous execution keeps data close\n * Frequent switching pushes data farther away\n\n\nFewer context switches  better data reuse  faster execution\n\n\n\n\n3. Less Time Spent Coordinating, More Time Doing Work\n\n\nEvery context switch requires the operating system to:\n\n\n * Decide which thread should run\n * Stop one execution\n * Start another execution\n\n\nThis coordination work is necessary, but it produces no business value. It does not process requests, compute results, or serve users.\n\n\nReducing context switches reduces this pure overhead.\n\n\nFewer context switches  less coordination  more CPU time for real work\n\n\n\n\n4. Fewer Artificial Execution Interruptions\n\n\nContext switches often interrupt threads that are actively making progress. These interruptions break the natural flow of execution and reduce efficiency.\n\n\nAllowing threads to run longer reduces artificial pauses and improves overall execution efficiency.\n\n\nFewer context switches  smoother execution  higher efficiency\n\n\n\n\n5. Faster Completion of Tasks\n\n\nWhen threads are interrupted frequently, tasks take longer to complete because they are repeatedly paused and resumed.\n\n\nLonger task durations create secondary problems:\n\n\n * Resources are held longer\n * More threads remain active\n * System pressure increases\n\n\nReducing context switching shortens task lifetimes and lowers overall contention.\n\n\nFewer context switches  faster task completion  less system pressure\n\n\n\n\n6. More Predictable Performance and Latency\n\n\nFrequent context switching introduces randomness:\n\n\n * Some threads are interrupted many times\n * Others run longer\n * Latency becomes inconsistent\n\n\nReducing context switching stabilizes execution timing and improves predictability, especially for worst-case latency.\n\n\nFewer context switches  less variability  more predictable performance\n\n\n\n\n7. Better Behavior Under Load\n\n\nUnder load, negative effects amplify:\n\n\n * Slower execution increases contention\n * Increased contention increases switching\n * Increased switching slows execution further\n\n\nReducing context switching weakens this negative feedback loop and allows the system to degrade more gracefully under stress.\n\n\nFewer context switches  weaker feedback loops  better scalability\n\n\n\n\nHow to Reduce Context Switching\n\n\nReducing context switching means reducing how often threads are stopped and restarted. The goal is not to eliminate concurrency, but to avoid unnecessary interruptions.\n\n\n\n\n1. Use Thread Pools Instead of Creating Threads\n\n\nWhat goes wrong\n\nCreating a new thread for each task quickly leads to a large number of threads that all want CPU time. Since the CPU can only run a limited number of threads at once, the operating system must constantly stop one thread and start another so they can take turns.\n\n\nWhat changes\n\nThread pools create a fixed number of threads ahead of time and reuse them for many tasks. New work waits in a queue instead of creating new threads.\n\n\nWhy it helps\n\nBecause the number of threads is limited, fewer threads compete for the CPU at the same time. This reduces how often the operating system needs to interrupt threads, which directly reduces context switching.\n\n\n\n\n2. Limit Concurrency to CPU Cores for CPU-Bound Work\n\n\nWhat goes wrong\n\nCPU-bound tasks always want to run and do not naturally pause. If there are more CPU-bound threads than CPU cores, the operating system must constantly rotate between them, interrupting each one even while it is making progress.\n\n\nWhat changes\n\nConcurrency is limited so that only as many CPU-bound tasks run at the same time as there are CPU cores.\n\n\nWhy it helps\n\nWhen the number of running threads matches the number of cores, threads can run longer without being interrupted. Fewer forced interruptions mean fewer involuntary context switches.\n\n\n\n\n3. Use Async / Non-Blocking I/O Instead of Blocking I/O\n\n\nWhat goes wrong\n\nWith blocking I/O, a thread starts an operation (like a network or disk call) and then stops doing anything while waiting for the result. That thread must later be restarted when the operation completes, causing additional context switches.\n\n\nWhat changes\n\nWith async I/O, the thread starts the operation and immediately returns. The operating system notifies the application later when the I/O is complete, without stopping a thread just to wait.\n\n\nWhy it helps\n\nThreads are no longer stopped and restarted simply to wait for I/O. Fewer threads move between running and waiting states, which significantly reduces context switching.\n\n\n\n\n4. Minimize Lock Contention\n\n\nWhat goes wrong\n\nWhen multiple threads need the same lock, some threads must wait. Waiting threads stop running and later resume when the lock becomes available. Each wait and resume introduces context switches.\n\n\nWhat changes\n\nReducing shared state, using finer-grained locks, or redesigning code to avoid locks reduces how often threads must wait.\n\n\nWhy it helps\n\nWhen threads wait less, they are stopped and restarted less often. This directly reduces both voluntary and involuntary context switches.\n\n\n\n\n5. Reduce Thread Migration Between CPU Cores\n\n\nWhat goes wrong\n\nThreads may resume execution on a different CPU core than the one they previously ran on. When this happens, execution becomes less efficient and threads take longer to complete their work.\n\n\nWhat changes\n\nKeeping threads on the same CPU core when possible improves execution continuity.\n\n\nWhy it helps\n\nThreads that run more efficiently finish their work faster and spend less time competing for CPU time, which indirectly reduces context switching.\n\n\n\n\n6. Avoid sleep, yield, and Busy Waiting\n\n\nWhat goes wrong\n\nCalling sleep or yield explicitly tells the operating system to stop running the thread, even if no real reason exists. Busy waiting repeatedly checks for work and causes unnecessary scheduling activity.\n\n\nWhat changes\n\nEvent-driven or signal-based mechanisms allow threads to stop only when real work is unavailable and resume exactly when needed.\n\n\nWhy it helps\n\nThreads are no longer interrupted artificially. They stop and resume only when necessary, reducing unnecessary context switches.\n\n\n\n\n7. Batch Work Instead of Scheduling Tiny Tasks\n\n\nWhat goes wrong\n\nScheduling many very small tasks causes frequent thread scheduling and frequent interruptions. The overhead of stopping and starting threads can become comparable to the work itself.\n\n\nWhat changes\n\nMultiple small tasks are grouped together and processed in a single execution.\n\n\nWhy it helps\n\nThreads do more useful work each time they run, which reduces how often they need to be interrupted and rescheduled.\n\n\n\n\n8. Use Work-Stealing Schedulers\n\n\nWhat goes wrong\n\nSome threads become idle while others are overloaded. Idle threads may block while busy threads continue to accumulate work.\n\n\nWhat changes\n\nWork-stealing allows idle threads to take work from busy threads instead of blocking or creating new threads.\n\n\nWhy it helps\n\nThreads remain productive without increasing the total number of threads or causing additional stop-and-restart cycles, reducing context switching overall.\n\n\n\n\nSummary and Key Takeaways\n\n\nContext switching becomes harmful when threads are interrupted too often. The CPU performs best when threads can run long enough to make meaningful progress.\n\n\nThe goal is not fewer threads, but fewer unnecessary interruptions.\n\n\nRule of thumb:\n\nIf adding more threads makes performance worse, excessive context switching is often the reason.\n\n\n\n\nExample in C#\n\n\n//  Bad: create many threads\nfor (int i = 0; i < 1000; i++)\n{\n    new Thread(() => DoWork()).Start();\n}\n\n//  Good: bounded concurrency\nParallel.For(0, 1000, i => DoWork());\n\n//  Best for I/O-bound work\nawait Task.WhenAll(\n    Enumerable.Range(0, 1000)\n        .Select(_ => DoWorkAsync())\n);\n\n',NULL,0,'post','published',NULL,'public','all','2025-12-27 19:33:34','2025-12-29 05:04:07','2025-12-27 19:33:34','694f2665c3e44e650f0e5dd8',NULL,NULL,NULL,NULL,NULL,NULL,1),('6951521f19c94fae00e3739b','4148955c-d1d1-46fb-aace-aa3c83f79315','Pin Processes and Threads to Specific CPU Cores for Predictable Performance','pin-processes-and-threads-to-specific-cpu-cores-for-predictable-performance',NULL,'{\"root\":{\"children\":[{\"type\":\"markdown\",\"version\":1,\"markdown\":\"**Control CPU affinity to improve execution predictability and reduce unnecessary movement between CPU cores, at the cost of scheduler flexibility.**\\n\\n---\\n\\n## Executive Summary (TL;DR)\\n\\nCPU affinity (also called *CPU pinning*) means telling the operating system that a process or thread should run only on specific CPU cores. By default, the operating system is free to move threads between cores to balance load. While this is usually beneficial, it can hurt performance for latency-sensitive or cache-sensitive workloads.\\n\\nPinning execution to specific cores reduces unnecessary movement, improves execution continuity, and makes performance more predictable. Typical improvements range from **520%** for cache-sensitive workloads and can be much higher on multi-socket (NUMA) systems. The trade-off is reduced scheduler flexibility and potential load imbalance.\\n\\nUse CPU pinning when predictability matters more than maximum flexibility.\\n\\n---\\n\\n## A Simple Mental Model (Read This First)\\n\\nThink of a CPU core as a **desk**, and a thread as a **worker**.\\n\\n- When the worker stays at the same desk, their tools are already laid out.\\n- When the worker is moved to another desk, they must set everything up again.\\n- If the worker is moved frequently, more time is spent setting up than working.\\n\\nCPU pinning simply tells the operating system:  \\n**Stop moving this worker between desks.**\\n\\nYou dont need to know what the tools are internally.  \\nLess movement means less wasted time.\\n\\n---\\n\\n## Problem Context\\n\\nModern operating systems use schedulers designed to keep all CPU cores busy. To achieve this, they dynamically move processes and threads between cores to balance load, improve fairness, and optimize power usage.\\n\\nFor most applications, this behavior is ideal. However, for workloads that:\\n- require consistent timing\\n- repeatedly access the same data\\n- are sensitive to latency spikes\\n\\nfrequent core migration becomes expensive.\\n\\nEach migration interrupts execution and forces internal state to be rebuilt. Over time, this creates instability and unpredictable performance.\\n\\n---\\n\\n## What Is CPU Affinity?\\n\\nCPU affinity defines **which CPU cores a process or thread is allowed to run on**.\\n\\n- **Without affinity**: the OS may run your code on any core\\n- **With affinity**: the OS is restricted to a specific set of cores\\n\\nOnce affinity is set, the scheduler respects that constraint and avoids migrating execution outside the allowed cores.\\n\\n---\\n\\n## Default Thread Affinity and Why Threads Move Between Cores\\n\\nBy default, operating systems do **not** strictly bind threads to specific CPU cores.\\n\\nInstead, the scheduler uses a *soft affinity* strategy:\\n- It prefers to run a thread on the same core it ran on last\\n- But it is free to move the thread whenever it decides it is beneficial\\n\\nThis means:\\n- Threads can run on any core\\n- Core assignment is dynamic\\n- There is no guarantee that a thread will stay on the same core\\n\\n### Why Threads Are Migrated\\n\\nThreads move between cores for several reasons:\\n\\n- **Load balancing**  \\n  To prevent some cores from being overloaded while others are idle\\n\\n- **Fairness**  \\n  To ensure all runnable threads get CPU time\\n\\n- **Power and thermal management**  \\n  To spread heat and optimize energy usage\\n\\n- **Oversubscription**  \\n  When more threads are active than available cores\\n\\n- **System activity**  \\n  Interrupt handling, kernel threads, and background services\\n\\nFrom the schedulers perspective, migration is often the right choice.  \\nFrom the applications perspective, migration can introduce overhead and unpredictability.\\n\\nCPU pinning exists to override this behavior when predictability matters more than flexibility.\\n\\n---\\n\\n## Why Uncontrolled Core Migration Hurts Performance\\n\\n### 1. Execution Is Repeatedly Interrupted\\n\\nEach time a thread moves to another core, execution is paused and resumed elsewhere. Even though this is fast, doing it repeatedly adds noticeable overhead.\\n\\n---\\n\\n### 2. Internal Execution State Is Rebuilt\\n\\nWhile running, the CPU prepares internal state to execute code efficiently. When execution moves to another core, that preparation must be rebuilt, slowing progress.\\n\\nYou dont need hardware details to understand this:\\n**movement causes repetition**.\\n\\n---\\n\\n### 3. Latency Becomes Unpredictable\\n\\nSome executions run uninterrupted, others are moved several times. This creates variability and hurts worst-case latency.\\n\\n---\\n\\n### 4. Problems Amplify Under Load\\n\\nAs load increases:\\n- more threads compete for cores\\n- more migration occurs\\n- delays compound\\n\\nThis creates a feedback loop that rapidly degrades performance.\\n\\n---\\n\\n## Why Pinning Improves Many Things at Once\\n\\nCPU pinning improves performance not by optimizing one component, but by removing an entire class of inefficiency.\\n\\n- Less movement  less repeated setup\\n- Longer uninterrupted execution  more useful work\\n- Faster task completion  less system pressure\\n- More stable timing  better tail latency\\n\\nThis is why pinning often improves latency, throughput, and predictability simultaneously.\\n\\n---\\n\\n## How CPU Pinning Works (High-Level)\\n\\n### Hardware Perspective\\n- Each CPU core maintains its own execution context\\n- Staying on the same core preserves continuity\\n- Multi-socket (NUMA) systems amplify the cost of movement\\n\\n### Operating System Perspective\\n- The OS maintains an affinity mask for each process or thread\\n- The scheduler respects this mask\\n- Context switches still happen, but core migration is reduced or eliminated\\n\\n---\\n\\n## Advantages\\n\\n- More predictable performance\\n- Reduced execution interruption\\n- Improved behavior under load\\n- Strong benefits in NUMA systems\\n- Better isolation for critical workloads\\n\\n---\\n\\n## Disadvantages and Trade-offs\\n\\n- Reduced scheduler flexibility\\n- Risk of load imbalance\\n- Requires hardware awareness\\n- Can waste resources if misused\\n- Not suitable for dynamic workloads\\n\\nCPU pinning trades flexibility for predictability.\\n\\n---\\n\\n## Why Thread Affinity Improves Performance (Deeper Explanation)\\n\\nThread and CPU affinity improve performance primarily by **preserving execution locality**. When a thread remains on the same CPU core, the system avoids repeatedly rebuilding execution context that was already optimized.\\n\\nThis improvement comes from several reinforcing effects.\\n\\n---\\n\\n### 1. Preserving CPU Cache Locality\\n\\nEach CPU core maintains private caches that store recently used data and instructions. When a thread runs continuously on the same core, the data it needs remains readily available.\\n\\nWhen a thread migrates to another core:\\n- Its data is no longer in the local cache\\n- The new core must fetch data again\\n- Execution stalls while data is reloaded\\n\\nThis work was already done once and must now be repeated.\\n\\n**Thread affinity keeps caches warm**, reducing cache misses and avoiding repeated data loading.\\n\\n---\\n\\n### 2. Fewer Cache Misses Means Fewer CPU Stalls\\n\\nCache misses force the CPU to wait for data. These waits add latency even if no context switch occurs.\\n\\nCore migration dramatically increases cache misses:\\n- Local cache state is lost\\n- Execution restarts in a colder environment\\n- Progress slows\\n\\nBy keeping threads on the same core, affinity reduces cache miss frequency and improves sustained execution speed.\\n\\n---\\n\\n### 3. Longer Uninterrupted Execution Windows\\n\\nThread migration is often correlated with:\\n- Preemption\\n- Short execution slices\\n- Increased context switching\\n\\nWhen a thread is pinned to a core:\\n- It is more likely to continue running\\n- It completes work faster\\n- It exits the run queue sooner\\n\\n**Faster completion reduces contention**, which indirectly reduces both voluntary and involuntary context switches.\\n\\n---\\n\\n### 4. Reduced Context Switching Pressure\\n\\nAlthough affinity does not eliminate context switching, it reduces **unnecessary switches caused by migration and rebalancing**.\\n\\nWith affinity:\\n- Scheduler decisions are simpler\\n- Threads are less likely to be displaced\\n- Execution becomes more stable\\n\\nLess scheduling churn means more CPU time is spent executing useful work instead of coordinating execution.\\n\\n---\\n\\n### 5. Better Execution Predictability\\n\\nWithout affinity, execution timing varies depending on:\\n- Which core runs the thread\\n- Cache warmth\\n- Migration frequency\\n- System activity\\n\\nAffinity reduces these variables.\\n\\nThis leads to:\\n- Lower latency jitter\\n- More consistent response times\\n- Improved worst-case latency\\n\\nThis predictability is often more valuable than raw throughput.\\n\\n---\\n\\n### 6. Amplified Benefits on NUMA Systems\\n\\nOn multi-socket systems, memory access latency depends on proximity to the CPU core.\\n\\nWithout affinity:\\n- Threads may run far from their memory\\n- Memory access becomes slower\\n- Execution time increases\\n\\nWith affinity:\\n- Threads stay close to their memory\\n- Remote memory access is reduced\\n- Latency becomes more stable\\n\\nNUMA systems benefit disproportionately from proper pinning.\\n\\n---\\n\\n## When CPU / Thread Affinity Is Most Beneficial\\n\\nCPU / thread affinity is useful when a thread performs similar work repeatedly and loses performance because it is moved between CPU cores.  \\nMoving a thread is not free: the CPU must warm up again before it can run efficiently.\\n\\nAffinity helps by **letting a thread stay where it already works well**.\\n\\n---\\n\\n### 1. CPU-Bound Applications\\n**What this means**  \\nThe application spends most of its time doing calculations, not waiting for disk or network.\\n\\n**What goes wrong without affinity**  \\nThe thread is constantly ready to run, but the scheduler moves it between cores to balance load.  \\nEach move forces the CPU to restart execution in a colder state.\\n\\n**Why affinity helps**  \\nKeeping the thread on one core allows it to run longer without interruption and avoids repeated warm-up.\\n\\n**Typical effect**\\n- More work done per second\\n- Less CPU time wasted restarting execution\\n\\n---\\n\\n### 2. Long-Lived Worker Threads\\n**What this means**  \\nThe application uses a fixed set of worker threads that stay alive for a long time and process similar tasks.\\n\\n**What goes wrong without affinity**  \\nWorkers are moved between cores even though their work pattern does not change.  \\nEach move discards progress the CPU already made optimizing execution.\\n\\n**Why affinity helps**  \\nA worker that stays on one core becomes more efficient over time and finishes tasks faster.\\n\\n**Typical effect**\\n- Better steady-state performance\\n- More stable timing once the system warms up\\n\\n---\\n\\n### 3. Cache-Sensitive Workloads\\n**What this means**  \\nThe application repeatedly accesses the same data in memory.\\n\\n**What goes wrong without affinity**  \\nWhen a thread moves to another core, the data it was using is no longer nearby.  \\nThe CPU must fetch the same data again, which takes time.\\n\\n**Why affinity helps**  \\nKeeping the thread on the same core keeps its data close, so memory access stays fast.\\n\\n**Typical effect**\\n- Lower memory latency\\n- Faster execution of repeated operations\\n\\n---\\n\\n### 4. Latency-Sensitive Applications\\n**What this means**  \\nResponse time matters more than average throughput, and delays must be predictable.\\n\\n**What goes wrong without affinity**  \\nSome requests run on warm cores and are fast, others resume on cold cores and are slow.  \\nThis creates unpredictable latency spikes.\\n\\n**Why affinity helps**  \\nThreads resume execution in a similar environment every time, reducing randomness.\\n\\n**Typical effect**\\n- More consistent response times\\n- Better worst-case latency (P99 / P999)\\n\\n---\\n\\n### 5. Predictable, Steady Workloads\\n**What this means**  \\nThe workload shape does not change much over time.\\n\\n**What goes wrong without affinity**  \\nThe scheduler keeps moving threads even though there is no real imbalance to fix.\\n\\n**Why affinity helps**  \\nFixed core assignment avoids unnecessary movement and keeps execution stable.\\n\\n**Typical effect**\\n- Repeatable performance\\n- Easier tuning and capacity planning\\n\\n---\\n\\n### 6. Dedicated or Single-Tenant Systems\\n**What this means**  \\nOne application owns the machine.\\n\\n**What goes wrong without affinity**  \\nThreads are moved around for fairness that the application does not need.\\n\\n**Why affinity helps**  \\nRestricting movement reduces interference and improves isolation.\\n\\n**Typical effect**\\n- Better resource usage\\n- More predictable system behavior\\n\\n---\\n\\n### 7. NUMA Systems (Without NUMA-Aware Code)\\n**What this means**  \\nThe machine has multiple CPU sockets, each with its own memory.\\n\\n**What goes wrong without affinity**  \\nThreads move between sockets and access memory that is far away, which is slower.\\n\\n**Why affinity helps**  \\nKeeping threads near their memory avoids expensive remote access.\\n\\n**Typical effect**\\n- Lower memory access latency\\n- More consistent throughput\\n\\n## Rule of Thumb\\n\\n> Use CPU / thread affinity when threads do similar work for a long time and lose performance because they are moved between CPU cores.\\n---\\n\\n## C# Examples\\n\\n### Process Affinity (Most Common)\\n\\n```csharp\\nusing System;\\nusing System.Diagnostics;\\n\\nclass Program\\n{\\n    static void Main()\\n    {\\n        var process = Process.GetCurrentProcess();\\n\\n        // Pin process to CPU 0 and 1 (binary mask: 00000011)\\n        process.ProcessorAffinity = new IntPtr(0b00000011);\\n\\n        Console.WriteLine(\\\"Process pinned to CPU 0 and 1\\\");\\n    }\\n}\\n\"}],\"direction\":null,\"format\":\"\",\"indent\":0,\"type\":\"root\",\"version\":1}}','<p><strong>Control CPU affinity to improve execution predictability and reduce unnecessary movement between CPU cores, at the cost of scheduler flexibility.</strong></p>\n<hr>\n<h2 id=\"executive-summary-tldr\">Executive Summary (TL;DR)</h2>\n<p>CPU affinity (also called <em>CPU pinning</em>) means telling the operating system that a process or thread should run only on specific CPU cores. By default, the operating system is free to move threads between cores to balance load. While this is usually beneficial, it can hurt performance for latency-sensitive or cache-sensitive workloads.</p>\n<p>Pinning execution to specific cores reduces unnecessary movement, improves execution continuity, and makes performance more predictable. Typical improvements range from <strong>520%</strong> for cache-sensitive workloads and can be much higher on multi-socket (NUMA) systems. The trade-off is reduced scheduler flexibility and potential load imbalance.</p>\n<p>Use CPU pinning when predictability matters more than maximum flexibility.</p>\n<hr>\n<h2 id=\"a-simple-mental-model-read-this-first\">A Simple Mental Model (Read This First)</h2>\n<p>Think of a CPU core as a <strong>desk</strong>, and a thread as a <strong>worker</strong>.</p>\n<ul>\n<li>When the worker stays at the same desk, their tools are already laid out.</li>\n<li>When the worker is moved to another desk, they must set everything up again.</li>\n<li>If the worker is moved frequently, more time is spent setting up than working.</li>\n</ul>\n<p>CPU pinning simply tells the operating system:<br>\n<strong>Stop moving this worker between desks.</strong></p>\n<p>You dont need to know what the tools are internally.<br>\nLess movement means less wasted time.</p>\n<hr>\n<h2 id=\"problem-context\">Problem Context</h2>\n<p>Modern operating systems use schedulers designed to keep all CPU cores busy. To achieve this, they dynamically move processes and threads between cores to balance load, improve fairness, and optimize power usage.</p>\n<p>For most applications, this behavior is ideal. However, for workloads that:</p>\n<ul>\n<li>require consistent timing</li>\n<li>repeatedly access the same data</li>\n<li>are sensitive to latency spikes</li>\n</ul>\n<p>frequent core migration becomes expensive.</p>\n<p>Each migration interrupts execution and forces internal state to be rebuilt. Over time, this creates instability and unpredictable performance.</p>\n<hr>\n<h2 id=\"what-is-cpu-affinity\">What Is CPU Affinity?</h2>\n<p>CPU affinity defines <strong>which CPU cores a process or thread is allowed to run on</strong>.</p>\n<ul>\n<li><strong>Without affinity</strong>: the OS may run your code on any core</li>\n<li><strong>With affinity</strong>: the OS is restricted to a specific set of cores</li>\n</ul>\n<p>Once affinity is set, the scheduler respects that constraint and avoids migrating execution outside the allowed cores.</p>\n<hr>\n<h2 id=\"default-thread-affinity-and-why-threads-move-between-cores\">Default Thread Affinity and Why Threads Move Between Cores</h2>\n<p>By default, operating systems do <strong>not</strong> strictly bind threads to specific CPU cores.</p>\n<p>Instead, the scheduler uses a <em>soft affinity</em> strategy:</p>\n<ul>\n<li>It prefers to run a thread on the same core it ran on last</li>\n<li>But it is free to move the thread whenever it decides it is beneficial</li>\n</ul>\n<p>This means:</p>\n<ul>\n<li>Threads can run on any core</li>\n<li>Core assignment is dynamic</li>\n<li>There is no guarantee that a thread will stay on the same core</li>\n</ul>\n<h3 id=\"why-threads-are-migrated\">Why Threads Are Migrated</h3>\n<p>Threads move between cores for several reasons:</p>\n<ul>\n<li>\n<p><strong>Load balancing</strong><br>\nTo prevent some cores from being overloaded while others are idle</p>\n</li>\n<li>\n<p><strong>Fairness</strong><br>\nTo ensure all runnable threads get CPU time</p>\n</li>\n<li>\n<p><strong>Power and thermal management</strong><br>\nTo spread heat and optimize energy usage</p>\n</li>\n<li>\n<p><strong>Oversubscription</strong><br>\nWhen more threads are active than available cores</p>\n</li>\n<li>\n<p><strong>System activity</strong><br>\nInterrupt handling, kernel threads, and background services</p>\n</li>\n</ul>\n<p>From the schedulers perspective, migration is often the right choice.<br>\nFrom the applications perspective, migration can introduce overhead and unpredictability.</p>\n<p>CPU pinning exists to override this behavior when predictability matters more than flexibility.</p>\n<hr>\n<h2 id=\"why-uncontrolled-core-migration-hurts-performance\">Why Uncontrolled Core Migration Hurts Performance</h2>\n<h3 id=\"1-execution-is-repeatedly-interrupted\">1. Execution Is Repeatedly Interrupted</h3>\n<p>Each time a thread moves to another core, execution is paused and resumed elsewhere. Even though this is fast, doing it repeatedly adds noticeable overhead.</p>\n<hr>\n<h3 id=\"2-internal-execution-state-is-rebuilt\">2. Internal Execution State Is Rebuilt</h3>\n<p>While running, the CPU prepares internal state to execute code efficiently. When execution moves to another core, that preparation must be rebuilt, slowing progress.</p>\n<p>You dont need hardware details to understand this:<br>\n<strong>movement causes repetition</strong>.</p>\n<hr>\n<h3 id=\"3-latency-becomes-unpredictable\">3. Latency Becomes Unpredictable</h3>\n<p>Some executions run uninterrupted, others are moved several times. This creates variability and hurts worst-case latency.</p>\n<hr>\n<h3 id=\"4-problems-amplify-under-load\">4. Problems Amplify Under Load</h3>\n<p>As load increases:</p>\n<ul>\n<li>more threads compete for cores</li>\n<li>more migration occurs</li>\n<li>delays compound</li>\n</ul>\n<p>This creates a feedback loop that rapidly degrades performance.</p>\n<hr>\n<h2 id=\"why-pinning-improves-many-things-at-once\">Why Pinning Improves Many Things at Once</h2>\n<p>CPU pinning improves performance not by optimizing one component, but by removing an entire class of inefficiency.</p>\n<ul>\n<li>Less movement  less repeated setup</li>\n<li>Longer uninterrupted execution  more useful work</li>\n<li>Faster task completion  less system pressure</li>\n<li>More stable timing  better tail latency</li>\n</ul>\n<p>This is why pinning often improves latency, throughput, and predictability simultaneously.</p>\n<hr>\n<h2 id=\"how-cpu-pinning-works-high-level\">How CPU Pinning Works (High-Level)</h2>\n<h3 id=\"hardware-perspective\">Hardware Perspective</h3>\n<ul>\n<li>Each CPU core maintains its own execution context</li>\n<li>Staying on the same core preserves continuity</li>\n<li>Multi-socket (NUMA) systems amplify the cost of movement</li>\n</ul>\n<h3 id=\"operating-system-perspective\">Operating System Perspective</h3>\n<ul>\n<li>The OS maintains an affinity mask for each process or thread</li>\n<li>The scheduler respects this mask</li>\n<li>Context switches still happen, but core migration is reduced or eliminated</li>\n</ul>\n<hr>\n<h2 id=\"advantages\">Advantages</h2>\n<ul>\n<li>More predictable performance</li>\n<li>Reduced execution interruption</li>\n<li>Improved behavior under load</li>\n<li>Strong benefits in NUMA systems</li>\n<li>Better isolation for critical workloads</li>\n</ul>\n<hr>\n<h2 id=\"disadvantages-and-trade-offs\">Disadvantages and Trade-offs</h2>\n<ul>\n<li>Reduced scheduler flexibility</li>\n<li>Risk of load imbalance</li>\n<li>Requires hardware awareness</li>\n<li>Can waste resources if misused</li>\n<li>Not suitable for dynamic workloads</li>\n</ul>\n<p>CPU pinning trades flexibility for predictability.</p>\n<hr>\n<h2 id=\"why-thread-affinity-improves-performance-deeper-explanation\">Why Thread Affinity Improves Performance (Deeper Explanation)</h2>\n<p>Thread and CPU affinity improve performance primarily by <strong>preserving execution locality</strong>. When a thread remains on the same CPU core, the system avoids repeatedly rebuilding execution context that was already optimized.</p>\n<p>This improvement comes from several reinforcing effects.</p>\n<hr>\n<h3 id=\"1-preserving-cpu-cache-locality\">1. Preserving CPU Cache Locality</h3>\n<p>Each CPU core maintains private caches that store recently used data and instructions. When a thread runs continuously on the same core, the data it needs remains readily available.</p>\n<p>When a thread migrates to another core:</p>\n<ul>\n<li>Its data is no longer in the local cache</li>\n<li>The new core must fetch data again</li>\n<li>Execution stalls while data is reloaded</li>\n</ul>\n<p>This work was already done once and must now be repeated.</p>\n<p><strong>Thread affinity keeps caches warm</strong>, reducing cache misses and avoiding repeated data loading.</p>\n<hr>\n<h3 id=\"2-fewer-cache-misses-means-fewer-cpu-stalls\">2. Fewer Cache Misses Means Fewer CPU Stalls</h3>\n<p>Cache misses force the CPU to wait for data. These waits add latency even if no context switch occurs.</p>\n<p>Core migration dramatically increases cache misses:</p>\n<ul>\n<li>Local cache state is lost</li>\n<li>Execution restarts in a colder environment</li>\n<li>Progress slows</li>\n</ul>\n<p>By keeping threads on the same core, affinity reduces cache miss frequency and improves sustained execution speed.</p>\n<hr>\n<h3 id=\"3-longer-uninterrupted-execution-windows\">3. Longer Uninterrupted Execution Windows</h3>\n<p>Thread migration is often correlated with:</p>\n<ul>\n<li>Preemption</li>\n<li>Short execution slices</li>\n<li>Increased context switching</li>\n</ul>\n<p>When a thread is pinned to a core:</p>\n<ul>\n<li>It is more likely to continue running</li>\n<li>It completes work faster</li>\n<li>It exits the run queue sooner</li>\n</ul>\n<p><strong>Faster completion reduces contention</strong>, which indirectly reduces both voluntary and involuntary context switches.</p>\n<hr>\n<h3 id=\"4-reduced-context-switching-pressure\">4. Reduced Context Switching Pressure</h3>\n<p>Although affinity does not eliminate context switching, it reduces <strong>unnecessary switches caused by migration and rebalancing</strong>.</p>\n<p>With affinity:</p>\n<ul>\n<li>Scheduler decisions are simpler</li>\n<li>Threads are less likely to be displaced</li>\n<li>Execution becomes more stable</li>\n</ul>\n<p>Less scheduling churn means more CPU time is spent executing useful work instead of coordinating execution.</p>\n<hr>\n<h3 id=\"5-better-execution-predictability\">5. Better Execution Predictability</h3>\n<p>Without affinity, execution timing varies depending on:</p>\n<ul>\n<li>Which core runs the thread</li>\n<li>Cache warmth</li>\n<li>Migration frequency</li>\n<li>System activity</li>\n</ul>\n<p>Affinity reduces these variables.</p>\n<p>This leads to:</p>\n<ul>\n<li>Lower latency jitter</li>\n<li>More consistent response times</li>\n<li>Improved worst-case latency</li>\n</ul>\n<p>This predictability is often more valuable than raw throughput.</p>\n<hr>\n<h3 id=\"6-amplified-benefits-on-numa-systems\">6. Amplified Benefits on NUMA Systems</h3>\n<p>On multi-socket systems, memory access latency depends on proximity to the CPU core.</p>\n<p>Without affinity:</p>\n<ul>\n<li>Threads may run far from their memory</li>\n<li>Memory access becomes slower</li>\n<li>Execution time increases</li>\n</ul>\n<p>With affinity:</p>\n<ul>\n<li>Threads stay close to their memory</li>\n<li>Remote memory access is reduced</li>\n<li>Latency becomes more stable</li>\n</ul>\n<p>NUMA systems benefit disproportionately from proper pinning.</p>\n<hr>\n<h2 id=\"when-cpu-thread-affinity-is-most-beneficial\">When CPU / Thread Affinity Is Most Beneficial</h2>\n<p>CPU / thread affinity is useful when a thread performs similar work repeatedly and loses performance because it is moved between CPU cores.<br>\nMoving a thread is not free: the CPU must warm up again before it can run efficiently.</p>\n<p>Affinity helps by <strong>letting a thread stay where it already works well</strong>.</p>\n<hr>\n<h3 id=\"1-cpu-bound-applications\">1. CPU-Bound Applications</h3>\n<p><strong>What this means</strong><br>\nThe application spends most of its time doing calculations, not waiting for disk or network.</p>\n<p><strong>What goes wrong without affinity</strong><br>\nThe thread is constantly ready to run, but the scheduler moves it between cores to balance load.<br>\nEach move forces the CPU to restart execution in a colder state.</p>\n<p><strong>Why affinity helps</strong><br>\nKeeping the thread on one core allows it to run longer without interruption and avoids repeated warm-up.</p>\n<p><strong>Typical effect</strong></p>\n<ul>\n<li>More work done per second</li>\n<li>Less CPU time wasted restarting execution</li>\n</ul>\n<hr>\n<h3 id=\"2-long-lived-worker-threads\">2. Long-Lived Worker Threads</h3>\n<p><strong>What this means</strong><br>\nThe application uses a fixed set of worker threads that stay alive for a long time and process similar tasks.</p>\n<p><strong>What goes wrong without affinity</strong><br>\nWorkers are moved between cores even though their work pattern does not change.<br>\nEach move discards progress the CPU already made optimizing execution.</p>\n<p><strong>Why affinity helps</strong><br>\nA worker that stays on one core becomes more efficient over time and finishes tasks faster.</p>\n<p><strong>Typical effect</strong></p>\n<ul>\n<li>Better steady-state performance</li>\n<li>More stable timing once the system warms up</li>\n</ul>\n<hr>\n<h3 id=\"3-cache-sensitive-workloads\">3. Cache-Sensitive Workloads</h3>\n<p><strong>What this means</strong><br>\nThe application repeatedly accesses the same data in memory.</p>\n<p><strong>What goes wrong without affinity</strong><br>\nWhen a thread moves to another core, the data it was using is no longer nearby.<br>\nThe CPU must fetch the same data again, which takes time.</p>\n<p><strong>Why affinity helps</strong><br>\nKeeping the thread on the same core keeps its data close, so memory access stays fast.</p>\n<p><strong>Typical effect</strong></p>\n<ul>\n<li>Lower memory latency</li>\n<li>Faster execution of repeated operations</li>\n</ul>\n<hr>\n<h3 id=\"4-latency-sensitive-applications\">4. Latency-Sensitive Applications</h3>\n<p><strong>What this means</strong><br>\nResponse time matters more than average throughput, and delays must be predictable.</p>\n<p><strong>What goes wrong without affinity</strong><br>\nSome requests run on warm cores and are fast, others resume on cold cores and are slow.<br>\nThis creates unpredictable latency spikes.</p>\n<p><strong>Why affinity helps</strong><br>\nThreads resume execution in a similar environment every time, reducing randomness.</p>\n<p><strong>Typical effect</strong></p>\n<ul>\n<li>More consistent response times</li>\n<li>Better worst-case latency (P99 / P999)</li>\n</ul>\n<hr>\n<h3 id=\"5-predictable-steady-workloads\">5. Predictable, Steady Workloads</h3>\n<p><strong>What this means</strong><br>\nThe workload shape does not change much over time.</p>\n<p><strong>What goes wrong without affinity</strong><br>\nThe scheduler keeps moving threads even though there is no real imbalance to fix.</p>\n<p><strong>Why affinity helps</strong><br>\nFixed core assignment avoids unnecessary movement and keeps execution stable.</p>\n<p><strong>Typical effect</strong></p>\n<ul>\n<li>Repeatable performance</li>\n<li>Easier tuning and capacity planning</li>\n</ul>\n<hr>\n<h3 id=\"6-dedicated-or-single-tenant-systems\">6. Dedicated or Single-Tenant Systems</h3>\n<p><strong>What this means</strong><br>\nOne application owns the machine.</p>\n<p><strong>What goes wrong without affinity</strong><br>\nThreads are moved around for fairness that the application does not need.</p>\n<p><strong>Why affinity helps</strong><br>\nRestricting movement reduces interference and improves isolation.</p>\n<p><strong>Typical effect</strong></p>\n<ul>\n<li>Better resource usage</li>\n<li>More predictable system behavior</li>\n</ul>\n<hr>\n<h3 id=\"7-numa-systems-without-numa-aware-code\">7. NUMA Systems (Without NUMA-Aware Code)</h3>\n<p><strong>What this means</strong><br>\nThe machine has multiple CPU sockets, each with its own memory.</p>\n<p><strong>What goes wrong without affinity</strong><br>\nThreads move between sockets and access memory that is far away, which is slower.</p>\n<p><strong>Why affinity helps</strong><br>\nKeeping threads near their memory avoids expensive remote access.</p>\n<p><strong>Typical effect</strong></p>\n<ul>\n<li>Lower memory access latency</li>\n<li>More consistent throughput</li>\n</ul>\n<h2 id=\"rule-of-thumb\">Rule of Thumb</h2>\n<blockquote>\n<p>Use CPU / thread affinity when threads do similar work for a long time and lose performance because they are moved between CPU cores.</p>\n</blockquote>\n<hr>\n<h2 id=\"c-examples\">C# Examples</h2>\n<h3 id=\"process-affinity-most-common\">Process Affinity (Most Common)</h3>\n<pre><code class=\"language-csharp\">using System;\nusing System.Diagnostics;\n\nclass Program\n{\n    static void Main()\n    {\n        var process = Process.GetCurrentProcess();\n\n        // Pin process to CPU 0 and 1 (binary mask: 00000011)\n        process.ProcessorAffinity = new IntPtr(0b00000011);\n\n        Console.WriteLine(\"Process pinned to CPU 0 and 1\");\n    }\n}\n</code></pre>\n','6951521f19c94fae00e3739b','Control CPU affinity to improve execution predictability and reduce unnecessary movement between CPU cores, at the cost of scheduler flexibility.\n\n\n\n\nExecutive Summary (TL;DR)\n\n\nCPU affinity (also called CPU pinning) means telling the operating system that a process or thread should run only on specific CPU cores. By default, the operating system is free to move threads between cores to balance load. While this is usually beneficial, it can hurt performance for latency-sensitive or cache-sensitive workloads.\n\n\nPinning execution to specific cores reduces unnecessary movement, improves execution continuity, and makes performance more predictable. Typical improvements range from 520% for cache-sensitive workloads and can be much higher on multi-socket (NUMA) systems. The trade-off is reduced scheduler flexibility and potential load imbalance.\n\n\nUse CPU pinning when predictability matters more than maximum flexibility.\n\n\n\n\nA Simple Mental Model (Read This First)\n\n\nThink of a CPU core as a desk, and a thread as a worker.\n\n\n * When the worker stays at the same desk, their tools are already laid out.\n * When the worker is moved to another desk, they must set everything up again.\n * If the worker is moved frequently, more time is spent setting up than working.\n\n\nCPU pinning simply tells the operating system:\n\nStop moving this worker between desks.\n\n\nYou dont need to know what the tools are internally.\n\nLess movement means less wasted time.\n\n\n\n\nProblem Context\n\n\nModern operating systems use schedulers designed to keep all CPU cores busy. To achieve this, they dynamically move processes and threads between cores to balance load, improve fairness, and optimize power usage.\n\n\nFor most applications, this behavior is ideal. However, for workloads that:\n\n\n * require consistent timing\n * repeatedly access the same data\n * are sensitive to latency spikes\n\n\nfrequent core migration becomes expensive.\n\n\nEach migration interrupts execution and forces internal state to be rebuilt. Over time, this creates instability and unpredictable performance.\n\n\n\n\nWhat Is CPU Affinity?\n\n\nCPU affinity defines which CPU cores a process or thread is allowed to run on.\n\n\n * Without affinity: the OS may run your code on any core\n * With affinity: the OS is restricted to a specific set of cores\n\n\nOnce affinity is set, the scheduler respects that constraint and avoids migrating execution outside the allowed cores.\n\n\n\n\nDefault Thread Affinity and Why Threads Move Between Cores\n\n\nBy default, operating systems do not strictly bind threads to specific CPU cores.\n\n\nInstead, the scheduler uses a soft affinity strategy:\n\n\n * It prefers to run a thread on the same core it ran on last\n * But it is free to move the thread whenever it decides it is beneficial\n\n\nThis means:\n\n\n * Threads can run on any core\n * Core assignment is dynamic\n * There is no guarantee that a thread will stay on the same core\n\n\n\nWhy Threads Are Migrated\n\n\nThreads move between cores for several reasons:\n\n\n * \n   \n   \n   Load balancing\n   \n   To prevent some cores from being overloaded while others are idle\n   \n\n * \n   \n   \n   Fairness\n   \n   To ensure all runnable threads get CPU time\n   \n\n * \n   \n   \n   Power and thermal management\n   \n   To spread heat and optimize energy usage\n   \n\n * \n   \n   \n   Oversubscription\n   \n   When more threads are active than available cores\n   \n\n * \n   \n   \n   System activity\n   \n   Interrupt handling, kernel threads, and background services\n   \n\n\nFrom the schedulers perspective, migration is often the right choice.\n\nFrom the applications perspective, migration can introduce overhead and unpredictability.\n\n\nCPU pinning exists to override this behavior when predictability matters more than flexibility.\n\n\n\n\nWhy Uncontrolled Core Migration Hurts Performance\n\n\n\n1. Execution Is Repeatedly Interrupted\n\n\nEach time a thread moves to another core, execution is paused and resumed elsewhere. Even though this is fast, doing it repeatedly adds noticeable overhead.\n\n\n\n\n2. Internal Execution State Is Rebuilt\n\n\nWhile running, the CPU prepares internal state to execute code efficiently. When execution moves to another core, that preparation must be rebuilt, slowing progress.\n\n\nYou dont need hardware details to understand this:\n\nmovement causes repetition.\n\n\n\n\n3. Latency Becomes Unpredictable\n\n\nSome executions run uninterrupted, others are moved several times. This creates variability and hurts worst-case latency.\n\n\n\n\n4. Problems Amplify Under Load\n\n\nAs load increases:\n\n\n * more threads compete for cores\n * more migration occurs\n * delays compound\n\n\nThis creates a feedback loop that rapidly degrades performance.\n\n\n\n\nWhy Pinning Improves Many Things at Once\n\n\nCPU pinning improves performance not by optimizing one component, but by removing an entire class of inefficiency.\n\n\n * Less movement  less repeated setup\n * Longer uninterrupted execution  more useful work\n * Faster task completion  less system pressure\n * More stable timing  better tail latency\n\n\nThis is why pinning often improves latency, throughput, and predictability simultaneously.\n\n\n\n\nHow CPU Pinning Works (High-Level)\n\n\n\nHardware Perspective\n\n\n * Each CPU core maintains its own execution context\n * Staying on the same core preserves continuity\n * Multi-socket (NUMA) systems amplify the cost of movement\n\n\n\nOperating System Perspective\n\n\n * The OS maintains an affinity mask for each process or thread\n * The scheduler respects this mask\n * Context switches still happen, but core migration is reduced or eliminated\n\n\n\n\nAdvantages\n\n\n * More predictable performance\n * Reduced execution interruption\n * Improved behavior under load\n * Strong benefits in NUMA systems\n * Better isolation for critical workloads\n\n\n\n\nDisadvantages and Trade-offs\n\n\n * Reduced scheduler flexibility\n * Risk of load imbalance\n * Requires hardware awareness\n * Can waste resources if misused\n * Not suitable for dynamic workloads\n\n\nCPU pinning trades flexibility for predictability.\n\n\n\n\nWhy Thread Affinity Improves Performance (Deeper Explanation)\n\n\nThread and CPU affinity improve performance primarily by preserving execution locality. When a thread remains on the same CPU core, the system avoids repeatedly rebuilding execution context that was already optimized.\n\n\nThis improvement comes from several reinforcing effects.\n\n\n\n\n1. Preserving CPU Cache Locality\n\n\nEach CPU core maintains private caches that store recently used data and instructions. When a thread runs continuously on the same core, the data it needs remains readily available.\n\n\nWhen a thread migrates to another core:\n\n\n * Its data is no longer in the local cache\n * The new core must fetch data again\n * Execution stalls while data is reloaded\n\n\nThis work was already done once and must now be repeated.\n\n\nThread affinity keeps caches warm, reducing cache misses and avoiding repeated data loading.\n\n\n\n\n2. Fewer Cache Misses Means Fewer CPU Stalls\n\n\nCache misses force the CPU to wait for data. These waits add latency even if no context switch occurs.\n\n\nCore migration dramatically increases cache misses:\n\n\n * Local cache state is lost\n * Execution restarts in a colder environment\n * Progress slows\n\n\nBy keeping threads on the same core, affinity reduces cache miss frequency and improves sustained execution speed.\n\n\n\n\n3. Longer Uninterrupted Execution Windows\n\n\nThread migration is often correlated with:\n\n\n * Preemption\n * Short execution slices\n * Increased context switching\n\n\nWhen a thread is pinned to a core:\n\n\n * It is more likely to continue running\n * It completes work faster\n * It exits the run queue sooner\n\n\nFaster completion reduces contention, which indirectly reduces both voluntary and involuntary context switches.\n\n\n\n\n4. Reduced Context Switching Pressure\n\n\nAlthough affinity does not eliminate context switching, it reduces unnecessary switches caused by migration and rebalancing.\n\n\nWith affinity:\n\n\n * Scheduler decisions are simpler\n * Threads are less likely to be displaced\n * Execution becomes more stable\n\n\nLess scheduling churn means more CPU time is spent executing useful work instead of coordinating execution.\n\n\n\n\n5. Better Execution Predictability\n\n\nWithout affinity, execution timing varies depending on:\n\n\n * Which core runs the thread\n * Cache warmth\n * Migration frequency\n * System activity\n\n\nAffinity reduces these variables.\n\n\nThis leads to:\n\n\n * Lower latency jitter\n * More consistent response times\n * Improved worst-case latency\n\n\nThis predictability is often more valuable than raw throughput.\n\n\n\n\n6. Amplified Benefits on NUMA Systems\n\n\nOn multi-socket systems, memory access latency depends on proximity to the CPU core.\n\n\nWithout affinity:\n\n\n * Threads may run far from their memory\n * Memory access becomes slower\n * Execution time increases\n\n\nWith affinity:\n\n\n * Threads stay close to their memory\n * Remote memory access is reduced\n * Latency becomes more stable\n\n\nNUMA systems benefit disproportionately from proper pinning.\n\n\n\n\nWhen CPU / Thread Affinity Is Most Beneficial\n\n\nCPU / thread affinity is useful when a thread performs similar work repeatedly and loses performance because it is moved between CPU cores.\n\nMoving a thread is not free: the CPU must warm up again before it can run efficiently.\n\n\nAffinity helps by letting a thread stay where it already works well.\n\n\n\n\n1. CPU-Bound Applications\n\n\nWhat this means\n\nThe application spends most of its time doing calculations, not waiting for disk or network.\n\n\nWhat goes wrong without affinity\n\nThe thread is constantly ready to run, but the scheduler moves it between cores to balance load.\n\nEach move forces the CPU to restart execution in a colder state.\n\n\nWhy affinity helps\n\nKeeping the thread on one core allows it to run longer without interruption and avoids repeated warm-up.\n\n\nTypical effect\n\n\n * More work done per second\n * Less CPU time wasted restarting execution\n\n\n\n\n2. Long-Lived Worker Threads\n\n\nWhat this means\n\nThe application uses a fixed set of worker threads that stay alive for a long time and process similar tasks.\n\n\nWhat goes wrong without affinity\n\nWorkers are moved between cores even though their work pattern does not change.\n\nEach move discards progress the CPU already made optimizing execution.\n\n\nWhy affinity helps\n\nA worker that stays on one core becomes more efficient over time and finishes tasks faster.\n\n\nTypical effect\n\n\n * Better steady-state performance\n * More stable timing once the system warms up\n\n\n\n\n3. Cache-Sensitive Workloads\n\n\nWhat this means\n\nThe application repeatedly accesses the same data in memory.\n\n\nWhat goes wrong without affinity\n\nWhen a thread moves to another core, the data it was using is no longer nearby.\n\nThe CPU must fetch the same data again, which takes time.\n\n\nWhy affinity helps\n\nKeeping the thread on the same core keeps its data close, so memory access stays fast.\n\n\nTypical effect\n\n\n * Lower memory latency\n * Faster execution of repeated operations\n\n\n\n\n4. Latency-Sensitive Applications\n\n\nWhat this means\n\nResponse time matters more than average throughput, and delays must be predictable.\n\n\nWhat goes wrong without affinity\n\nSome requests run on warm cores and are fast, others resume on cold cores and are slow.\n\nThis creates unpredictable latency spikes.\n\n\nWhy affinity helps\n\nThreads resume execution in a similar environment every time, reducing randomness.\n\n\nTypical effect\n\n\n * More consistent response times\n * Better worst-case latency (P99 / P999)\n\n\n\n\n5. Predictable, Steady Workloads\n\n\nWhat this means\n\nThe workload shape does not change much over time.\n\n\nWhat goes wrong without affinity\n\nThe scheduler keeps moving threads even though there is no real imbalance to fix.\n\n\nWhy affinity helps\n\nFixed core assignment avoids unnecessary movement and keeps execution stable.\n\n\nTypical effect\n\n\n * Repeatable performance\n * Easier tuning and capacity planning\n\n\n\n\n6. Dedicated or Single-Tenant Systems\n\n\nWhat this means\n\nOne application owns the machine.\n\n\nWhat goes wrong without affinity\n\nThreads are moved around for fairness that the application does not need.\n\n\nWhy affinity helps\n\nRestricting movement reduces interference and improves isolation.\n\n\nTypical effect\n\n\n * Better resource usage\n * More predictable system behavior\n\n\n\n\n7. NUMA Systems (Without NUMA-Aware Code)\n\n\nWhat this means\n\nThe machine has multiple CPU sockets, each with its own memory.\n\n\nWhat goes wrong without affinity\n\nThreads move between sockets and access memory that is far away, which is slower.\n\n\nWhy affinity helps\n\nKeeping threads near their memory avoids expensive remote access.\n\n\nTypical effect\n\n\n * Lower memory access latency\n * More consistent throughput\n\n\n\nRule of Thumb\n\n\n\n\n\nUse CPU / thread affinity when threads do similar work for a long time and lose performance because they are moved between CPU cores.\n\n\n\n\n\nC# Examples\n\n\n\nProcess Affinity (Most Common)\n\n\nusing System;\nusing System.Diagnostics;\n\nclass Program\n{\n    static void Main()\n    {\n        var process = Process.GetCurrentProcess();\n\n        // Pin process to CPU 0 and 1 (binary mask: 00000011)\n        process.ProcessorAffinity = new IntPtr(0b00000011);\n\n        Console.WriteLine(\"Process pinned to CPU 0 and 1\");\n    }\n}\n\n',NULL,0,'post','published',NULL,'public','all','2025-12-28 15:51:59','2025-12-29 05:25:52','2025-12-28 15:51:59','694f2665c3e44e650f0e5dd8',NULL,NULL,NULL,NULL,NULL,NULL,1),('6951b07819c94fae00e373ad','1f7b4160-2067-472c-99ee-acd3dfe82d8a','Privacy Policy','privacy-policy',NULL,'{\"root\":{\"children\":[{\"children\":[{\"detail\":0,\"format\":0,\"mode\":\"normal\",\"style\":\"\",\"text\":\"This website uses cookies and similar technologies to enhance user experience, analyze traffic, and display advertisements.\",\"type\":\"extended-text\",\"version\":1}],\"direction\":\"ltr\",\"format\":\"\",\"indent\":0,\"type\":\"paragraph\",\"version\":1},{\"children\":[{\"detail\":0,\"format\":0,\"mode\":\"normal\",\"style\":\"\",\"text\":\"Third-party vendors, including Google, use cookies to serve ads based on a user\'s prior visits to this website or other websites.\",\"type\":\"extended-text\",\"version\":1}],\"direction\":\"ltr\",\"format\":\"\",\"indent\":0,\"type\":\"paragraph\",\"version\":1},{\"children\":[{\"detail\":0,\"format\":0,\"mode\":\"normal\",\"style\":\"\",\"text\":\"Google\'s use of advertising cookies enables it and its partners to serve ads to users based on their visits to this site and/or other sites on the Internet.\",\"type\":\"extended-text\",\"version\":1}],\"direction\":\"ltr\",\"format\":\"\",\"indent\":0,\"type\":\"paragraph\",\"version\":1},{\"children\":[{\"detail\":0,\"format\":0,\"mode\":\"normal\",\"style\":\"\",\"text\":\"Users may opt out of personalized advertising by visiting:\",\"type\":\"extended-text\",\"version\":1},{\"type\":\"linebreak\",\"version\":1},{\"children\":[{\"detail\":0,\"format\":0,\"mode\":\"normal\",\"style\":\"\",\"text\":\"https://adssettings.google.com\",\"type\":\"extended-text\",\"version\":1}],\"direction\":\"ltr\",\"format\":\"\",\"indent\":0,\"type\":\"link\",\"version\":1,\"rel\":null,\"target\":null,\"title\":null,\"url\":\"https://adssettings.google.com\"}],\"direction\":\"ltr\",\"format\":\"\",\"indent\":0,\"type\":\"paragraph\",\"version\":1},{\"children\":[{\"detail\":0,\"format\":0,\"mode\":\"normal\",\"style\":\"\",\"text\":\"We do not sell, trade, or otherwise transfer your personally identifiable information to outside parties except as required by law.\",\"type\":\"extended-text\",\"version\":1}],\"direction\":\"ltr\",\"format\":\"\",\"indent\":0,\"type\":\"paragraph\",\"version\":1},{\"children\":[{\"detail\":0,\"format\":0,\"mode\":\"normal\",\"style\":\"\",\"text\":\"By using this website, you consent to this Privacy Policy.\",\"type\":\"extended-text\",\"version\":1}],\"direction\":\"ltr\",\"format\":\"\",\"indent\":0,\"type\":\"paragraph\",\"version\":1},{\"children\":[{\"detail\":0,\"format\":0,\"mode\":\"normal\",\"style\":\"\",\"text\":\"If you have any questions regarding this Privacy Policy, please contact us through the Contact page.\",\"type\":\"extended-text\",\"version\":1}],\"direction\":\"ltr\",\"format\":\"\",\"indent\":0,\"type\":\"paragraph\",\"version\":1}],\"direction\":\"ltr\",\"format\":\"\",\"indent\":0,\"type\":\"root\",\"version\":1}}','<p>This website uses cookies and similar technologies to enhance user experience, analyze traffic, and display advertisements.</p><p>Third-party vendors, including Google, use cookies to serve ads based on a user\'s prior visits to this website or other websites.</p><p>Google\'s use of advertising cookies enables it and its partners to serve ads to users based on their visits to this site and/or other sites on the Internet.</p><p>Users may opt out of personalized advertising by visiting:<br><a href=\"https://adssettings.google.com\">https://adssettings.google.com</a></p><p>We do not sell, trade, or otherwise transfer your personally identifiable information to outside parties except as required by law.</p><p>By using this website, you consent to this Privacy Policy.</p><p>If you have any questions regarding this Privacy Policy, please contact us through the Contact page.</p>','6951b07819c94fae00e373ad','This website uses cookies and similar technologies to enhance user experience, analyze traffic, and display advertisements.\n\nThird-party vendors, including Google, use cookies to serve ads based on a user\'s prior visits to this website or other websites.\n\nGoogle\'s use of advertising cookies enables it and its partners to serve ads to users based on their visits to this site and/or other sites on the Internet.\n\nUsers may opt out of personalized advertising by visiting:\nhttps://adssettings.google.com\n\nWe do not sell, trade, or otherwise transfer your personally identifiable information to outside parties except as required by law.\n\nBy using this website, you consent to this Privacy Policy.\n\nIf you have any questions regarding this Privacy Policy, please contact us through the Contact page.',NULL,0,'page','published',NULL,'public','all','2025-12-28 22:34:32','2025-12-29 00:17:05','2025-12-28 22:35:21','694f2665c3e44e650f0e5dd8',NULL,NULL,NULL,NULL,NULL,NULL,1),('6951b0ef19c94fae00e373b5','a3ca305f-625d-4bf1-a62a-bfb01bd379dd','Terms of Service','terms-of-service',NULL,'{\"root\":{\"children\":[{\"children\":[{\"detail\":0,\"format\":0,\"mode\":\"normal\",\"style\":\"\",\"text\":\"By accessing and using this website, you accept and agree to be bound by the terms and provision of this agreement.\",\"type\":\"extended-text\",\"version\":1}],\"direction\":\"ltr\",\"format\":\"\",\"indent\":0,\"type\":\"paragraph\",\"version\":1},{\"children\":[{\"detail\":0,\"format\":0,\"mode\":\"normal\",\"style\":\"\",\"text\":\"The content provided on this website is for informational purposes only. We make no representations or warranties of any kind regarding the accuracy, reliability, or completeness of any information.\",\"type\":\"extended-text\",\"version\":1}],\"direction\":\"ltr\",\"format\":\"\",\"indent\":0,\"type\":\"paragraph\",\"version\":1},{\"children\":[{\"detail\":0,\"format\":0,\"mode\":\"normal\",\"style\":\"\",\"text\":\"We reserve the right to modify, suspend, or discontinue any part of the website at any time without notice.\",\"type\":\"extended-text\",\"version\":1}],\"direction\":\"ltr\",\"format\":\"\",\"indent\":0,\"type\":\"paragraph\",\"version\":1},{\"children\":[{\"detail\":0,\"format\":0,\"mode\":\"normal\",\"style\":\"\",\"text\":\"Your use of this website is at your own risk.\",\"type\":\"extended-text\",\"version\":1}],\"direction\":\"ltr\",\"format\":\"\",\"indent\":0,\"type\":\"paragraph\",\"version\":1},{\"children\":[{\"detail\":0,\"format\":0,\"mode\":\"normal\",\"style\":\"\",\"text\":\"These Terms of Service are subject to change without notice.\",\"type\":\"extended-text\",\"version\":1}],\"direction\":\"ltr\",\"format\":\"\",\"indent\":0,\"type\":\"paragraph\",\"version\":1}],\"direction\":\"ltr\",\"format\":\"\",\"indent\":0,\"type\":\"root\",\"version\":1}}','<p>By accessing and using this website, you accept and agree to be bound by the terms and provision of this agreement.</p><p>The content provided on this website is for informational purposes only. We make no representations or warranties of any kind regarding the accuracy, reliability, or completeness of any information.</p><p>We reserve the right to modify, suspend, or discontinue any part of the website at any time without notice.</p><p>Your use of this website is at your own risk.</p><p>These Terms of Service are subject to change without notice.</p>','6951b0ef19c94fae00e373b5','By accessing and using this website, you accept and agree to be bound by the terms and provision of this agreement.\n\nThe content provided on this website is for informational purposes only. We make no representations or warranties of any kind regarding the accuracy, reliability, or completeness of any information.\n\nWe reserve the right to modify, suspend, or discontinue any part of the website at any time without notice.\n\nYour use of this website is at your own risk.\n\nThese Terms of Service are subject to change without notice.',NULL,0,'page','published',NULL,'public','all','2025-12-28 22:36:31','2025-12-29 00:17:30','2025-12-28 22:36:48','694f2665c3e44e650f0e5dd8',NULL,NULL,NULL,NULL,NULL,NULL,1),('6951b1c119c94fae00e373bd','b991a96e-cf9d-4fa8-b2d7-f70f0dafd328','Contact','contact',NULL,'{\"root\":{\"children\":[{\"children\":[{\"detail\":0,\"format\":0,\"mode\":\"normal\",\"style\":\"\",\"text\":\"Contact\",\"type\":\"extended-text\",\"version\":1}],\"direction\":\"ltr\",\"format\":\"\",\"indent\":0,\"type\":\"extended-heading\",\"version\":1,\"tag\":\"h1\"},{\"children\":[{\"detail\":0,\"format\":0,\"mode\":\"normal\",\"style\":\"\",\"text\":\"If you have any questions, feedback, or business inquiries, please contact us using the information below:\",\"type\":\"extended-text\",\"version\":1}],\"direction\":\"ltr\",\"format\":\"\",\"indent\":0,\"type\":\"paragraph\",\"version\":1},{\"children\":[{\"detail\":0,\"format\":1,\"mode\":\"normal\",\"style\":\"\",\"text\":\"Email:\",\"type\":\"extended-text\",\"version\":1},{\"detail\":0,\"format\":0,\"mode\":\"normal\",\"style\":\"\",\"text\":\" jackodes404@gmail.com\",\"type\":\"extended-text\",\"version\":1}],\"direction\":\"ltr\",\"format\":\"\",\"indent\":0,\"type\":\"paragraph\",\"version\":1},{\"children\":[{\"detail\":0,\"format\":0,\"mode\":\"normal\",\"style\":\"\",\"text\":\"We aim to respond within 2448 hours.\",\"type\":\"extended-text\",\"version\":1}],\"direction\":\"ltr\",\"format\":\"\",\"indent\":0,\"type\":\"paragraph\",\"version\":1}],\"direction\":\"ltr\",\"format\":\"\",\"indent\":0,\"type\":\"root\",\"version\":1}}','<h1 id=\"contact\">Contact</h1><p>If you have any questions, feedback, or business inquiries, please contact us using the information below:</p><p><strong>Email:</strong> jackodes404@gmail.com</p><p>We aim to respond within 2448 hours.</p>','6951b1c119c94fae00e373bd','Contact\n\nIf you have any questions, feedback, or business inquiries, please contact us using the information below:\n\nEmail: jackodes404@gmail.com\n\nWe aim to respond within 2448 hours.',NULL,0,'page','published',NULL,'public','all','2025-12-28 22:40:01','2025-12-29 00:20:17','2025-12-28 22:40:19','694f2665c3e44e650f0e5dd8',NULL,NULL,NULL,NULL,NULL,NULL,1),('69532d9aebcfaae683eec996','d8466ef0-6f9a-4ae2-864d-dd3affca316c','Avoid False Sharing and Cache Line Contention','avoid-false-sharing-and-cache-line-contention',NULL,'{\"root\":{\"children\":[{\"type\":\"markdown\",\"version\":1,\"markdown\":\"## Executive Summary (TL;DR)\\n\\nFalse sharing happens when **multiple threads write to different variables that live inside the same CPU cache line** (usually 64 bytes).\\n\\nEven though the variables are logically independent, the CPU cache works with **cache lines**, not variables.  \\nWhen one thread writes, other cores must invalidate their copy of the entire cache line.  \\nThe cache line then **bounces between cores**, creating hidden serialization.\\n\\nThis causes:\\n- Poor scalability\\n- High CPU usage with low throughput\\n- Performance degradation (1050% or worse)\\n\\nFalse sharing **does NOT break correctness**, only performance  which makes it hard to notice.\\n\\n**Solution:** ensure that data frequently written by different threads lives in **different cache lines**, using:\\n- Per-thread data (preferred)\\n- Padding and alignment (when necessary)\\n\\nOnly apply after **profiling confirms cache contention**.\\n\\n---\\n\\n## 1. What Is a Cache Line? (From Zero)\\n\\nThe CPU does **not** load individual variables into cache.\\n\\nInstead, it loads memory in **fixed-size blocks** called **cache lines**.\\n\\n- Typical size: **64 bytes** (x86-64)\\n- Some ARM CPUs: **128 bytes**\\n- Minimum unit moved between:\\n  - RAM  cache\\n  - Core  core\\n\\n> You cannot partially load or partially invalidate a cache line.  \\n> It is always **all or nothing**.\\n\\n---\\n\\n## 2. Simple Analogy (Very Important)\\n\\nThink of memory like this:\\n\\n- **RAM** = a large filing cabinet\\n- **CPU cache** = your desk\\n- **Cache line** = a folder\\n\\nEven if you only need **one page**, you must bring **the entire folder** to your desk.\\n\\nThe CPU works exactly the same way.\\n\\n---\\n\\n## 3. How Variables End Up Sharing Cache Lines\\n\\n```csharp\\nclass Counters {\\n    public long A; // 8 bytes\\n    public long B; // 8 bytes\\n}\\n```\\n\\n**Memory layout (simplified):**\\n\\n```\\n| A (8B) | B (8B) | other data ... |\\n|<---------- 64 bytes (one cache line) ---------->|\\n```\\n\\nEven though A and B are:\\n- Different variables\\n- Used by different threads\\n\\nThey live in the same cache line.\\n\\n---\\n\\n## 4. The Golden Rule (Explains Everything)\\n\\n**If two threads write to anything inside the same cache line, they compete.**\\n\\nIt does **NOT** matter:\\n- That variables are different\\n- That there are no locks\\n- That the code is correct\\n\\nThis is false sharing.\\n\\n---\\n\\n## 5. Why the CPU Forces This (Cache Coherency)\\n\\nEach CPU core has its own cache.\\n\\nThe CPU must guarantee:\\n\\n> \\\"All cores see a consistent view of memory.\\\"\\n\\nTo do this, it uses cache coherency protocols (e.g., MESI).\\n\\n**Core rule:**\\n- Only one core can modify a cache line at a time\\n- Other cores must invalidate their copy before writing\\n\\nCorrect behavior  but expensive.\\n\\n---\\n\\n## 6. False Sharing Explained as a Timeline (Cronograma)\\n\\n### Scenario Setup\\n\\n- **Core 0**  Thread 0  writes to variable `A`\\n- **Core 1**  Thread 1  writes to variable `B`\\n- `A` and `B` share the same cache line\\n\\n### Why Different Cores Have the Same Cache Line\\n\\nWhen Thread 0 on Core 0 first reads `A`, the CPU loads the entire 64-byte cache line containing `A` (and `B`) into Core 0\'s L1 cache.\\n\\nLater, when Thread 1 on Core 1 reads `B`, the CPU loads the same cache line (now containing both `A` and `B`) into Core 1\'s L1 cache.\\n\\n**Both cores now have identical copies of the same cache line in their local caches.**\\n\\n### Detailed Timeline\\n\\n```\\nTime    Core 0 (Thread 0)              Core 1 (Thread 1)              Cache Line State\\n\\nT0      Reads A                        -                               Core 0: Exclusive (E)\\n                                                                        Cache line loaded from RAM\\n                                                                        Cost: ~100-300 cycles\\n\\nT1      -                              Reads B                         Core 0: Shared (S)\\n                                                                        Core 1: Shared (S)\\n                                                                        Both have read-only copies\\n                                                                        Cost: ~40-100 cycles (transfer from Core 0)\\n\\nT2      Writes to A                    -                               Core 0: Modified (M)\\n                                                                        Core 1: Invalid (I)  INVALIDATION!\\n                                                                        \\n                                                                        Core 0 sends \\\"invalidate\\\" message to Core 1\\n                                                                        Cost: ~10-20 cycles (inter-core communication)\\n\\nT3      (continues working)            Wants to write B                Core 1 detects cache line is Invalid\\n                                                                        Core 1 must request cache line from Core 0\\n                                                                        \\n                                                                        Request sent to Core 0: \\\"I need this cache line\\\"\\n                                                                        Cost: ~10-20 cycles (request)\\n\\nT4      Receives request               (waiting...)                    Core 0 must write back to memory (if Modified)\\n                                                                        Core 0 sends cache line to Core 1\\n                                                                        Cost: ~40-100 cycles (write-back + transfer)\\n\\nT5      -                              Receives cache line             Core 0: Shared (S)\\n                                                                        Core 1: Modified (M)\\n                                                                        Now Core 1 has exclusive ownership\\n                                                                        Cost: ~10 cycles (state update)\\n\\nT6      (wants A again)                Writes to B                     Core 1: Modified (M)\\n                                                                        Core 0: Invalid (I)  INVALIDATION AGAIN!\\n                                                                        \\n                                                                        Process repeats...\\n                                                                        Cost: ~40-100 cycles per cycle\\n```\\n\\n### Who Controls the Cache Line Updates?\\n\\n**The CPU\'s cache coherency protocol (MESI) controls everything automatically:**\\n\\n1. **Hardware-level**: No software involvement requiredit happens in CPU hardware\\n2. **Cache controller**: Each core has a cache controller that manages MESI states\\n3. **Interconnect**: Cores communicate through the CPU interconnect (bus or mesh)\\n4. **Snooping**: Cores \\\"snoop\\\" on each other\'s cache transactions to maintain coherency\\n\\n### Cost Breakdown\\n\\n**Per false sharing cycle:**\\n- Invalidation message: ~10-20 cycles\\n- Cache line request: ~10-20 cycles  \\n- Write-back to memory (if needed): ~10-30 cycles\\n- Cache line transfer: ~40-100 cycles\\n- State updates: ~5-10 cycles\\n\\n**Total per cycle: ~75-180 CPU cycles**\\n\\nIf Thread 0 writes to `A` 1 million times per second, and Thread 1 writes to `B` 1 million times per second:\\n- **Potential cache line transfers: 2 million per second**\\n- **Wasted cycles: ~150-360 million cycles per second**\\n- **On a 3GHz CPU: 5-12% of total CPU time wasted on false sharing overhead**\\n\\n### Why This Creates Serialization\\n\\nEven though Thread 0 and Thread 1 are running on different cores (true parallelism), they **cannot write simultaneously** because:\\n\\n1. Only one core can have the cache line in Modified state\\n2. The other core must wait for the transfer to complete\\n3. This creates **implicit serialization** at the hardware level\\n\\n**Result**: What looks like parallel execution is actually serial execution with expensive synchronization.\\n\\n### Visual Representation\\n\\n```\\nNormal Parallel Execution (no false sharing):\\n\\nCore 0: [Write A][Write A][Write A][Write A]...   Continuous\\nCore 1: [Write B][Write B][Write B][Write B]...   Continuous\\n         Both working simultaneously\\n\\nWith False Sharing:\\n\\nCore 0: [Write A][----WAIT----][Write A][----WAIT----]...\\nCore 1: [----WAIT----][Write B][----WAIT----][Write B]...\\n         Taking turns (serialized!)\\n         Wasted cycles during WAIT\\n```\\n\\nThis hidden serialization is why performance degrades even though your code looks perfectly parallel.\\n\\n---\\n\\n### Common Misconceptions\\n\\n**\\\"Separate variables mean separate memory locations\\\"**\\n- The CPU caches data in 64-byte chunks called cache lines. Two variables declared separately can end up on the same cache line if they\'re close in memory. Think of it like apartment buildings: even though you live in apartment 101 and your neighbor in 102, you share the same building (cache line).\\n\\n**\\\"Lock-free code is automatically fast\\\"**\\n- Lock-free data structures avoid blocking but can suffer from false sharing when multiple threads update adjacent fields. Atomic operations still trigger cache invalidations.\\n\\n**\\\"High CPU usage means good parallelization\\\"**\\n- False sharing can cause high CPU usage while destroying actual parallelism. CPUs spend time waiting for cache lines to transfer between cores, not doing useful work.\\n\\n**\\\"The compiler/runtime will optimize this away\\\"**\\n- Compilers don\'t automatically pad structures to prevent false sharing. They optimize for single-threaded performance, not multi-threaded cache behavior.\\n\\n### Why Naive Solutions Fail\\n\\n**Adding locks**: Makes it worse by serializing execution completely. The problem is cache contention, not lack of synchronization.\\n\\n**Increasing thread count**: More threads mean more cache invalidations (O(n) growth). The problem compounds.\\n\\n**Logical separation**: Different variables can still share cache lines if they\'re physically close in memory. CPU sees physical layout, not code structure.\\n\\n**Algorithm optimization alone**: Doesn\'t help if threads are fighting over cache lines at the hardware level.\\n\\n## Why This Becomes a Bottleneck\\n\\n**Cache Line Ping-Pong**: Cache lines constantly bounce between cores. Each transfer consumes bandwidth and creates latency. Impact: 10-50% performance degradation.\\n\\n**Serialization Despite Parallelism**: Threads appear to run in parallel but wait for each other at the cache level. Impact: Applications that should scale linearly instead plateau or degrade.\\n\\n**Memory Bus Saturation**: Cache line transfers consume bandwidth, competing with actual data access. Impact: System-wide performance degradation.\\n\\n**NUMA Amplification**: In NUMA systems, remote cache transfers are 2-3x slower. Impact: 50-100% additional latency.\\n\\n**Scalability Collapse**: Performance degrades with more threads instead of improving. Impact: Applications fail to utilize available CPU cores.\\n\\n## When to Use This Approach\\n\\n**High-performance multi-threaded applications**: Many threads, high throughput requirements, processing large volumes of data.\\n\\n**Frequently-updated shared state**: Per-thread counters, statistics structures, lock-free data structures with adjacent fields.\\n\\n**Profiling indicates cache contention**: High cache miss rates, poor scalability, cache line transfers detected by tools.\\n\\n**High-thread-count systems**: 8+ cores where false sharing effects are amplified. Especially important in NUMA systems.\\n\\n**Latency-sensitive parallel workloads**: Real-time systems, financial trading, game engines, media processing.\\n\\n**Lock-free algorithms**: Lock-free queues, stacks, hash tables. These are particularly susceptible to false sharing.\\n\\n## When Not to Use It\\n\\n**Single-threaded applications**: No parallelism means no false sharing.\\n\\n**Read-only shared data**: False sharing only occurs with writes. Multiple threads reading is fine (Shared state).\\n\\n**Infrequently accessed data**: Padding overhead isn\'t justified by occasional access.\\n\\n**Small data structures**: Structures that naturally span multiple cache lines might not need explicit padding.\\n\\n**Cloud/containerized environments**: Hardware topology is abstracted, cache line sizes may vary.\\n\\n**Development/prototyping**: Premature optimization distracts from correctness. Profile first.\\n\\n**Memory-constrained systems**: Embedded systems, mobile devices. Padding overhead might be unacceptable.\\n\\n**When profiling shows no issue**: Don\'t optimize what isn\'t broken. Use tools to confirm before adding padding.\\n\\n---\\n\\n## How to Avoid False Sharing (General Principles)\\n\\n### Strategy 1: Per-Thread Data (Preferred)\\n\\n**Best approach**: Give each thread its own copy of data. No sharing = no false sharing.\\n\\n**When to use**: \\n- Per-thread counters, statistics, or accumulators\\n- Thread-local state that\'s aggregated later\\n\\n**Benefits**:\\n- No padding overhead\\n- No false sharing (each thread has separate memory)\\n- Cleaner, simpler code\\n\\n**Trade-off**: Must aggregate results when needed (but this is usually infrequent).\\n\\n**Example pattern**:\\n- Each thread maintains its own counter/state\\n- Periodically (or at end), aggregate across all threads\\n- Much cheaper than constant cache line contention\\n\\n### Strategy 2: Padding and Alignment\\n\\n**When per-thread data isn\'t feasible**: Use padding to separate shared data into different cache lines.\\n\\n**Key principles**:\\n- Ensure frequently-written variables start at cache line boundaries\\n- Pad each variable to at least cache line size (64 or 128 bytes)\\n- Use compiler directives to enforce alignment\\n\\n**Benefits**:\\n- Works when data must be shared\\n- Predictable memory layout\\n\\n**Trade-offs**:\\n- Increased memory usage (4-8x)\\n- More complex code\\n- Platform-specific (cache line sizes vary)\\n\\n### Strategy 3: Separate Data Structures\\n\\n**Design approach**: Design data structures so hot fields written by different threads are naturally separated.\\n\\n**Principles**:\\n- Place head/tail pointers in separate cache lines\\n- Separate producer/consumer fields\\n- Group data by access pattern (hot vs. cold)\\n\\n**Benefits**:\\n- Natural separation, less artificial padding\\n- Better overall data structure design\\n\\n### Strategy 4: Reduce Write Frequency\\n\\n**Optimization**: Reduce how often threads write to shared data.\\n\\n**Techniques**:\\n- Batch updates (write every N operations instead of every operation)\\n- Use local accumulators, then periodically update shared state\\n- Prefer read-heavy patterns\\n\\n**Benefits**:\\n- Less false sharing even if data shares cache lines\\n- Better cache efficiency overall\\n\\n**When to use**: When you can\'t avoid sharing but can reduce write frequency.\\n\\n### Strategy 5: Cache Line Size Awareness\\n\\n**Know your platform**:\\n- x86-64: 64 bytes\\n- Some ARM: 128 bytes\\n- Test on target hardware\\n\\n**Implementation**:\\n- Use constants for cache line size\\n- Consider padding to 128 bytes for cross-platform safety\\n- Document assumptions\\n\\n### General Checklist\\n\\n1. **Profile first**: Use `perf c2c` or VTune to identify false sharing\\n2. **Choose strategy**: Prefer per-thread data when possible\\n3. **Measure impact**: Verify improvements after changes\\n4. **Document decisions**: Explain why padding/alignment exists\\n5. **Test on target**: Different platforms have different cache line sizes\\n\\n---\\n\\n## How to Avoid False Sharing in C#\\n\\n### Method 1: ThreadLocal<T> (Best for Per-Thread Data)\\n\\n**Use when**: Each thread needs its own accumulator, counter, or state.\\n\\n```csharp\\nusing System.Threading;\\n\\npublic class RequestCounter {\\n    // Each thread gets its own counter - no sharing!\\n    private readonly ThreadLocal<long> _counter = new ThreadLocal<long>(() => 0);\\n    \\n    public void Increment() {\\n        _counter.Value++;  // Thread-local, no false sharing\\n    }\\n    \\n    public long GetTotal() {\\n        // Aggregate across all threads when needed\\n        long total = 0;\\n        // Note: ThreadLocal doesn\'t provide easy enumeration\\n        // You might need to track threads manually or use a different approach\\n        return total;\\n    }\\n}\\n\\n// Better: Use ThreadLocal with explicit thread tracking\\npublic class ThreadSafeCounter {\\n    private readonly ThreadLocal<long> _counter = new ThreadLocal<long>(() => 0);\\n    private readonly ConcurrentDictionary<int, long> _threadCounters = new();\\n    \\n    public void Increment() {\\n        _counter.Value++;\\n        _threadCounters[Thread.CurrentThread.ManagedThreadId] = _counter.Value;\\n    }\\n    \\n    public long GetTotal() {\\n        return _threadCounters.Values.Sum();\\n    }\\n}\\n```\\n\\n**Why it works**: Each thread accesses completely separate memory locations. No cache line sharing possible.\\n\\n**When to use**: Counters, statistics, accumulators that need per-thread isolation.\\n\\n### Method 2: StructLayout with Padding (For Shared Data)\\n\\n**Use when**: Data must be shared but needs cache line separation.\\n\\n```csharp\\nusing System.Runtime.InteropServices;\\n\\n// Option 1: Explicit size with padding\\n[StructLayout(LayoutKind.Explicit, Size = 128)]  // Pad to 128 bytes (safe for 64 and 128-byte cache lines)\\npublic struct PaddedCounter {\\n    [FieldOffset(0)]\\n    public long Value;\\n    // Rest is automatic padding to 128 bytes\\n}\\n\\npublic class ThreadSafeCounters {\\n    private readonly PaddedCounter[] _counters;\\n    \\n    public ThreadSafeCounters(int threadCount) {\\n        _counters = new PaddedCounter[threadCount];\\n    }\\n    \\n    public void Increment(int threadId) {\\n        Interlocked.Increment(ref _counters[threadId].Value);\\n    }\\n}\\n\\n// Option 2: Manual padding fields\\npublic class ManualPaddedCounter {\\n    private long _counter;\\n    \\n    // Pad to ensure next instance starts at new cache line\\n    // Cache line is 64 bytes, long is 8 bytes\\n    // Need 7 more longs (56 bytes) to reach 64 bytes total\\n    private long _padding1, _padding2, _padding3, _padding4,\\n                 _padding5, _padding6, _padding7;\\n    \\n    public long Value {\\n        get => _counter;\\n        set => _counter = value;\\n    }\\n}\\n```\\n\\n**Why it works**: Forces each `PaddedCounter` to occupy a full cache line (128 bytes), ensuring separate cache lines.\\n\\n**When to use**: Arrays of per-thread data that must be indexed by thread ID.\\n\\n### Method 3: Separate Cache Lines for Lock-Free Structures\\n\\n**Use when**: Building lock-free queues, stacks, or other concurrent structures.\\n\\n```csharp\\nusing System.Runtime.InteropServices;\\n\\n[StructLayout(LayoutKind.Explicit, Size = 128)]\\npublic class LockFreeQueue<T> where T : class {\\n    private readonly T[] _buffer;\\n    private readonly int _capacity;\\n    \\n    [FieldOffset(0)]\\n    private volatile int _head;  // Consumer writes here - first cache line\\n    \\n    // Padding to next cache line (64 bytes)\\n    [FieldOffset(64)]\\n    private volatile int _tail;  // Producer writes here - second cache line\\n    \\n    public LockFreeQueue(int capacity) {\\n        _capacity = capacity;\\n        _buffer = new T[capacity];\\n        _head = 0;\\n        _tail = 0;\\n    }\\n    \\n    public bool TryEnqueue(T item) {\\n        int currentTail = _tail;\\n        int nextTail = (currentTail + 1) % _capacity;\\n        \\n        if (nextTail == _head) {\\n            return false; // Queue full\\n        }\\n        \\n        _buffer[currentTail] = item;\\n        _tail = nextTail;  // Producer writes to separate cache line\\n        return true;\\n    }\\n    \\n    public bool TryDequeue(out T item) {\\n        int currentHead = _head;\\n        \\n        if (currentHead == _tail) {\\n            item = default(T);\\n            return false; // Queue empty\\n        }\\n        \\n        item = _buffer[currentHead];\\n        _buffer[currentHead] = null;\\n        _head = (currentHead + 1) % _capacity;  // Consumer writes to separate cache line\\n        return true;\\n    }\\n}\\n```\\n\\n**Why it works**: Producer (`_tail`) and consumer (`_head`) write to different cache lines, eliminating contention.\\n\\n### Method 4: Separate Arrays for Different Threads\\n\\n**Use when**: You need indexed access but can separate by thread.\\n\\n```csharp\\n//  Bad: All counters in one array - false sharing\\npublic class BadCounters {\\n    private readonly long[] _counters = new long[Environment.ProcessorCount];\\n    \\n    public void Increment(int threadId) {\\n        _counters[threadId]++;  // False sharing if elements share cache lines\\n    }\\n}\\n\\n//  Good: Use padded structures\\npublic class GoodCounters {\\n    private readonly PaddedCounter[] _counters;\\n    \\n    public GoodCounters() {\\n        int threadCount = Environment.ProcessorCount;\\n        _counters = new PaddedCounter[threadCount];\\n    }\\n    \\n    public void Increment(int threadId) {\\n        Interlocked.Increment(ref _counters[threadId].Value);\\n    }\\n}\\n\\n// Using the PaddedCounter struct from Method 2\\n[StructLayout(LayoutKind.Explicit, Size = 128)]\\npublic struct PaddedCounter {\\n    [FieldOffset(0)]\\n    public long Value;\\n}\\n```\\n\\n### Method 5: Reduce Write Frequency (Batching)\\n\\n**Use when**: You can\'t avoid sharing but can reduce write frequency.\\n\\n```csharp\\npublic class BatchedCounter {\\n    // Thread-local accumulator\\n    private readonly ThreadLocal<long> _localCounter = new ThreadLocal<long>(() => 0);\\n    \\n    // Shared counter, updated less frequently\\n    private long _sharedCounter;\\n    private readonly object _lock = new object();\\n    private const int BATCH_SIZE = 1000;\\n    \\n    public void Increment() {\\n        _localCounter.Value++;\\n        \\n        // Only update shared counter every BATCH_SIZE increments\\n        if (_localCounter.Value % BATCH_SIZE == 0) {\\n            lock (_lock) {\\n                _sharedCounter += BATCH_SIZE;\\n            }\\n        }\\n    }\\n    \\n    public long GetTotal() {\\n        long total = _sharedCounter;\\n        // Add any remaining in thread-local counters\\n        // (simplified - in practice, you\'d need to track all threads)\\n        return total;\\n    }\\n}\\n```\\n\\n**Why it works**: Reduces writes to shared data by 1000x (from every increment to every 1000 increments).\\n\\n### Method 6: Using Memory-Mapped or Aligned Allocation (Advanced)\\n\\n**Use when**: You need precise control over memory layout.\\n\\n```csharp\\nusing System.Runtime.InteropServices;\\n\\n// Note: This requires unsafe code and platform-specific implementation\\npublic unsafe class AlignedCounter {\\n    private long* _counter;\\n    \\n    public AlignedCounter() {\\n        // Allocate aligned to cache line boundary (64 bytes)\\n        // This is platform-specific and requires P/Invoke or native allocation\\n        _counter = (long*)AlignedAlloc(64, sizeof(long));\\n    }\\n    \\n    private void* AlignedAlloc(ulong alignment, ulong size) {\\n        // Platform-specific implementation needed:\\n        // - Windows: _aligned_malloc\\n        // - Linux: posix_memalign or aligned_alloc\\n        // - Use DllImport or NativeMemory.AlignedAlloc (modern .NET)\\n        throw new NotImplementedException(\\\"Platform-specific implementation\\\");\\n    }\\n    \\n    // Modern .NET alternative (if available)\\n    public void ModernApproach() {\\n        // .NET 6+ has NativeMemory.AlignedAlloc\\n        // IntPtr ptr = NativeMemory.AlignedAlloc((nuint)sizeof(long), 64);\\n    }\\n}\\n```\\n\\n**Why it works**: Ensures memory starts exactly at a cache line boundary.\\n\\n**When to use**: When you need guaranteed alignment and can\'t use `StructLayout`.\\n\\n### C# Best Practices Summary\\n\\n1. **Prefer ThreadLocal<T>**: Simplest and most effective for per-thread data\\n2. **Use StructLayout for arrays**: When you need indexed access to per-thread data\\n3. **Separate producer/consumer fields**: For lock-free structures, ensure 64+ bytes separation\\n4. **Batch updates**: Reduce write frequency when you can\'t avoid sharing\\n5. **Use constants**: Define `CACHE_LINE_SIZE = 64` or `128` as a constant\\n6. **Verify with tools**: Use profiling tools to confirm false sharing is fixed\\n7. **Document**: Add comments explaining why padding exists\\n\\n### Common C# Pitfalls\\n\\n**Pitfall 1: Assuming array elements are separate**\\n```csharp\\n//  Bad: Array of longs - elements might share cache lines\\nlong[] counters = new long[8];  // 8 * 8 = 64 bytes - all in one cache line!\\n\\n//  Good: Use padded structures\\nPaddedCounter[] counters = new PaddedCounter[8];  // Each is 128 bytes - separate cache lines\\n```\\n\\n**Pitfall 2: Not using StructLayout**\\n```csharp\\n//  Bad: Compiler might reorder fields\\npublic struct Counter {\\n    public long Value;\\n    public long Padding1, Padding2, ...;  // Might not work!\\n}\\n\\n//  Good: Explicit layout\\n[StructLayout(LayoutKind.Explicit, Size = 128)]\\npublic struct Counter {\\n    [FieldOffset(0)]\\n    public long Value;\\n}\\n```\\n\\n**Pitfall 3: Forgetting about object headers**\\n```csharp\\n// In C#, objects have headers (overhead)\\n// PaddedCounter struct is fine, but arrays of objects might have additional overhead\\n// Prefer structs over classes for per-thread data\\n```\\n\\n### Performance Impact in C#\\n\\nTypical improvements when fixing false sharing in C#:\\n- **Per-thread counters**: 30-50% throughput improvement\\n- **Lock-free queues**: 40-60% latency reduction\\n- **Thread pool statistics**: 25-40% overhead reduction\\n- **Scalability**: Can often restore linear scaling up to 16-32 threads\\n\\n---\"}],\"direction\":null,\"format\":\"\",\"indent\":0,\"type\":\"root\",\"version\":1}}','<h2 id=\"executive-summary-tldr\">Executive Summary (TL;DR)</h2>\n<p>False sharing happens when <strong>multiple threads write to different variables that live inside the same CPU cache line</strong> (usually 64 bytes).</p>\n<p>Even though the variables are logically independent, the CPU cache works with <strong>cache lines</strong>, not variables.<br>\nWhen one thread writes, other cores must invalidate their copy of the entire cache line.<br>\nThe cache line then <strong>bounces between cores</strong>, creating hidden serialization.</p>\n<p>This causes:</p>\n<ul>\n<li>Poor scalability</li>\n<li>High CPU usage with low throughput</li>\n<li>Performance degradation (1050% or worse)</li>\n</ul>\n<p>False sharing <strong>does NOT break correctness</strong>, only performance  which makes it hard to notice.</p>\n<p><strong>Solution:</strong> ensure that data frequently written by different threads lives in <strong>different cache lines</strong>, using:</p>\n<ul>\n<li>Per-thread data (preferred)</li>\n<li>Padding and alignment (when necessary)</li>\n</ul>\n<p>Only apply after <strong>profiling confirms cache contention</strong>.</p>\n<hr>\n<h2 id=\"1-what-is-a-cache-line-from-zero\">1. What Is a Cache Line? (From Zero)</h2>\n<p>The CPU does <strong>not</strong> load individual variables into cache.</p>\n<p>Instead, it loads memory in <strong>fixed-size blocks</strong> called <strong>cache lines</strong>.</p>\n<ul>\n<li>Typical size: <strong>64 bytes</strong> (x86-64)</li>\n<li>Some ARM CPUs: <strong>128 bytes</strong></li>\n<li>Minimum unit moved between:\n<ul>\n<li>RAM  cache</li>\n<li>Core  core</li>\n</ul>\n</li>\n</ul>\n<blockquote>\n<p>You cannot partially load or partially invalidate a cache line.<br>\nIt is always <strong>all or nothing</strong>.</p>\n</blockquote>\n<hr>\n<h2 id=\"2-simple-analogy-very-important\">2. Simple Analogy (Very Important)</h2>\n<p>Think of memory like this:</p>\n<ul>\n<li><strong>RAM</strong> = a large filing cabinet</li>\n<li><strong>CPU cache</strong> = your desk</li>\n<li><strong>Cache line</strong> = a folder</li>\n</ul>\n<p>Even if you only need <strong>one page</strong>, you must bring <strong>the entire folder</strong> to your desk.</p>\n<p>The CPU works exactly the same way.</p>\n<hr>\n<h2 id=\"3-how-variables-end-up-sharing-cache-lines\">3. How Variables End Up Sharing Cache Lines</h2>\n<pre><code class=\"language-csharp\">class Counters {\n    public long A; // 8 bytes\n    public long B; // 8 bytes\n}\n</code></pre>\n<p><strong>Memory layout (simplified):</strong></p>\n<pre><code>| A (8B) | B (8B) | other data ... |\n|&lt;---------- 64 bytes (one cache line) ----------&gt;|\n</code></pre>\n<p>Even though A and B are:</p>\n<ul>\n<li>Different variables</li>\n<li>Used by different threads</li>\n</ul>\n<p>They live in the same cache line.</p>\n<hr>\n<h2 id=\"4-the-golden-rule-explains-everything\">4. The Golden Rule (Explains Everything)</h2>\n<p><strong>If two threads write to anything inside the same cache line, they compete.</strong></p>\n<p>It does <strong>NOT</strong> matter:</p>\n<ul>\n<li>That variables are different</li>\n<li>That there are no locks</li>\n<li>That the code is correct</li>\n</ul>\n<p>This is false sharing.</p>\n<hr>\n<h2 id=\"5-why-the-cpu-forces-this-cache-coherency\">5. Why the CPU Forces This (Cache Coherency)</h2>\n<p>Each CPU core has its own cache.</p>\n<p>The CPU must guarantee:</p>\n<blockquote>\n<p>\"All cores see a consistent view of memory.\"</p>\n</blockquote>\n<p>To do this, it uses cache coherency protocols (e.g., MESI).</p>\n<p><strong>Core rule:</strong></p>\n<ul>\n<li>Only one core can modify a cache line at a time</li>\n<li>Other cores must invalidate their copy before writing</li>\n</ul>\n<p>Correct behavior  but expensive.</p>\n<hr>\n<h2 id=\"6-false-sharing-explained-as-a-timeline-cronograma\">6. False Sharing Explained as a Timeline (Cronograma)</h2>\n<h3 id=\"scenario-setup\">Scenario Setup</h3>\n<ul>\n<li><strong>Core 0</strong>  Thread 0  writes to variable <code>A</code></li>\n<li><strong>Core 1</strong>  Thread 1  writes to variable <code>B</code></li>\n<li><code>A</code> and <code>B</code> share the same cache line</li>\n</ul>\n<h3 id=\"why-different-cores-have-the-same-cache-line\">Why Different Cores Have the Same Cache Line</h3>\n<p>When Thread 0 on Core 0 first reads <code>A</code>, the CPU loads the entire 64-byte cache line containing <code>A</code> (and <code>B</code>) into Core 0\'s L1 cache.</p>\n<p>Later, when Thread 1 on Core 1 reads <code>B</code>, the CPU loads the same cache line (now containing both <code>A</code> and <code>B</code>) into Core 1\'s L1 cache.</p>\n<p><strong>Both cores now have identical copies of the same cache line in their local caches.</strong></p>\n<h3 id=\"detailed-timeline\">Detailed Timeline</h3>\n<pre><code>Time    Core 0 (Thread 0)              Core 1 (Thread 1)              Cache Line State\n\nT0      Reads A                        -                               Core 0: Exclusive (E)\n                                                                        Cache line loaded from RAM\n                                                                        Cost: ~100-300 cycles\n\nT1      -                              Reads B                         Core 0: Shared (S)\n                                                                        Core 1: Shared (S)\n                                                                        Both have read-only copies\n                                                                        Cost: ~40-100 cycles (transfer from Core 0)\n\nT2      Writes to A                    -                               Core 0: Modified (M)\n                                                                        Core 1: Invalid (I)  INVALIDATION!\n                                                                        \n                                                                        Core 0 sends \"invalidate\" message to Core 1\n                                                                        Cost: ~10-20 cycles (inter-core communication)\n\nT3      (continues working)            Wants to write B                Core 1 detects cache line is Invalid\n                                                                        Core 1 must request cache line from Core 0\n                                                                        \n                                                                        Request sent to Core 0: \"I need this cache line\"\n                                                                        Cost: ~10-20 cycles (request)\n\nT4      Receives request               (waiting...)                    Core 0 must write back to memory (if Modified)\n                                                                        Core 0 sends cache line to Core 1\n                                                                        Cost: ~40-100 cycles (write-back + transfer)\n\nT5      -                              Receives cache line             Core 0: Shared (S)\n                                                                        Core 1: Modified (M)\n                                                                        Now Core 1 has exclusive ownership\n                                                                        Cost: ~10 cycles (state update)\n\nT6      (wants A again)                Writes to B                     Core 1: Modified (M)\n                                                                        Core 0: Invalid (I)  INVALIDATION AGAIN!\n                                                                        \n                                                                        Process repeats...\n                                                                        Cost: ~40-100 cycles per cycle\n</code></pre>\n<h3 id=\"who-controls-the-cache-line-updates\">Who Controls the Cache Line Updates?</h3>\n<p><strong>The CPU\'s cache coherency protocol (MESI) controls everything automatically:</strong></p>\n<ol>\n<li><strong>Hardware-level</strong>: No software involvement requiredit happens in CPU hardware</li>\n<li><strong>Cache controller</strong>: Each core has a cache controller that manages MESI states</li>\n<li><strong>Interconnect</strong>: Cores communicate through the CPU interconnect (bus or mesh)</li>\n<li><strong>Snooping</strong>: Cores \"snoop\" on each other\'s cache transactions to maintain coherency</li>\n</ol>\n<h3 id=\"cost-breakdown\">Cost Breakdown</h3>\n<p><strong>Per false sharing cycle:</strong></p>\n<ul>\n<li>Invalidation message: ~10-20 cycles</li>\n<li>Cache line request: ~10-20 cycles</li>\n<li>Write-back to memory (if needed): ~10-30 cycles</li>\n<li>Cache line transfer: ~40-100 cycles</li>\n<li>State updates: ~5-10 cycles</li>\n</ul>\n<p><strong>Total per cycle: ~75-180 CPU cycles</strong></p>\n<p>If Thread 0 writes to <code>A</code> 1 million times per second, and Thread 1 writes to <code>B</code> 1 million times per second:</p>\n<ul>\n<li><strong>Potential cache line transfers: 2 million per second</strong></li>\n<li><strong>Wasted cycles: ~150-360 million cycles per second</strong></li>\n<li><strong>On a 3GHz CPU: 5-12% of total CPU time wasted on false sharing overhead</strong></li>\n</ul>\n<h3 id=\"why-this-creates-serialization\">Why This Creates Serialization</h3>\n<p>Even though Thread 0 and Thread 1 are running on different cores (true parallelism), they <strong>cannot write simultaneously</strong> because:</p>\n<ol>\n<li>Only one core can have the cache line in Modified state</li>\n<li>The other core must wait for the transfer to complete</li>\n<li>This creates <strong>implicit serialization</strong> at the hardware level</li>\n</ol>\n<p><strong>Result</strong>: What looks like parallel execution is actually serial execution with expensive synchronization.</p>\n<h3 id=\"visual-representation\">Visual Representation</h3>\n<pre><code>Normal Parallel Execution (no false sharing):\n\nCore 0: [Write A][Write A][Write A][Write A]...   Continuous\nCore 1: [Write B][Write B][Write B][Write B]...   Continuous\n         Both working simultaneously\n\nWith False Sharing:\n\nCore 0: [Write A][----WAIT----][Write A][----WAIT----]...\nCore 1: [----WAIT----][Write B][----WAIT----][Write B]...\n         Taking turns (serialized!)\n         Wasted cycles during WAIT\n</code></pre>\n<p>This hidden serialization is why performance degrades even though your code looks perfectly parallel.</p>\n<hr>\n<h3 id=\"common-misconceptions\">Common Misconceptions</h3>\n<p><strong>\"Separate variables mean separate memory locations\"</strong></p>\n<ul>\n<li>The CPU caches data in 64-byte chunks called cache lines. Two variables declared separately can end up on the same cache line if they\'re close in memory. Think of it like apartment buildings: even though you live in apartment 101 and your neighbor in 102, you share the same building (cache line).</li>\n</ul>\n<p><strong>\"Lock-free code is automatically fast\"</strong></p>\n<ul>\n<li>Lock-free data structures avoid blocking but can suffer from false sharing when multiple threads update adjacent fields. Atomic operations still trigger cache invalidations.</li>\n</ul>\n<p><strong>\"High CPU usage means good parallelization\"</strong></p>\n<ul>\n<li>False sharing can cause high CPU usage while destroying actual parallelism. CPUs spend time waiting for cache lines to transfer between cores, not doing useful work.</li>\n</ul>\n<p><strong>\"The compiler/runtime will optimize this away\"</strong></p>\n<ul>\n<li>Compilers don\'t automatically pad structures to prevent false sharing. They optimize for single-threaded performance, not multi-threaded cache behavior.</li>\n</ul>\n<h3 id=\"why-naive-solutions-fail\">Why Naive Solutions Fail</h3>\n<p><strong>Adding locks</strong>: Makes it worse by serializing execution completely. The problem is cache contention, not lack of synchronization.</p>\n<p><strong>Increasing thread count</strong>: More threads mean more cache invalidations (O(n) growth). The problem compounds.</p>\n<p><strong>Logical separation</strong>: Different variables can still share cache lines if they\'re physically close in memory. CPU sees physical layout, not code structure.</p>\n<p><strong>Algorithm optimization alone</strong>: Doesn\'t help if threads are fighting over cache lines at the hardware level.</p>\n<h2 id=\"why-this-becomes-a-bottleneck\">Why This Becomes a Bottleneck</h2>\n<p><strong>Cache Line Ping-Pong</strong>: Cache lines constantly bounce between cores. Each transfer consumes bandwidth and creates latency. Impact: 10-50% performance degradation.</p>\n<p><strong>Serialization Despite Parallelism</strong>: Threads appear to run in parallel but wait for each other at the cache level. Impact: Applications that should scale linearly instead plateau or degrade.</p>\n<p><strong>Memory Bus Saturation</strong>: Cache line transfers consume bandwidth, competing with actual data access. Impact: System-wide performance degradation.</p>\n<p><strong>NUMA Amplification</strong>: In NUMA systems, remote cache transfers are 2-3x slower. Impact: 50-100% additional latency.</p>\n<p><strong>Scalability Collapse</strong>: Performance degrades with more threads instead of improving. Impact: Applications fail to utilize available CPU cores.</p>\n<h2 id=\"when-to-use-this-approach\">When to Use This Approach</h2>\n<p><strong>High-performance multi-threaded applications</strong>: Many threads, high throughput requirements, processing large volumes of data.</p>\n<p><strong>Frequently-updated shared state</strong>: Per-thread counters, statistics structures, lock-free data structures with adjacent fields.</p>\n<p><strong>Profiling indicates cache contention</strong>: High cache miss rates, poor scalability, cache line transfers detected by tools.</p>\n<p><strong>High-thread-count systems</strong>: 8+ cores where false sharing effects are amplified. Especially important in NUMA systems.</p>\n<p><strong>Latency-sensitive parallel workloads</strong>: Real-time systems, financial trading, game engines, media processing.</p>\n<p><strong>Lock-free algorithms</strong>: Lock-free queues, stacks, hash tables. These are particularly susceptible to false sharing.</p>\n<h2 id=\"when-not-to-use-it\">When Not to Use It</h2>\n<p><strong>Single-threaded applications</strong>: No parallelism means no false sharing.</p>\n<p><strong>Read-only shared data</strong>: False sharing only occurs with writes. Multiple threads reading is fine (Shared state).</p>\n<p><strong>Infrequently accessed data</strong>: Padding overhead isn\'t justified by occasional access.</p>\n<p><strong>Small data structures</strong>: Structures that naturally span multiple cache lines might not need explicit padding.</p>\n<p><strong>Cloud/containerized environments</strong>: Hardware topology is abstracted, cache line sizes may vary.</p>\n<p><strong>Development/prototyping</strong>: Premature optimization distracts from correctness. Profile first.</p>\n<p><strong>Memory-constrained systems</strong>: Embedded systems, mobile devices. Padding overhead might be unacceptable.</p>\n<p><strong>When profiling shows no issue</strong>: Don\'t optimize what isn\'t broken. Use tools to confirm before adding padding.</p>\n<hr>\n<h2 id=\"how-to-avoid-false-sharing-general-principles\">How to Avoid False Sharing (General Principles)</h2>\n<h3 id=\"strategy-1-per-thread-data-preferred\">Strategy 1: Per-Thread Data (Preferred)</h3>\n<p><strong>Best approach</strong>: Give each thread its own copy of data. No sharing = no false sharing.</p>\n<p><strong>When to use</strong>:</p>\n<ul>\n<li>Per-thread counters, statistics, or accumulators</li>\n<li>Thread-local state that\'s aggregated later</li>\n</ul>\n<p><strong>Benefits</strong>:</p>\n<ul>\n<li>No padding overhead</li>\n<li>No false sharing (each thread has separate memory)</li>\n<li>Cleaner, simpler code</li>\n</ul>\n<p><strong>Trade-off</strong>: Must aggregate results when needed (but this is usually infrequent).</p>\n<p><strong>Example pattern</strong>:</p>\n<ul>\n<li>Each thread maintains its own counter/state</li>\n<li>Periodically (or at end), aggregate across all threads</li>\n<li>Much cheaper than constant cache line contention</li>\n</ul>\n<h3 id=\"strategy-2-padding-and-alignment\">Strategy 2: Padding and Alignment</h3>\n<p><strong>When per-thread data isn\'t feasible</strong>: Use padding to separate shared data into different cache lines.</p>\n<p><strong>Key principles</strong>:</p>\n<ul>\n<li>Ensure frequently-written variables start at cache line boundaries</li>\n<li>Pad each variable to at least cache line size (64 or 128 bytes)</li>\n<li>Use compiler directives to enforce alignment</li>\n</ul>\n<p><strong>Benefits</strong>:</p>\n<ul>\n<li>Works when data must be shared</li>\n<li>Predictable memory layout</li>\n</ul>\n<p><strong>Trade-offs</strong>:</p>\n<ul>\n<li>Increased memory usage (4-8x)</li>\n<li>More complex code</li>\n<li>Platform-specific (cache line sizes vary)</li>\n</ul>\n<h3 id=\"strategy-3-separate-data-structures\">Strategy 3: Separate Data Structures</h3>\n<p><strong>Design approach</strong>: Design data structures so hot fields written by different threads are naturally separated.</p>\n<p><strong>Principles</strong>:</p>\n<ul>\n<li>Place head/tail pointers in separate cache lines</li>\n<li>Separate producer/consumer fields</li>\n<li>Group data by access pattern (hot vs. cold)</li>\n</ul>\n<p><strong>Benefits</strong>:</p>\n<ul>\n<li>Natural separation, less artificial padding</li>\n<li>Better overall data structure design</li>\n</ul>\n<h3 id=\"strategy-4-reduce-write-frequency\">Strategy 4: Reduce Write Frequency</h3>\n<p><strong>Optimization</strong>: Reduce how often threads write to shared data.</p>\n<p><strong>Techniques</strong>:</p>\n<ul>\n<li>Batch updates (write every N operations instead of every operation)</li>\n<li>Use local accumulators, then periodically update shared state</li>\n<li>Prefer read-heavy patterns</li>\n</ul>\n<p><strong>Benefits</strong>:</p>\n<ul>\n<li>Less false sharing even if data shares cache lines</li>\n<li>Better cache efficiency overall</li>\n</ul>\n<p><strong>When to use</strong>: When you can\'t avoid sharing but can reduce write frequency.</p>\n<h3 id=\"strategy-5-cache-line-size-awareness\">Strategy 5: Cache Line Size Awareness</h3>\n<p><strong>Know your platform</strong>:</p>\n<ul>\n<li>x86-64: 64 bytes</li>\n<li>Some ARM: 128 bytes</li>\n<li>Test on target hardware</li>\n</ul>\n<p><strong>Implementation</strong>:</p>\n<ul>\n<li>Use constants for cache line size</li>\n<li>Consider padding to 128 bytes for cross-platform safety</li>\n<li>Document assumptions</li>\n</ul>\n<h3 id=\"general-checklist\">General Checklist</h3>\n<ol>\n<li><strong>Profile first</strong>: Use <code>perf c2c</code> or VTune to identify false sharing</li>\n<li><strong>Choose strategy</strong>: Prefer per-thread data when possible</li>\n<li><strong>Measure impact</strong>: Verify improvements after changes</li>\n<li><strong>Document decisions</strong>: Explain why padding/alignment exists</li>\n<li><strong>Test on target</strong>: Different platforms have different cache line sizes</li>\n</ol>\n<hr>\n<h2 id=\"how-to-avoid-false-sharing-in-c\">How to Avoid False Sharing in C#</h2>\n<h3 id=\"method-1-threadlocalt-best-for-per-thread-data\">Method 1: ThreadLocal<t> (Best for Per-Thread Data)</t></h3>\n<p><strong>Use when</strong>: Each thread needs its own accumulator, counter, or state.</p>\n<pre><code class=\"language-csharp\">using System.Threading;\n\npublic class RequestCounter {\n    // Each thread gets its own counter - no sharing!\n    private readonly ThreadLocal&lt;long&gt; _counter = new ThreadLocal&lt;long&gt;(() =&gt; 0);\n    \n    public void Increment() {\n        _counter.Value++;  // Thread-local, no false sharing\n    }\n    \n    public long GetTotal() {\n        // Aggregate across all threads when needed\n        long total = 0;\n        // Note: ThreadLocal doesn\'t provide easy enumeration\n        // You might need to track threads manually or use a different approach\n        return total;\n    }\n}\n\n// Better: Use ThreadLocal with explicit thread tracking\npublic class ThreadSafeCounter {\n    private readonly ThreadLocal&lt;long&gt; _counter = new ThreadLocal&lt;long&gt;(() =&gt; 0);\n    private readonly ConcurrentDictionary&lt;int, long&gt; _threadCounters = new();\n    \n    public void Increment() {\n        _counter.Value++;\n        _threadCounters[Thread.CurrentThread.ManagedThreadId] = _counter.Value;\n    }\n    \n    public long GetTotal() {\n        return _threadCounters.Values.Sum();\n    }\n}\n</code></pre>\n<p><strong>Why it works</strong>: Each thread accesses completely separate memory locations. No cache line sharing possible.</p>\n<p><strong>When to use</strong>: Counters, statistics, accumulators that need per-thread isolation.</p>\n<h3 id=\"method-2-structlayout-with-padding-for-shared-data\">Method 2: StructLayout with Padding (For Shared Data)</h3>\n<p><strong>Use when</strong>: Data must be shared but needs cache line separation.</p>\n<pre><code class=\"language-csharp\">using System.Runtime.InteropServices;\n\n// Option 1: Explicit size with padding\n[StructLayout(LayoutKind.Explicit, Size = 128)]  // Pad to 128 bytes (safe for 64 and 128-byte cache lines)\npublic struct PaddedCounter {\n    [FieldOffset(0)]\n    public long Value;\n    // Rest is automatic padding to 128 bytes\n}\n\npublic class ThreadSafeCounters {\n    private readonly PaddedCounter[] _counters;\n    \n    public ThreadSafeCounters(int threadCount) {\n        _counters = new PaddedCounter[threadCount];\n    }\n    \n    public void Increment(int threadId) {\n        Interlocked.Increment(ref _counters[threadId].Value);\n    }\n}\n\n// Option 2: Manual padding fields\npublic class ManualPaddedCounter {\n    private long _counter;\n    \n    // Pad to ensure next instance starts at new cache line\n    // Cache line is 64 bytes, long is 8 bytes\n    // Need 7 more longs (56 bytes) to reach 64 bytes total\n    private long _padding1, _padding2, _padding3, _padding4,\n                 _padding5, _padding6, _padding7;\n    \n    public long Value {\n        get =&gt; _counter;\n        set =&gt; _counter = value;\n    }\n}\n</code></pre>\n<p><strong>Why it works</strong>: Forces each <code>PaddedCounter</code> to occupy a full cache line (128 bytes), ensuring separate cache lines.</p>\n<p><strong>When to use</strong>: Arrays of per-thread data that must be indexed by thread ID.</p>\n<h3 id=\"method-3-separate-cache-lines-for-lock-free-structures\">Method 3: Separate Cache Lines for Lock-Free Structures</h3>\n<p><strong>Use when</strong>: Building lock-free queues, stacks, or other concurrent structures.</p>\n<pre><code class=\"language-csharp\">using System.Runtime.InteropServices;\n\n[StructLayout(LayoutKind.Explicit, Size = 128)]\npublic class LockFreeQueue&lt;T&gt; where T : class {\n    private readonly T[] _buffer;\n    private readonly int _capacity;\n    \n    [FieldOffset(0)]\n    private volatile int _head;  // Consumer writes here - first cache line\n    \n    // Padding to next cache line (64 bytes)\n    [FieldOffset(64)]\n    private volatile int _tail;  // Producer writes here - second cache line\n    \n    public LockFreeQueue(int capacity) {\n        _capacity = capacity;\n        _buffer = new T[capacity];\n        _head = 0;\n        _tail = 0;\n    }\n    \n    public bool TryEnqueue(T item) {\n        int currentTail = _tail;\n        int nextTail = (currentTail + 1) % _capacity;\n        \n        if (nextTail == _head) {\n            return false; // Queue full\n        }\n        \n        _buffer[currentTail] = item;\n        _tail = nextTail;  // Producer writes to separate cache line\n        return true;\n    }\n    \n    public bool TryDequeue(out T item) {\n        int currentHead = _head;\n        \n        if (currentHead == _tail) {\n            item = default(T);\n            return false; // Queue empty\n        }\n        \n        item = _buffer[currentHead];\n        _buffer[currentHead] = null;\n        _head = (currentHead + 1) % _capacity;  // Consumer writes to separate cache line\n        return true;\n    }\n}\n</code></pre>\n<p><strong>Why it works</strong>: Producer (<code>_tail</code>) and consumer (<code>_head</code>) write to different cache lines, eliminating contention.</p>\n<h3 id=\"method-4-separate-arrays-for-different-threads\">Method 4: Separate Arrays for Different Threads</h3>\n<p><strong>Use when</strong>: You need indexed access but can separate by thread.</p>\n<pre><code class=\"language-csharp\">//  Bad: All counters in one array - false sharing\npublic class BadCounters {\n    private readonly long[] _counters = new long[Environment.ProcessorCount];\n    \n    public void Increment(int threadId) {\n        _counters[threadId]++;  // False sharing if elements share cache lines\n    }\n}\n\n//  Good: Use padded structures\npublic class GoodCounters {\n    private readonly PaddedCounter[] _counters;\n    \n    public GoodCounters() {\n        int threadCount = Environment.ProcessorCount;\n        _counters = new PaddedCounter[threadCount];\n    }\n    \n    public void Increment(int threadId) {\n        Interlocked.Increment(ref _counters[threadId].Value);\n    }\n}\n\n// Using the PaddedCounter struct from Method 2\n[StructLayout(LayoutKind.Explicit, Size = 128)]\npublic struct PaddedCounter {\n    [FieldOffset(0)]\n    public long Value;\n}\n</code></pre>\n<h3 id=\"method-5-reduce-write-frequency-batching\">Method 5: Reduce Write Frequency (Batching)</h3>\n<p><strong>Use when</strong>: You can\'t avoid sharing but can reduce write frequency.</p>\n<pre><code class=\"language-csharp\">public class BatchedCounter {\n    // Thread-local accumulator\n    private readonly ThreadLocal&lt;long&gt; _localCounter = new ThreadLocal&lt;long&gt;(() =&gt; 0);\n    \n    // Shared counter, updated less frequently\n    private long _sharedCounter;\n    private readonly object _lock = new object();\n    private const int BATCH_SIZE = 1000;\n    \n    public void Increment() {\n        _localCounter.Value++;\n        \n        // Only update shared counter every BATCH_SIZE increments\n        if (_localCounter.Value % BATCH_SIZE == 0) {\n            lock (_lock) {\n                _sharedCounter += BATCH_SIZE;\n            }\n        }\n    }\n    \n    public long GetTotal() {\n        long total = _sharedCounter;\n        // Add any remaining in thread-local counters\n        // (simplified - in practice, you\'d need to track all threads)\n        return total;\n    }\n}\n</code></pre>\n<p><strong>Why it works</strong>: Reduces writes to shared data by 1000x (from every increment to every 1000 increments).</p>\n<h3 id=\"method-6-using-memory-mapped-or-aligned-allocation-advanced\">Method 6: Using Memory-Mapped or Aligned Allocation (Advanced)</h3>\n<p><strong>Use when</strong>: You need precise control over memory layout.</p>\n<pre><code class=\"language-csharp\">using System.Runtime.InteropServices;\n\n// Note: This requires unsafe code and platform-specific implementation\npublic unsafe class AlignedCounter {\n    private long* _counter;\n    \n    public AlignedCounter() {\n        // Allocate aligned to cache line boundary (64 bytes)\n        // This is platform-specific and requires P/Invoke or native allocation\n        _counter = (long*)AlignedAlloc(64, sizeof(long));\n    }\n    \n    private void* AlignedAlloc(ulong alignment, ulong size) {\n        // Platform-specific implementation needed:\n        // - Windows: _aligned_malloc\n        // - Linux: posix_memalign or aligned_alloc\n        // - Use DllImport or NativeMemory.AlignedAlloc (modern .NET)\n        throw new NotImplementedException(\"Platform-specific implementation\");\n    }\n    \n    // Modern .NET alternative (if available)\n    public void ModernApproach() {\n        // .NET 6+ has NativeMemory.AlignedAlloc\n        // IntPtr ptr = NativeMemory.AlignedAlloc((nuint)sizeof(long), 64);\n    }\n}\n</code></pre>\n<p><strong>Why it works</strong>: Ensures memory starts exactly at a cache line boundary.</p>\n<p><strong>When to use</strong>: When you need guaranteed alignment and can\'t use <code>StructLayout</code>.</p>\n<h3 id=\"c-best-practices-summary\">C# Best Practices Summary</h3>\n<ol>\n<li><strong>Prefer ThreadLocal<t></t></strong>: Simplest and most effective for per-thread data</li>\n<li><strong>Use StructLayout for arrays</strong>: When you need indexed access to per-thread data</li>\n<li><strong>Separate producer/consumer fields</strong>: For lock-free structures, ensure 64+ bytes separation</li>\n<li><strong>Batch updates</strong>: Reduce write frequency when you can\'t avoid sharing</li>\n<li><strong>Use constants</strong>: Define <code>CACHE_LINE_SIZE = 64</code> or <code>128</code> as a constant</li>\n<li><strong>Verify with tools</strong>: Use profiling tools to confirm false sharing is fixed</li>\n<li><strong>Document</strong>: Add comments explaining why padding exists</li>\n</ol>\n<h3 id=\"common-c-pitfalls\">Common C# Pitfalls</h3>\n<p><strong>Pitfall 1: Assuming array elements are separate</strong></p>\n<pre><code class=\"language-csharp\">//  Bad: Array of longs - elements might share cache lines\nlong[] counters = new long[8];  // 8 * 8 = 64 bytes - all in one cache line!\n\n//  Good: Use padded structures\nPaddedCounter[] counters = new PaddedCounter[8];  // Each is 128 bytes - separate cache lines\n</code></pre>\n<p><strong>Pitfall 2: Not using StructLayout</strong></p>\n<pre><code class=\"language-csharp\">//  Bad: Compiler might reorder fields\npublic struct Counter {\n    public long Value;\n    public long Padding1, Padding2, ...;  // Might not work!\n}\n\n//  Good: Explicit layout\n[StructLayout(LayoutKind.Explicit, Size = 128)]\npublic struct Counter {\n    [FieldOffset(0)]\n    public long Value;\n}\n</code></pre>\n<p><strong>Pitfall 3: Forgetting about object headers</strong></p>\n<pre><code class=\"language-csharp\">// In C#, objects have headers (overhead)\n// PaddedCounter struct is fine, but arrays of objects might have additional overhead\n// Prefer structs over classes for per-thread data\n</code></pre>\n<h3 id=\"performance-impact-in-c\">Performance Impact in C#</h3>\n<p>Typical improvements when fixing false sharing in C#:</p>\n<ul>\n<li><strong>Per-thread counters</strong>: 30-50% throughput improvement</li>\n<li><strong>Lock-free queues</strong>: 40-60% latency reduction</li>\n<li><strong>Thread pool statistics</strong>: 25-40% overhead reduction</li>\n<li><strong>Scalability</strong>: Can often restore linear scaling up to 16-32 threads</li>\n</ul>\n<hr>\n','69532d9aebcfaae683eec996','Executive Summary (TL;DR)\n\n\nFalse sharing happens when multiple threads write to different variables that live inside the same CPU cache line (usually 64 bytes).\n\n\nEven though the variables are logically independent, the CPU cache works with cache lines, not variables.\n\nWhen one thread writes, other cores must invalidate their copy of the entire cache line.\n\nThe cache line then bounces between cores, creating hidden serialization.\n\n\nThis causes:\n\n\n * Poor scalability\n * High CPU usage with low throughput\n * Performance degradation (1050% or worse)\n\n\nFalse sharing does NOT break correctness, only performance  which makes it hard to notice.\n\n\nSolution: ensure that data frequently written by different threads lives in different cache lines, using:\n\n\n * Per-thread data (preferred)\n * Padding and alignment (when necessary)\n\n\nOnly apply after profiling confirms cache contention.\n\n\n\n\n1. What Is a Cache Line? (From Zero)\n\n\nThe CPU does not load individual variables into cache.\n\n\nInstead, it loads memory in fixed-size blocks called cache lines.\n\n\n * Typical size: 64 bytes (x86-64)\n * Some ARM CPUs: 128 bytes\n * Minimum unit moved between:\n   \n   * RAM  cache\n   * Core  core\n   \n\n\n\n\n\nYou cannot partially load or partially invalidate a cache line.\n\nIt is always all or nothing.\n\n\n\n\n\n2. Simple Analogy (Very Important)\n\n\nThink of memory like this:\n\n\n * RAM = a large filing cabinet\n * CPU cache = your desk\n * Cache line = a folder\n\n\nEven if you only need one page, you must bring the entire folder to your desk.\n\n\nThe CPU works exactly the same way.\n\n\n\n\n3. How Variables End Up Sharing Cache Lines\n\n\nclass Counters {\n    public long A; // 8 bytes\n    public long B; // 8 bytes\n}\n\n\n\nMemory layout (simplified):\n\n\n| A (8B) | B (8B) | other data ... |\n|<---------- 64 bytes (one cache line) ---------->|\n\n\n\nEven though A and B are:\n\n\n * Different variables\n * Used by different threads\n\n\nThey live in the same cache line.\n\n\n\n\n4. The Golden Rule (Explains Everything)\n\n\nIf two threads write to anything inside the same cache line, they compete.\n\n\nIt does NOT matter:\n\n\n * That variables are different\n * That there are no locks\n * That the code is correct\n\n\nThis is false sharing.\n\n\n\n\n5. Why the CPU Forces This (Cache Coherency)\n\n\nEach CPU core has its own cache.\n\n\nThe CPU must guarantee:\n\n\n\n\n\n\"All cores see a consistent view of memory.\"\n\n\n\nTo do this, it uses cache coherency protocols (e.g., MESI).\n\n\nCore rule:\n\n\n * Only one core can modify a cache line at a time\n * Other cores must invalidate their copy before writing\n\n\nCorrect behavior  but expensive.\n\n\n\n\n6. False Sharing Explained as a Timeline (Cronograma)\n\n\n\nScenario Setup\n\n\n * Core 0  Thread 0  writes to variable A\n * Core 1  Thread 1  writes to variable B\n * A and B share the same cache line\n\n\n\nWhy Different Cores Have the Same Cache Line\n\n\nWhen Thread 0 on Core 0 first reads A, the CPU loads the entire 64-byte cache line containing A (and B) into Core 0\'s L1 cache.\n\n\nLater, when Thread 1 on Core 1 reads B, the CPU loads the same cache line (now containing both A and B) into Core 1\'s L1 cache.\n\n\nBoth cores now have identical copies of the same cache line in their local caches.\n\n\n\nDetailed Timeline\n\n\nTime    Core 0 (Thread 0)              Core 1 (Thread 1)              Cache Line State\n\nT0      Reads A                        -                               Core 0: Exclusive (E)\n                                                                        Cache line loaded from RAM\n                                                                        Cost: ~100-300 cycles\n\nT1      -                              Reads B                         Core 0: Shared (S)\n                                                                        Core 1: Shared (S)\n                                                                        Both have read-only copies\n                                                                        Cost: ~40-100 cycles (transfer from Core 0)\n\nT2      Writes to A                    -                               Core 0: Modified (M)\n                                                                        Core 1: Invalid (I)  INVALIDATION!\n                                                                        \n                                                                        Core 0 sends \"invalidate\" message to Core 1\n                                                                        Cost: ~10-20 cycles (inter-core communication)\n\nT3      (continues working)            Wants to write B                Core 1 detects cache line is Invalid\n                                                                        Core 1 must request cache line from Core 0\n                                                                        \n                                                                        Request sent to Core 0: \"I need this cache line\"\n                                                                        Cost: ~10-20 cycles (request)\n\nT4      Receives request               (waiting...)                    Core 0 must write back to memory (if Modified)\n                                                                        Core 0 sends cache line to Core 1\n                                                                        Cost: ~40-100 cycles (write-back + transfer)\n\nT5      -                              Receives cache line             Core 0: Shared (S)\n                                                                        Core 1: Modified (M)\n                                                                        Now Core 1 has exclusive ownership\n                                                                        Cost: ~10 cycles (state update)\n\nT6      (wants A again)                Writes to B                     Core 1: Modified (M)\n                                                                        Core 0: Invalid (I)  INVALIDATION AGAIN!\n                                                                        \n                                                                        Process repeats...\n                                                                        Cost: ~40-100 cycles per cycle\n\n\n\n\nWho Controls the Cache Line Updates?\n\n\nThe CPU\'s cache coherency protocol (MESI) controls everything automatically:\n\n\n 1. Hardware-level: No software involvement requiredit happens in CPU hardware\n 2. Cache controller: Each core has a cache controller that manages MESI states\n 3. Interconnect: Cores communicate through the CPU interconnect (bus or mesh)\n 4. Snooping: Cores \"snoop\" on each other\'s cache transactions to maintain coherency\n\n\n\nCost Breakdown\n\n\nPer false sharing cycle:\n\n\n * Invalidation message: ~10-20 cycles\n * Cache line request: ~10-20 cycles\n * Write-back to memory (if needed): ~10-30 cycles\n * Cache line transfer: ~40-100 cycles\n * State updates: ~5-10 cycles\n\n\nTotal per cycle: ~75-180 CPU cycles\n\n\nIf Thread 0 writes to A 1 million times per second, and Thread 1 writes to B 1 million times per second:\n\n\n * Potential cache line transfers: 2 million per second\n * Wasted cycles: ~150-360 million cycles per second\n * On a 3GHz CPU: 5-12% of total CPU time wasted on false sharing overhead\n\n\n\nWhy This Creates Serialization\n\n\nEven though Thread 0 and Thread 1 are running on different cores (true parallelism), they cannot write simultaneously because:\n\n\n 1. Only one core can have the cache line in Modified state\n 2. The other core must wait for the transfer to complete\n 3. This creates implicit serialization at the hardware level\n\n\nResult: What looks like parallel execution is actually serial execution with expensive synchronization.\n\n\n\nVisual Representation\n\n\nNormal Parallel Execution (no false sharing):\n\nCore 0: [Write A][Write A][Write A][Write A]...   Continuous\nCore 1: [Write B][Write B][Write B][Write B]...   Continuous\n         Both working simultaneously\n\nWith False Sharing:\n\nCore 0: [Write A][----WAIT----][Write A][----WAIT----]...\nCore 1: [----WAIT----][Write B][----WAIT----][Write B]...\n         Taking turns (serialized!)\n         Wasted cycles during WAIT\n\n\n\nThis hidden serialization is why performance degrades even though your code looks perfectly parallel.\n\n\n\n\nCommon Misconceptions\n\n\n\"Separate variables mean separate memory locations\"\n\n\n * The CPU caches data in 64-byte chunks called cache lines. Two variables declared separately can end up on the same cache line if they\'re close in memory. Think of it like apartment buildings: even though you live in apartment 101 and your neighbor in 102, you share the same building (cache line).\n\n\n\"Lock-free code is automatically fast\"\n\n\n * Lock-free data structures avoid blocking but can suffer from false sharing when multiple threads update adjacent fields. Atomic operations still trigger cache invalidations.\n\n\n\"High CPU usage means good parallelization\"\n\n\n * False sharing can cause high CPU usage while destroying actual parallelism. CPUs spend time waiting for cache lines to transfer between cores, not doing useful work.\n\n\n\"The compiler/runtime will optimize this away\"\n\n\n * Compilers don\'t automatically pad structures to prevent false sharing. They optimize for single-threaded performance, not multi-threaded cache behavior.\n\n\n\nWhy Naive Solutions Fail\n\n\nAdding locks: Makes it worse by serializing execution completely. The problem is cache contention, not lack of synchronization.\n\n\nIncreasing thread count: More threads mean more cache invalidations (O(n) growth). The problem compounds.\n\n\nLogical separation: Different variables can still share cache lines if they\'re physically close in memory. CPU sees physical layout, not code structure.\n\n\nAlgorithm optimization alone: Doesn\'t help if threads are fighting over cache lines at the hardware level.\n\n\n\nWhy This Becomes a Bottleneck\n\n\nCache Line Ping-Pong: Cache lines constantly bounce between cores. Each transfer consumes bandwidth and creates latency. Impact: 10-50% performance degradation.\n\n\nSerialization Despite Parallelism: Threads appear to run in parallel but wait for each other at the cache level. Impact: Applications that should scale linearly instead plateau or degrade.\n\n\nMemory Bus Saturation: Cache line transfers consume bandwidth, competing with actual data access. Impact: System-wide performance degradation.\n\n\nNUMA Amplification: In NUMA systems, remote cache transfers are 2-3x slower. Impact: 50-100% additional latency.\n\n\nScalability Collapse: Performance degrades with more threads instead of improving. Impact: Applications fail to utilize available CPU cores.\n\n\n\nWhen to Use This Approach\n\n\nHigh-performance multi-threaded applications: Many threads, high throughput requirements, processing large volumes of data.\n\n\nFrequently-updated shared state: Per-thread counters, statistics structures, lock-free data structures with adjacent fields.\n\n\nProfiling indicates cache contention: High cache miss rates, poor scalability, cache line transfers detected by tools.\n\n\nHigh-thread-count systems: 8+ cores where false sharing effects are amplified. Especially important in NUMA systems.\n\n\nLatency-sensitive parallel workloads: Real-time systems, financial trading, game engines, media processing.\n\n\nLock-free algorithms: Lock-free queues, stacks, hash tables. These are particularly susceptible to false sharing.\n\n\n\nWhen Not to Use It\n\n\nSingle-threaded applications: No parallelism means no false sharing.\n\n\nRead-only shared data: False sharing only occurs with writes. Multiple threads reading is fine (Shared state).\n\n\nInfrequently accessed data: Padding overhead isn\'t justified by occasional access.\n\n\nSmall data structures: Structures that naturally span multiple cache lines might not need explicit padding.\n\n\nCloud/containerized environments: Hardware topology is abstracted, cache line sizes may vary.\n\n\nDevelopment/prototyping: Premature optimization distracts from correctness. Profile first.\n\n\nMemory-constrained systems: Embedded systems, mobile devices. Padding overhead might be unacceptable.\n\n\nWhen profiling shows no issue: Don\'t optimize what isn\'t broken. Use tools to confirm before adding padding.\n\n\n\n\nHow to Avoid False Sharing (General Principles)\n\n\n\nStrategy 1: Per-Thread Data (Preferred)\n\n\nBest approach: Give each thread its own copy of data. No sharing = no false sharing.\n\n\nWhen to use:\n\n\n * Per-thread counters, statistics, or accumulators\n * Thread-local state that\'s aggregated later\n\n\nBenefits:\n\n\n * No padding overhead\n * No false sharing (each thread has separate memory)\n * Cleaner, simpler code\n\n\nTrade-off: Must aggregate results when needed (but this is usually infrequent).\n\n\nExample pattern:\n\n\n * Each thread maintains its own counter/state\n * Periodically (or at end), aggregate across all threads\n * Much cheaper than constant cache line contention\n\n\n\nStrategy 2: Padding and Alignment\n\n\nWhen per-thread data isn\'t feasible: Use padding to separate shared data into different cache lines.\n\n\nKey principles:\n\n\n * Ensure frequently-written variables start at cache line boundaries\n * Pad each variable to at least cache line size (64 or 128 bytes)\n * Use compiler directives to enforce alignment\n\n\nBenefits:\n\n\n * Works when data must be shared\n * Predictable memory layout\n\n\nTrade-offs:\n\n\n * Increased memory usage (4-8x)\n * More complex code\n * Platform-specific (cache line sizes vary)\n\n\n\nStrategy 3: Separate Data Structures\n\n\nDesign approach: Design data structures so hot fields written by different threads are naturally separated.\n\n\nPrinciples:\n\n\n * Place head/tail pointers in separate cache lines\n * Separate producer/consumer fields\n * Group data by access pattern (hot vs. cold)\n\n\nBenefits:\n\n\n * Natural separation, less artificial padding\n * Better overall data structure design\n\n\n\nStrategy 4: Reduce Write Frequency\n\n\nOptimization: Reduce how often threads write to shared data.\n\n\nTechniques:\n\n\n * Batch updates (write every N operations instead of every operation)\n * Use local accumulators, then periodically update shared state\n * Prefer read-heavy patterns\n\n\nBenefits:\n\n\n * Less false sharing even if data shares cache lines\n * Better cache efficiency overall\n\n\nWhen to use: When you can\'t avoid sharing but can reduce write frequency.\n\n\n\nStrategy 5: Cache Line Size Awareness\n\n\nKnow your platform:\n\n\n * x86-64: 64 bytes\n * Some ARM: 128 bytes\n * Test on target hardware\n\n\nImplementation:\n\n\n * Use constants for cache line size\n * Consider padding to 128 bytes for cross-platform safety\n * Document assumptions\n\n\n\nGeneral Checklist\n\n\n 1. Profile first: Use perf c2c or VTune to identify false sharing\n 2. Choose strategy: Prefer per-thread data when possible\n 3. Measure impact: Verify improvements after changes\n 4. Document decisions: Explain why padding/alignment exists\n 5. Test on target: Different platforms have different cache line sizes\n\n\n\n\nHow to Avoid False Sharing in C#\n\n\n\nMethod 1: ThreadLocal (Best for Per-Thread Data)\n\n\nUse when: Each thread needs its own accumulator, counter, or state.\n\n\nusing System.Threading;\n\npublic class RequestCounter {\n    // Each thread gets its own counter - no sharing!\n    private readonly ThreadLocal<long> _counter = new ThreadLocal<long>(() => 0);\n    \n    public void Increment() {\n        _counter.Value++;  // Thread-local, no false sharing\n    }\n    \n    public long GetTotal() {\n        // Aggregate across all threads when needed\n        long total = 0;\n        // Note: ThreadLocal doesn\'t provide easy enumeration\n        // You might need to track threads manually or use a different approach\n        return total;\n    }\n}\n\n// Better: Use ThreadLocal with explicit thread tracking\npublic class ThreadSafeCounter {\n    private readonly ThreadLocal<long> _counter = new ThreadLocal<long>(() => 0);\n    private readonly ConcurrentDictionary<int, long> _threadCounters = new();\n    \n    public void Increment() {\n        _counter.Value++;\n        _threadCounters[Thread.CurrentThread.ManagedThreadId] = _counter.Value;\n    }\n    \n    public long GetTotal() {\n        return _threadCounters.Values.Sum();\n    }\n}\n\n\n\nWhy it works: Each thread accesses completely separate memory locations. No cache line sharing possible.\n\n\nWhen to use: Counters, statistics, accumulators that need per-thread isolation.\n\n\n\nMethod 2: StructLayout with Padding (For Shared Data)\n\n\nUse when: Data must be shared but needs cache line separation.\n\n\nusing System.Runtime.InteropServices;\n\n// Option 1: Explicit size with padding\n[StructLayout(LayoutKind.Explicit, Size = 128)]  // Pad to 128 bytes (safe for 64 and 128-byte cache lines)\npublic struct PaddedCounter {\n    [FieldOffset(0)]\n    public long Value;\n    // Rest is automatic padding to 128 bytes\n}\n\npublic class ThreadSafeCounters {\n    private readonly PaddedCounter[] _counters;\n    \n    public ThreadSafeCounters(int threadCount) {\n        _counters = new PaddedCounter[threadCount];\n    }\n    \n    public void Increment(int threadId) {\n        Interlocked.Increment(ref _counters[threadId].Value);\n    }\n}\n\n// Option 2: Manual padding fields\npublic class ManualPaddedCounter {\n    private long _counter;\n    \n    // Pad to ensure next instance starts at new cache line\n    // Cache line is 64 bytes, long is 8 bytes\n    // Need 7 more longs (56 bytes) to reach 64 bytes total\n    private long _padding1, _padding2, _padding3, _padding4,\n                 _padding5, _padding6, _padding7;\n    \n    public long Value {\n        get => _counter;\n        set => _counter = value;\n    }\n}\n\n\n\nWhy it works: Forces each PaddedCounter to occupy a full cache line (128 bytes), ensuring separate cache lines.\n\n\nWhen to use: Arrays of per-thread data that must be indexed by thread ID.\n\n\n\nMethod 3: Separate Cache Lines for Lock-Free Structures\n\n\nUse when: Building lock-free queues, stacks, or other concurrent structures.\n\n\nusing System.Runtime.InteropServices;\n\n[StructLayout(LayoutKind.Explicit, Size = 128)]\npublic class LockFreeQueue<T> where T : class {\n    private readonly T[] _buffer;\n    private readonly int _capacity;\n    \n    [FieldOffset(0)]\n    private volatile int _head;  // Consumer writes here - first cache line\n    \n    // Padding to next cache line (64 bytes)\n    [FieldOffset(64)]\n    private volatile int _tail;  // Producer writes here - second cache line\n    \n    public LockFreeQueue(int capacity) {\n        _capacity = capacity;\n        _buffer = new T[capacity];\n        _head = 0;\n        _tail = 0;\n    }\n    \n    public bool TryEnqueue(T item) {\n        int currentTail = _tail;\n        int nextTail = (currentTail + 1) % _capacity;\n        \n        if (nextTail == _head) {\n            return false; // Queue full\n        }\n        \n        _buffer[currentTail] = item;\n        _tail = nextTail;  // Producer writes to separate cache line\n        return true;\n    }\n    \n    public bool TryDequeue(out T item) {\n        int currentHead = _head;\n        \n        if (currentHead == _tail) {\n            item = default(T);\n            return false; // Queue empty\n        }\n        \n        item = _buffer[currentHead];\n        _buffer[currentHead] = null;\n        _head = (currentHead + 1) % _capacity;  // Consumer writes to separate cache line\n        return true;\n    }\n}\n\n\n\nWhy it works: Producer (_tail) and consumer (_head) write to different cache lines, eliminating contention.\n\n\n\nMethod 4: Separate Arrays for Different Threads\n\n\nUse when: You need indexed access but can separate by thread.\n\n\n//  Bad: All counters in one array - false sharing\npublic class BadCounters {\n    private readonly long[] _counters = new long[Environment.ProcessorCount];\n    \n    public void Increment(int threadId) {\n        _counters[threadId]++;  // False sharing if elements share cache lines\n    }\n}\n\n//  Good: Use padded structures\npublic class GoodCounters {\n    private readonly PaddedCounter[] _counters;\n    \n    public GoodCounters() {\n        int threadCount = Environment.ProcessorCount;\n        _counters = new PaddedCounter[threadCount];\n    }\n    \n    public void Increment(int threadId) {\n        Interlocked.Increment(ref _counters[threadId].Value);\n    }\n}\n\n// Using the PaddedCounter struct from Method 2\n[StructLayout(LayoutKind.Explicit, Size = 128)]\npublic struct PaddedCounter {\n    [FieldOffset(0)]\n    public long Value;\n}\n\n\n\n\nMethod 5: Reduce Write Frequency (Batching)\n\n\nUse when: You can\'t avoid sharing but can reduce write frequency.\n\n\npublic class BatchedCounter {\n    // Thread-local accumulator\n    private readonly ThreadLocal<long> _localCounter = new ThreadLocal<long>(() => 0);\n    \n    // Shared counter, updated less frequently\n    private long _sharedCounter;\n    private readonly object _lock = new object();\n    private const int BATCH_SIZE = 1000;\n    \n    public void Increment() {\n        _localCounter.Value++;\n        \n        // Only update shared counter every BATCH_SIZE increments\n        if (_localCounter.Value % BATCH_SIZE == 0) {\n            lock (_lock) {\n                _sharedCounter += BATCH_SIZE;\n            }\n        }\n    }\n    \n    public long GetTotal() {\n        long total = _sharedCounter;\n        // Add any remaining in thread-local counters\n        // (simplified - in practice, you\'d need to track all threads)\n        return total;\n    }\n}\n\n\n\nWhy it works: Reduces writes to shared data by 1000x (from every increment to every 1000 increments).\n\n\n\nMethod 6: Using Memory-Mapped or Aligned Allocation (Advanced)\n\n\nUse when: You need precise control over memory layout.\n\n\nusing System.Runtime.InteropServices;\n\n// Note: This requires unsafe code and platform-specific implementation\npublic unsafe class AlignedCounter {\n    private long* _counter;\n    \n    public AlignedCounter() {\n        // Allocate aligned to cache line boundary (64 bytes)\n        // This is platform-specific and requires P/Invoke or native allocation\n        _counter = (long*)AlignedAlloc(64, sizeof(long));\n    }\n    \n    private void* AlignedAlloc(ulong alignment, ulong size) {\n        // Platform-specific implementation needed:\n        // - Windows: _aligned_malloc\n        // - Linux: posix_memalign or aligned_alloc\n        // - Use DllImport or NativeMemory.AlignedAlloc (modern .NET)\n        throw new NotImplementedException(\"Platform-specific implementation\");\n    }\n    \n    // Modern .NET alternative (if available)\n    public void ModernApproach() {\n        // .NET 6+ has NativeMemory.AlignedAlloc\n        // IntPtr ptr = NativeMemory.AlignedAlloc((nuint)sizeof(long), 64);\n    }\n}\n\n\n\nWhy it works: Ensures memory starts exactly at a cache line boundary.\n\n\nWhen to use: When you need guaranteed alignment and can\'t use StructLayout.\n\n\n\nC# Best Practices Summary\n\n\n 1. Prefer ThreadLocal: Simplest and most effective for per-thread data\n 2. Use StructLayout for arrays: When you need indexed access to per-thread data\n 3. Separate producer/consumer fields: For lock-free structures, ensure 64+ bytes separation\n 4. Batch updates: Reduce write frequency when you can\'t avoid sharing\n 5. Use constants: Define CACHE_LINE_SIZE = 64 or 128 as a constant\n 6. Verify with tools: Use profiling tools to confirm false sharing is fixed\n 7. Document: Add comments explaining why padding exists\n\n\n\nCommon C# Pitfalls\n\n\nPitfall 1: Assuming array elements are separate\n\n\n//  Bad: Array of longs - elements might share cache lines\nlong[] counters = new long[8];  // 8 * 8 = 64 bytes - all in one cache line!\n\n//  Good: Use padded structures\nPaddedCounter[] counters = new PaddedCounter[8];  // Each is 128 bytes - separate cache lines\n\n\n\nPitfall 2: Not using StructLayout\n\n\n//  Bad: Compiler might reorder fields\npublic struct Counter {\n    public long Value;\n    public long Padding1, Padding2, ...;  // Might not work!\n}\n\n//  Good: Explicit layout\n[StructLayout(LayoutKind.Explicit, Size = 128)]\npublic struct Counter {\n    [FieldOffset(0)]\n    public long Value;\n}\n\n\n\nPitfall 3: Forgetting about object headers\n\n\n// In C#, objects have headers (overhead)\n// PaddedCounter struct is fine, but arrays of objects might have additional overhead\n// Prefer structs over classes for per-thread data\n\n\n\n\nPerformance Impact in C#\n\n\nTypical improvements when fixing false sharing in C#:\n\n\n * Per-thread counters: 30-50% throughput improvement\n * Lock-free queues: 40-60% latency reduction\n * Thread pool statistics: 25-40% overhead reduction\n * Scalability: Can often restore linear scaling up to 16-32 threads\n\n',NULL,0,'post','published',NULL,'public','all','2025-12-30 01:40:42','2025-12-30 01:43:21','2025-12-30 01:40:42','694f2665c3e44e650f0e5dd8',NULL,NULL,NULL,NULL,NULL,NULL,1),('6954500debcfaae683eec9b2','c0a8ac93-dc31-4384-95a7-d62b4309d4e2','Optimize Branch Prediction for Better CPU Pipeline Utilization','optimize-branch-prediction-for-better-cpu-pipeline-utilization',NULL,'{\"root\":{\"children\":[{\"type\":\"markdown\",\"version\":1,\"markdown\":\"**Write code with predictable control flow to minimize branch mispredictions and maximize CPU pipeline efficiency.**\\n\\n---\\n\\n## Executive Summary (TL;DR)\\n\\nBranch prediction is a CPU optimization technique that guesses which path code will take at conditional statements (if/else, loops, switches) before the condition is actually evaluated. When predictions are correct, the CPU pipeline continues smoothly. When predictions are wrong, the CPU must discard speculative work and restart, costing 10-20 CPU cycles per misprediction. To optimize for branch prediction, write code with predictable patterns: move common cases first, use branchless operations where possible, and separate unpredictable branches. This typically improves performance by 5-30% in code-heavy with conditionals. The trade-off is that it may reduce code readability and requires profiling to identify problematic branches. Apply this optimization primarily to hot paths in performance-critical code after profiling confirms branch mispredictions.\\n\\n---\\n\\n## Problem Context\\n\\nModern CPUs can execute multiple instructions simultaneously through **pipelining**think of it like an assembly line where different stages process different instructions at the same time. However, conditional branches (if/else, loops, switches) create a problem: the CPU doesn\'t know which path to take until it evaluates the condition, but it needs to know *now* to keep the pipeline full.\\n\\n**What is pipelining?** Imagine a factory assembly line. Instead of building one car completely before starting the next, you have stages: frame, engine, wheels, paint. While one car is being painted, the next is getting wheels, and the one after that is getting an engine. Similarly, a CPU pipeline has stages like: fetch instruction, decode, execute, write result. Modern CPUs have 10-20 pipeline stages, and when full, they can process multiple instructions simultaneously.\\n\\n**What is a branch?** Any point in code where execution can take different paths: if/else statements, loops (should we continue?), switch statements, function calls, etc.\\n\\n**The problem**: When the CPU encounters a branch, it must wait for the condition to be evaluated before knowing which instructions to load next. This **stalls the pipeline**, wasting cycles. To avoid this, CPUs use **branch prediction**they guess which path will be taken based on historical patterns.\\n\\n### Common Misconceptions\\n\\n**\\\"Branch prediction is something I control\\\"**\\n- Branch prediction happens automatically in CPU hardware. You don\'t explicitly control it, but your code patterns influence how well it works.\\n\\n**\\\"All branches are equally expensive\\\"**\\n- Branches that are predictable (always true, always false, or follow patterns) have near-zero cost. Unpredictable branches cost 10-20 cycles per misprediction.\\n\\n**\\\"Modern CPUs are so fast, branches don\'t matter\\\"**\\n- Modern CPUs are fast *because* of optimizations like branch prediction. When prediction fails, you still pay the penalty. In tight loops with branches, this adds up quickly.\\n\\n**\\\"I should eliminate all branches\\\"**\\n- Not necessary. Eliminate or optimize *unpredictable* branches in hot paths. Predictable branches have minimal cost.\\n\\n### Why Naive Solutions Fail\\n\\n**Adding more branches to \\\"clarify\\\" logic**\\n- More branches mean more prediction opportunities. If they\'re unpredictable, performance gets worse.\\n\\n**Complex nested conditionals**\\n- Nested branches compound the problem. If the outer branch is mispredicted, inner branches may be evaluated speculatively (wrong path), wasting more cycles.\\n\\n**Assuming the compiler optimizes everything**\\n- Compilers do optimize, but they can\'t fix fundamentally unpredictable control flow patterns. The CPU\'s hardware predictor works with patterns, not logic.\\n\\n**Ignoring profiling data**\\n- Without profiling, you might optimize the wrong branches or miss the ones causing real performance problems.\\n\\n---\\n\\n## How It Works\\n\\n### Understanding CPU Pipelines\\n\\n**What is a CPU pipeline?** Modern CPUs break instruction execution into stages. While one instruction is being executed, the next is being decoded, and the one after that is being fetched from memory. This parallelism allows CPUs to process multiple instructions simultaneously.\\n\\n**Pipeline stages** (simplified):\\n1. **Fetch**: Load instruction from memory\\n2. **Decode**: Determine what the instruction does\\n3. **Execute**: Perform the operation\\n4. **Memory**: Access data memory (if needed)\\n5. **Write-back**: Store result\\n\\nWith a 5-stage pipeline, theoretically 5 instructions can be in flight at once. Modern CPUs have 10-20 stages.\\n\\n**The pipeline problem**: To keep the pipeline full, the CPU must fetch the next instruction *before* the current one finishes. But with branches, the CPU doesn\'t know which instruction comes next until the branch condition is evaluated. This creates a **hazard**a situation that prevents the pipeline from proceeding.\\n\\n**What is a hazard?** A situation where the pipeline cannot continue because it lacks necessary information. Branch hazards occur because we don\'t know which instruction to fetch next until the branch is resolved.\\n\\n### Branch Prediction Explained\\n\\n**What is branch prediction?** The CPU guesses which path a branch will take based on:\\n- **Static prediction**: Simple heuristics (e.g., forward branches are usually not taken, backward branches usually are)\\n- **Dynamic prediction**: Historical patterns (e.g., this branch was taken 90% of the time recently, so predict \\\"taken\\\")\\n- **Branch target prediction**: Predicting the target address for indirect branches\\n\\n**Modern branch predictors** use sophisticated algorithms:\\n- **Pattern history tables**: Track taken/not-taken patterns\\n- **Branch target buffers**: Cache target addresses\\n- **Global vs. local history**: Consider recent branches globally or per-branch\\n\\n**What happens when prediction is correct?**\\n- Pipeline continues smoothly\\n- Next instructions are already being processed\\n- Near-zero penalty (maybe 1 cycle for prediction logic)\\n\\n**What happens when prediction is wrong?**\\n- CPU must **flush the pipeline**discard all speculatively executed instructions\\n- Restart from the correct path\\n- **Branch misprediction penalty**: 10-20 cycles on modern CPUs\\n- All the work done speculatively is wasted\\n\\n**Why is the penalty so high?** The pipeline is deep (10-20 stages). When a misprediction is discovered, all instructions in the wrong path must be discarded, and the pipeline must restart. The deeper the pipeline, the higher the cost.\\n\\n### Real-World Example: Loop with Condition\\n\\n```csharp\\n// Loop that processes items, conditionally incrementing a counter\\nint count = 0;\\nforeach (var item in items) {\\n    if (item.Value > threshold) {  // Branch inside hot loop\\n        count++;\\n    }\\n}\\n```\\n\\n**What the CPU sees**:\\n- For each iteration, must predict: will `item.Value > threshold` be true or false?\\n- If items are mostly above threshold, predictor learns \\\"taken\\\"\\n- If items are mostly below, predictor learns \\\"not taken\\\"\\n- If it\'s random, predictor fails frequently  many mispredictions\\n\\n**Cost calculation**:\\n- 1,000,000 items\\n- If branch is 50/50 unpredictable: ~500,000 mispredictions\\n- 500,000  15 cycles = 7,500,000 wasted cycles\\n- On a 3GHz CPU: 2.5ms wasted (significant in a tight loop!)\\n\\n---\\n\\n## Why This Becomes a Bottleneck\\n\\n### Pipeline Stalls\\n\\n**What happens**: When a branch is mispredicted, the pipeline must flush and restart. During this time, no useful work is done.\\n\\n**Impact**: In tight loops with unpredictable branches, you might spend 10-20% of CPU time on misprediction penalties instead of actual computation.\\n\\n**Example**: A loop that processes 1 million items with a 50/50 unpredictable branch might waste 2-3ms just on branch mispredictions. In a function that should complete in 10ms, this is a 20-30% overhead.\\n\\n### Speculative Execution Waste\\n\\n**What is speculative execution?** When the CPU predicts a branch will be taken, it speculatively executes instructions from that path before confirming the prediction.\\n\\n**The waste**: If prediction is wrong, all speculatively executed instructions are discarded:\\n- Decoded instructions: wasted\\n- Executed operations: wasted (unless side-effect free)\\n- Cache loads: might still help (prefetched data)\\n- Memory bandwidth: partially wasted\\n\\n**Impact**: Not just the misprediction penalty, but also wasted work and resources.\\n\\n### Compounding Effects\\n\\n**Multiple branches in sequence**:\\n- If Branch A is mispredicted, Branch B might be evaluated on the wrong path\\n- When the pipeline corrects, Branch B must be re-evaluated\\n- Cascading waste from multiple mispredictions\\n\\n**Nested branches**:\\n- Outer branch misprediction causes inner branches to be evaluated speculatively on wrong path\\n- More instructions wasted, larger penalty\\n\\n**Impact**: Complex control flow with unpredictable branches can amplify the problem.\\n\\n### Cache and Memory Effects\\n\\n**Instruction cache misses**: When a branch is mispredicted, the CPU might load instructions from the wrong path into the instruction cache. When it corrects, it must load the correct instructions, potentially causing cache misses.\\n\\n**Data prefetching**: Speculative execution might prefetch data from the wrong path, wasting memory bandwidth.\\n\\n**Impact**: Additional penalties beyond the direct misprediction cost.\\n\\n---\\n\\n## When to Use This Approach\\n\\n**Hot paths in performance-critical code**: Code that executes frequently (millions of times per second) and is on the critical path for latency or throughput.\\n\\n**Loops with conditions**: Especially tight loops with branches inside. The branch is evaluated many times, so mispredictions compound.\\n\\n**After profiling confirms branch mispredictions**: Use profiling tools (perf, VTune) to identify branches with high misprediction rates. Optimize those, not all branches.\\n\\n**Code with predictable patterns**: When you can make branches more predictable (e.g., process common cases first, sort data to make comparisons predictable).\\n\\n**Latency-sensitive applications**: Where consistent, low latency matters (game engines, trading systems, real-time systems).\\n\\n**High-throughput processing**: Where processing speed directly impacts business metrics (data processing, request handling).\\n\\n**Why these scenarios**: Branch optimization only matters when branches are frequently executed. In cold code, the optimization cost (readability, maintenance) isn\'t worth it.\\n\\n---\\n\\n## When Not to Use It\\n\\n**Cold code**: Code that executes rarely. The optimization cost (readability) isn\'t worth the negligible performance benefit.\\n\\n**Already predictable branches**: If profiling shows branches are already well-predicted (low misprediction rate), optimization won\'t help.\\n\\n**One-time initialization**: Code that runs once at startup. Branch mispredictions here don\'t matter.\\n\\n**Readability is more important**: When code clarity and maintainability are priorities over micro-optimizations.\\n\\n**Compiler already optimizes it**: Modern compilers do branch optimization. Manual optimization might be redundant or conflict with compiler decisions.\\n\\n**No profiling data**: Don\'t optimize branches without data showing they\'re a problem. You might optimize the wrong thing.\\n\\n**Complex optimization for minimal gain**: If the optimization makes code much more complex for a 1-2% gain, it\'s probably not worth it.\\n\\n**Why avoid these**: Branch optimization has costs (readability, maintenance). Only apply when benefits clearly outweigh costs.\\n\\n---\\n\\n## How to Measure and Validate\\n\\n### Profiling Tools\\n\\n**perf (Linux)**:\\n```bash\\n# Measure branch mispredictions\\nperf stat -e branches,branch-misses ./your_application\\n\\n# Detailed branch analysis\\nperf record -e branch-misses ./your_application\\nperf report\\n```\\n\\n**Intel VTune**: \\n- \\\"Microarchitecture Exploration\\\" analysis\\n- Shows branch misprediction hotspots\\n- Visual representation of branch efficiency\\n\\n**Visual Studio Profiler (Windows)**:\\n- \\\"CPU Usage\\\" profiling\\n- Can show branch misprediction events\\n- Timeline view of performance issues\\n\\n### Key Metrics\\n\\n**Branch misprediction rate**: \\n- Formula: `(branch-misses / branches)  100%`\\n- Target: < 5% for hot code\\n- Action: If > 10%, investigate and optimize\\n\\n**Cycles lost to mispredictions**:\\n- Calculate: `branch-misses  misprediction-penalty`\\n- Compare to total cycles to see impact\\n\\n**Instruction-per-cycle (IPC)**:\\n- Higher is better (more work per cycle)\\n- Branch mispredictions reduce IPC\\n- Monitor IPC before/after optimization\\n\\n### Detection Strategies\\n\\n1. **Profile your hot paths**: Use profiling tools on code that executes frequently\\n2. **Look for high misprediction rates**: Branches with >10% misprediction rate are candidates\\n3. **Identify unpredictable patterns**: Look for branches that alternate unpredictably\\n4. **Measure before/after**: Profile, optimize, profile again to verify improvement\\n\\n---\\n\\n## Optimization Techniques\\n\\n### Technique 1: Common Case First\\n\\n**Principle**: Put the most likely path first in if/else statements.\\n\\n**Why it works**: CPU predictors often favor the first path (static prediction) or learn that the first path is more common.\\n\\n```csharp\\n//  Bad: Rare case first\\nif (errorOccurred) {  // Rare case\\n    HandleError();\\n} else {  // Common case\\n    ProcessNormal();\\n}\\n\\n//  Good: Common case first\\nif (!errorOccurred) {  // Common case\\n    ProcessNormal();\\n} else {  // Rare case\\n    HandleError();\\n}\\n```\\n\\n**Benefit**: Predictor learns the common path faster, fewer mispredictions.\\n\\n### Technique 2: Separate Unpredictable Branches\\n\\n**Principle**: If you have a loop with an unpredictable branch, separate the filtering from the processing.\\n\\n```csharp\\n//  Bad: Unpredictable branch in hot loop\\nint count = 0;\\nforeach (var item in items) {\\n    if (item.IsValid && item.Value > threshold) {  // Unpredictable\\n        Process(item);\\n        count++;\\n    }\\n}\\n\\n//  Good: Separate filtering (branch once per item)\\nvar validItems = items\\n    .Where(i => i.IsValid && i.Value > threshold)\\n    .ToList();  // Branch here, but only once per item\\n\\nforeach (var item in validItems) {  // No branches in hot loop!\\n    Process(item);\\n}\\n```\\n\\n**Benefit**: Branch happens during filtering (once), not in the hot processing loop (many times).\\n\\n### Technique 3: Branchless Operations\\n\\n**Principle**: Use arithmetic operations instead of branches when possible.\\n\\n```csharp\\n//  Bad: Branch in hot loop\\nint count = 0;\\nforeach (var value in values) {\\n    if (value > threshold) {\\n        count++;\\n    }\\n}\\n\\n//  Good: Branchless\\nint count = 0;\\nforeach (var value in values) {\\n    count += (value > threshold) ? 1 : 0;  // No branch, just arithmetic\\n}\\n\\n// Or using LINQ (compiler may optimize)\\nint count = values.Count(v => v > threshold);\\n```\\n\\n**Why it works**: Conditional expressions can be compiled to branchless code (conditional moves, bitwise operations). No branch = no misprediction.\\n\\n**Trade-off**: Might be slightly less readable. Use when profiling shows the branch is a problem.\\n\\n### Technique 4: Sort Data for Predictable Comparisons\\n\\n**Principle**: When comparing values in a loop, sorted data makes branches predictable.\\n\\n```csharp\\n// Unsorted data: comparisons are unpredictable\\nforeach (var item in unsortedItems) {\\n    if (item.Value > threshold) {  // 50/50 unpredictable\\n        Process(item);\\n    }\\n}\\n\\n// Sorted data: comparisons are predictable\\nArray.Sort(items, (a, b) => a.Value.CompareTo(b.Value));\\nforeach (var item in items) {\\n    if (item.Value > threshold) {  // First all false, then all true\\n        Process(item);  // Predictable pattern!\\n    }\\n}\\n```\\n\\n**Why it works**: With sorted data, comparisons follow a pattern (all false, then all true). Predictors learn this quickly.\\n\\n**Trade-off**: Sorting has a cost. Only worth it if you process the data multiple times or if sorting is cheap.\\n\\n### Technique 5: Use Lookup Tables Instead of Switches\\n\\n**Principle**: For small, dense switch statements, lookup tables can avoid branches.\\n\\n```csharp\\n//  Many branches (switch)\\nint result;\\nswitch (value) {\\n    case 0: result = Function0(); break;\\n    case 1: result = Function1(); break;\\n    case 2: result = Function2(); break;\\n    // ... many cases\\n}\\n\\n//  Lookup table (no branches if predictable)\\nvar functions = new Func<int>[] { Function0, Function1, Function2, ... };\\nint result = functions[value]();  // Direct jump, no branches\\n```\\n\\n**Why it works**: Direct array access and function call, no conditional branches to predict.\\n\\n**Trade-off**: Only works for dense, small ranges. Sparse switches might be better as switches.\\n\\n### Technique 6: Loop Unrolling for Predictable Patterns\\n\\n**Principle**: Reduce the number of loop branches by processing multiple items per iteration.\\n\\n```csharp\\n// Standard loop: branch every iteration\\nfor (int i = 0; i < array.Length; i++) {\\n    Process(array[i]);\\n}\\n\\n// Unrolled: branch every 4 iterations\\nfor (int i = 0; i < array.Length - 3; i += 4) {\\n    Process(array[i]);\\n    Process(array[i + 1]);\\n    Process(array[i + 2]);\\n    Process(array[i + 3]);\\n}\\n// Handle remainder...\\n\\n// Or let the compiler do it (modern compilers auto-unroll when beneficial)\\n```\\n\\n**Why it works**: Fewer loop branches = fewer misprediction opportunities.\\n\\n**Trade-off**: More code, might hurt instruction cache. Modern compilers often do this automatically.\\n\\n---\\n\\n## Example Scenarios\\n\\n### Scenario 1: Processing Items with Filters\\n\\n**Problem**: Loop with unpredictable filtering condition.\\n\\n**Solution**: Separate filtering from processing.\\n\\n```csharp\\n// Before: Unpredictable branch in hot loop\\npublic int ProcessValidItems(List<Item> items, int threshold) {\\n    int count = 0;\\n    foreach (var item in items) {\\n        if (item.IsValid && item.Value > threshold) {  // Unpredictable\\n            ProcessItem(item);\\n            count++;\\n        }\\n    }\\n    return count;\\n}\\n\\n// After: Predictable (or no branches in hot loop)\\npublic int ProcessValidItems(List<Item> items, int threshold) {\\n    var validItems = items\\n        .Where(i => i.IsValid && i.Value > threshold)\\n        .ToList();\\n    \\n    foreach (var item in validItems) {  // No branches!\\n        ProcessItem(item);\\n    }\\n    return validItems.Count;\\n}\\n```\\n\\n**Performance**: 10-20% improvement when the branch was unpredictable.\\n\\n### Scenario 2: Error Handling\\n\\n**Problem**: Error checks in hot path, but errors are rare.\\n\\n**Solution**: Common case first, or extract error handling.\\n\\n```csharp\\n// Before: Rare case first\\npublic void ProcessRequest(Request req) {\\n    if (req == null || !req.IsValid) {  // Rare\\n        throw new ArgumentException();\\n    }\\n    // Common case...\\n}\\n\\n// After: Common case first, or extract\\npublic void ProcessRequest(Request req) {\\n    if (req != null && req.IsValid) {  // Common case first\\n        // Process...\\n    } else {\\n        throw new ArgumentException();\\n    }\\n}\\n```\\n\\n**Performance**: 5-10% improvement in hot request processing path.\\n\\n### Scenario 3: Counting with Conditions\\n\\n**Problem**: Branch in counting loop.\\n\\n**Solution**: Branchless counting.\\n\\n```csharp\\n// Before: Branch\\nint count = 0;\\nforeach (var value in values) {\\n    if (value > threshold) {\\n        count++;\\n    }\\n}\\n\\n// After: Branchless\\nint count = 0;\\nforeach (var value in values) {\\n    count += (value > threshold) ? 1 : 0;\\n}\\n\\n// Or LINQ (compiler optimizes)\\nint count = values.Count(v => v > threshold);\\n```\\n\\n**Performance**: 15-25% improvement in tight counting loops.\"}],\"direction\":null,\"format\":\"\",\"indent\":0,\"type\":\"root\",\"version\":1}}','<p><strong>Write code with predictable control flow to minimize branch mispredictions and maximize CPU pipeline efficiency.</strong></p>\n<hr>\n<h2 id=\"executive-summary-tldr\">Executive Summary (TL;DR)</h2>\n<p>Branch prediction is a CPU optimization technique that guesses which path code will take at conditional statements (if/else, loops, switches) before the condition is actually evaluated. When predictions are correct, the CPU pipeline continues smoothly. When predictions are wrong, the CPU must discard speculative work and restart, costing 10-20 CPU cycles per misprediction. To optimize for branch prediction, write code with predictable patterns: move common cases first, use branchless operations where possible, and separate unpredictable branches. This typically improves performance by 5-30% in code-heavy with conditionals. The trade-off is that it may reduce code readability and requires profiling to identify problematic branches. Apply this optimization primarily to hot paths in performance-critical code after profiling confirms branch mispredictions.</p>\n<hr>\n<h2 id=\"problem-context\">Problem Context</h2>\n<p>Modern CPUs can execute multiple instructions simultaneously through <strong>pipelining</strong>think of it like an assembly line where different stages process different instructions at the same time. However, conditional branches (if/else, loops, switches) create a problem: the CPU doesn\'t know which path to take until it evaluates the condition, but it needs to know <em>now</em> to keep the pipeline full.</p>\n<p><strong>What is pipelining?</strong> Imagine a factory assembly line. Instead of building one car completely before starting the next, you have stages: frame, engine, wheels, paint. While one car is being painted, the next is getting wheels, and the one after that is getting an engine. Similarly, a CPU pipeline has stages like: fetch instruction, decode, execute, write result. Modern CPUs have 10-20 pipeline stages, and when full, they can process multiple instructions simultaneously.</p>\n<p><strong>What is a branch?</strong> Any point in code where execution can take different paths: if/else statements, loops (should we continue?), switch statements, function calls, etc.</p>\n<p><strong>The problem</strong>: When the CPU encounters a branch, it must wait for the condition to be evaluated before knowing which instructions to load next. This <strong>stalls the pipeline</strong>, wasting cycles. To avoid this, CPUs use <strong>branch prediction</strong>they guess which path will be taken based on historical patterns.</p>\n<h3 id=\"common-misconceptions\">Common Misconceptions</h3>\n<p><strong>\"Branch prediction is something I control\"</strong></p>\n<ul>\n<li>Branch prediction happens automatically in CPU hardware. You don\'t explicitly control it, but your code patterns influence how well it works.</li>\n</ul>\n<p><strong>\"All branches are equally expensive\"</strong></p>\n<ul>\n<li>Branches that are predictable (always true, always false, or follow patterns) have near-zero cost. Unpredictable branches cost 10-20 cycles per misprediction.</li>\n</ul>\n<p><strong>\"Modern CPUs are so fast, branches don\'t matter\"</strong></p>\n<ul>\n<li>Modern CPUs are fast <em>because</em> of optimizations like branch prediction. When prediction fails, you still pay the penalty. In tight loops with branches, this adds up quickly.</li>\n</ul>\n<p><strong>\"I should eliminate all branches\"</strong></p>\n<ul>\n<li>Not necessary. Eliminate or optimize <em>unpredictable</em> branches in hot paths. Predictable branches have minimal cost.</li>\n</ul>\n<h3 id=\"why-naive-solutions-fail\">Why Naive Solutions Fail</h3>\n<p><strong>Adding more branches to \"clarify\" logic</strong></p>\n<ul>\n<li>More branches mean more prediction opportunities. If they\'re unpredictable, performance gets worse.</li>\n</ul>\n<p><strong>Complex nested conditionals</strong></p>\n<ul>\n<li>Nested branches compound the problem. If the outer branch is mispredicted, inner branches may be evaluated speculatively (wrong path), wasting more cycles.</li>\n</ul>\n<p><strong>Assuming the compiler optimizes everything</strong></p>\n<ul>\n<li>Compilers do optimize, but they can\'t fix fundamentally unpredictable control flow patterns. The CPU\'s hardware predictor works with patterns, not logic.</li>\n</ul>\n<p><strong>Ignoring profiling data</strong></p>\n<ul>\n<li>Without profiling, you might optimize the wrong branches or miss the ones causing real performance problems.</li>\n</ul>\n<hr>\n<h2 id=\"how-it-works\">How It Works</h2>\n<h3 id=\"understanding-cpu-pipelines\">Understanding CPU Pipelines</h3>\n<p><strong>What is a CPU pipeline?</strong> Modern CPUs break instruction execution into stages. While one instruction is being executed, the next is being decoded, and the one after that is being fetched from memory. This parallelism allows CPUs to process multiple instructions simultaneously.</p>\n<p><strong>Pipeline stages</strong> (simplified):</p>\n<ol>\n<li><strong>Fetch</strong>: Load instruction from memory</li>\n<li><strong>Decode</strong>: Determine what the instruction does</li>\n<li><strong>Execute</strong>: Perform the operation</li>\n<li><strong>Memory</strong>: Access data memory (if needed)</li>\n<li><strong>Write-back</strong>: Store result</li>\n</ol>\n<p>With a 5-stage pipeline, theoretically 5 instructions can be in flight at once. Modern CPUs have 10-20 stages.</p>\n<p><strong>The pipeline problem</strong>: To keep the pipeline full, the CPU must fetch the next instruction <em>before</em> the current one finishes. But with branches, the CPU doesn\'t know which instruction comes next until the branch condition is evaluated. This creates a <strong>hazard</strong>a situation that prevents the pipeline from proceeding.</p>\n<p><strong>What is a hazard?</strong> A situation where the pipeline cannot continue because it lacks necessary information. Branch hazards occur because we don\'t know which instruction to fetch next until the branch is resolved.</p>\n<h3 id=\"branch-prediction-explained\">Branch Prediction Explained</h3>\n<p><strong>What is branch prediction?</strong> The CPU guesses which path a branch will take based on:</p>\n<ul>\n<li><strong>Static prediction</strong>: Simple heuristics (e.g., forward branches are usually not taken, backward branches usually are)</li>\n<li><strong>Dynamic prediction</strong>: Historical patterns (e.g., this branch was taken 90% of the time recently, so predict \"taken\")</li>\n<li><strong>Branch target prediction</strong>: Predicting the target address for indirect branches</li>\n</ul>\n<p><strong>Modern branch predictors</strong> use sophisticated algorithms:</p>\n<ul>\n<li><strong>Pattern history tables</strong>: Track taken/not-taken patterns</li>\n<li><strong>Branch target buffers</strong>: Cache target addresses</li>\n<li><strong>Global vs. local history</strong>: Consider recent branches globally or per-branch</li>\n</ul>\n<p><strong>What happens when prediction is correct?</strong></p>\n<ul>\n<li>Pipeline continues smoothly</li>\n<li>Next instructions are already being processed</li>\n<li>Near-zero penalty (maybe 1 cycle for prediction logic)</li>\n</ul>\n<p><strong>What happens when prediction is wrong?</strong></p>\n<ul>\n<li>CPU must <strong>flush the pipeline</strong>discard all speculatively executed instructions</li>\n<li>Restart from the correct path</li>\n<li><strong>Branch misprediction penalty</strong>: 10-20 cycles on modern CPUs</li>\n<li>All the work done speculatively is wasted</li>\n</ul>\n<p><strong>Why is the penalty so high?</strong> The pipeline is deep (10-20 stages). When a misprediction is discovered, all instructions in the wrong path must be discarded, and the pipeline must restart. The deeper the pipeline, the higher the cost.</p>\n<h3 id=\"real-world-example-loop-with-condition\">Real-World Example: Loop with Condition</h3>\n<pre><code class=\"language-csharp\">// Loop that processes items, conditionally incrementing a counter\nint count = 0;\nforeach (var item in items) {\n    if (item.Value &gt; threshold) {  // Branch inside hot loop\n        count++;\n    }\n}\n</code></pre>\n<p><strong>What the CPU sees</strong>:</p>\n<ul>\n<li>For each iteration, must predict: will <code>item.Value &gt; threshold</code> be true or false?</li>\n<li>If items are mostly above threshold, predictor learns \"taken\"</li>\n<li>If items are mostly below, predictor learns \"not taken\"</li>\n<li>If it\'s random, predictor fails frequently  many mispredictions</li>\n</ul>\n<p><strong>Cost calculation</strong>:</p>\n<ul>\n<li>1,000,000 items</li>\n<li>If branch is 50/50 unpredictable: ~500,000 mispredictions</li>\n<li>500,000  15 cycles = 7,500,000 wasted cycles</li>\n<li>On a 3GHz CPU: 2.5ms wasted (significant in a tight loop!)</li>\n</ul>\n<hr>\n<h2 id=\"why-this-becomes-a-bottleneck\">Why This Becomes a Bottleneck</h2>\n<h3 id=\"pipeline-stalls\">Pipeline Stalls</h3>\n<p><strong>What happens</strong>: When a branch is mispredicted, the pipeline must flush and restart. During this time, no useful work is done.</p>\n<p><strong>Impact</strong>: In tight loops with unpredictable branches, you might spend 10-20% of CPU time on misprediction penalties instead of actual computation.</p>\n<p><strong>Example</strong>: A loop that processes 1 million items with a 50/50 unpredictable branch might waste 2-3ms just on branch mispredictions. In a function that should complete in 10ms, this is a 20-30% overhead.</p>\n<h3 id=\"speculative-execution-waste\">Speculative Execution Waste</h3>\n<p><strong>What is speculative execution?</strong> When the CPU predicts a branch will be taken, it speculatively executes instructions from that path before confirming the prediction.</p>\n<p><strong>The waste</strong>: If prediction is wrong, all speculatively executed instructions are discarded:</p>\n<ul>\n<li>Decoded instructions: wasted</li>\n<li>Executed operations: wasted (unless side-effect free)</li>\n<li>Cache loads: might still help (prefetched data)</li>\n<li>Memory bandwidth: partially wasted</li>\n</ul>\n<p><strong>Impact</strong>: Not just the misprediction penalty, but also wasted work and resources.</p>\n<h3 id=\"compounding-effects\">Compounding Effects</h3>\n<p><strong>Multiple branches in sequence</strong>:</p>\n<ul>\n<li>If Branch A is mispredicted, Branch B might be evaluated on the wrong path</li>\n<li>When the pipeline corrects, Branch B must be re-evaluated</li>\n<li>Cascading waste from multiple mispredictions</li>\n</ul>\n<p><strong>Nested branches</strong>:</p>\n<ul>\n<li>Outer branch misprediction causes inner branches to be evaluated speculatively on wrong path</li>\n<li>More instructions wasted, larger penalty</li>\n</ul>\n<p><strong>Impact</strong>: Complex control flow with unpredictable branches can amplify the problem.</p>\n<h3 id=\"cache-and-memory-effects\">Cache and Memory Effects</h3>\n<p><strong>Instruction cache misses</strong>: When a branch is mispredicted, the CPU might load instructions from the wrong path into the instruction cache. When it corrects, it must load the correct instructions, potentially causing cache misses.</p>\n<p><strong>Data prefetching</strong>: Speculative execution might prefetch data from the wrong path, wasting memory bandwidth.</p>\n<p><strong>Impact</strong>: Additional penalties beyond the direct misprediction cost.</p>\n<hr>\n<h2 id=\"when-to-use-this-approach\">When to Use This Approach</h2>\n<p><strong>Hot paths in performance-critical code</strong>: Code that executes frequently (millions of times per second) and is on the critical path for latency or throughput.</p>\n<p><strong>Loops with conditions</strong>: Especially tight loops with branches inside. The branch is evaluated many times, so mispredictions compound.</p>\n<p><strong>After profiling confirms branch mispredictions</strong>: Use profiling tools (perf, VTune) to identify branches with high misprediction rates. Optimize those, not all branches.</p>\n<p><strong>Code with predictable patterns</strong>: When you can make branches more predictable (e.g., process common cases first, sort data to make comparisons predictable).</p>\n<p><strong>Latency-sensitive applications</strong>: Where consistent, low latency matters (game engines, trading systems, real-time systems).</p>\n<p><strong>High-throughput processing</strong>: Where processing speed directly impacts business metrics (data processing, request handling).</p>\n<p><strong>Why these scenarios</strong>: Branch optimization only matters when branches are frequently executed. In cold code, the optimization cost (readability, maintenance) isn\'t worth it.</p>\n<hr>\n<h2 id=\"when-not-to-use-it\">When Not to Use It</h2>\n<p><strong>Cold code</strong>: Code that executes rarely. The optimization cost (readability) isn\'t worth the negligible performance benefit.</p>\n<p><strong>Already predictable branches</strong>: If profiling shows branches are already well-predicted (low misprediction rate), optimization won\'t help.</p>\n<p><strong>One-time initialization</strong>: Code that runs once at startup. Branch mispredictions here don\'t matter.</p>\n<p><strong>Readability is more important</strong>: When code clarity and maintainability are priorities over micro-optimizations.</p>\n<p><strong>Compiler already optimizes it</strong>: Modern compilers do branch optimization. Manual optimization might be redundant or conflict with compiler decisions.</p>\n<p><strong>No profiling data</strong>: Don\'t optimize branches without data showing they\'re a problem. You might optimize the wrong thing.</p>\n<p><strong>Complex optimization for minimal gain</strong>: If the optimization makes code much more complex for a 1-2% gain, it\'s probably not worth it.</p>\n<p><strong>Why avoid these</strong>: Branch optimization has costs (readability, maintenance). Only apply when benefits clearly outweigh costs.</p>\n<hr>\n<h2 id=\"how-to-measure-and-validate\">How to Measure and Validate</h2>\n<h3 id=\"profiling-tools\">Profiling Tools</h3>\n<p><strong>perf (Linux)</strong>:</p>\n<pre><code class=\"language-bash\"># Measure branch mispredictions\nperf stat -e branches,branch-misses ./your_application\n\n# Detailed branch analysis\nperf record -e branch-misses ./your_application\nperf report\n</code></pre>\n<p><strong>Intel VTune</strong>:</p>\n<ul>\n<li>\"Microarchitecture Exploration\" analysis</li>\n<li>Shows branch misprediction hotspots</li>\n<li>Visual representation of branch efficiency</li>\n</ul>\n<p><strong>Visual Studio Profiler (Windows)</strong>:</p>\n<ul>\n<li>\"CPU Usage\" profiling</li>\n<li>Can show branch misprediction events</li>\n<li>Timeline view of performance issues</li>\n</ul>\n<h3 id=\"key-metrics\">Key Metrics</h3>\n<p><strong>Branch misprediction rate</strong>:</p>\n<ul>\n<li>Formula: <code>(branch-misses / branches)  100%</code></li>\n<li>Target: &lt; 5% for hot code</li>\n<li>Action: If &gt; 10%, investigate and optimize</li>\n</ul>\n<p><strong>Cycles lost to mispredictions</strong>:</p>\n<ul>\n<li>Calculate: <code>branch-misses  misprediction-penalty</code></li>\n<li>Compare to total cycles to see impact</li>\n</ul>\n<p><strong>Instruction-per-cycle (IPC)</strong>:</p>\n<ul>\n<li>Higher is better (more work per cycle)</li>\n<li>Branch mispredictions reduce IPC</li>\n<li>Monitor IPC before/after optimization</li>\n</ul>\n<h3 id=\"detection-strategies\">Detection Strategies</h3>\n<ol>\n<li><strong>Profile your hot paths</strong>: Use profiling tools on code that executes frequently</li>\n<li><strong>Look for high misprediction rates</strong>: Branches with &gt;10% misprediction rate are candidates</li>\n<li><strong>Identify unpredictable patterns</strong>: Look for branches that alternate unpredictably</li>\n<li><strong>Measure before/after</strong>: Profile, optimize, profile again to verify improvement</li>\n</ol>\n<hr>\n<h2 id=\"optimization-techniques\">Optimization Techniques</h2>\n<h3 id=\"technique-1-common-case-first\">Technique 1: Common Case First</h3>\n<p><strong>Principle</strong>: Put the most likely path first in if/else statements.</p>\n<p><strong>Why it works</strong>: CPU predictors often favor the first path (static prediction) or learn that the first path is more common.</p>\n<pre><code class=\"language-csharp\">//  Bad: Rare case first\nif (errorOccurred) {  // Rare case\n    HandleError();\n} else {  // Common case\n    ProcessNormal();\n}\n\n//  Good: Common case first\nif (!errorOccurred) {  // Common case\n    ProcessNormal();\n} else {  // Rare case\n    HandleError();\n}\n</code></pre>\n<p><strong>Benefit</strong>: Predictor learns the common path faster, fewer mispredictions.</p>\n<h3 id=\"technique-2-separate-unpredictable-branches\">Technique 2: Separate Unpredictable Branches</h3>\n<p><strong>Principle</strong>: If you have a loop with an unpredictable branch, separate the filtering from the processing.</p>\n<pre><code class=\"language-csharp\">//  Bad: Unpredictable branch in hot loop\nint count = 0;\nforeach (var item in items) {\n    if (item.IsValid &amp;&amp; item.Value &gt; threshold) {  // Unpredictable\n        Process(item);\n        count++;\n    }\n}\n\n//  Good: Separate filtering (branch once per item)\nvar validItems = items\n    .Where(i =&gt; i.IsValid &amp;&amp; i.Value &gt; threshold)\n    .ToList();  // Branch here, but only once per item\n\nforeach (var item in validItems) {  // No branches in hot loop!\n    Process(item);\n}\n</code></pre>\n<p><strong>Benefit</strong>: Branch happens during filtering (once), not in the hot processing loop (many times).</p>\n<h3 id=\"technique-3-branchless-operations\">Technique 3: Branchless Operations</h3>\n<p><strong>Principle</strong>: Use arithmetic operations instead of branches when possible.</p>\n<pre><code class=\"language-csharp\">//  Bad: Branch in hot loop\nint count = 0;\nforeach (var value in values) {\n    if (value &gt; threshold) {\n        count++;\n    }\n}\n\n//  Good: Branchless\nint count = 0;\nforeach (var value in values) {\n    count += (value &gt; threshold) ? 1 : 0;  // No branch, just arithmetic\n}\n\n// Or using LINQ (compiler may optimize)\nint count = values.Count(v =&gt; v &gt; threshold);\n</code></pre>\n<p><strong>Why it works</strong>: Conditional expressions can be compiled to branchless code (conditional moves, bitwise operations). No branch = no misprediction.</p>\n<p><strong>Trade-off</strong>: Might be slightly less readable. Use when profiling shows the branch is a problem.</p>\n<h3 id=\"technique-4-sort-data-for-predictable-comparisons\">Technique 4: Sort Data for Predictable Comparisons</h3>\n<p><strong>Principle</strong>: When comparing values in a loop, sorted data makes branches predictable.</p>\n<pre><code class=\"language-csharp\">// Unsorted data: comparisons are unpredictable\nforeach (var item in unsortedItems) {\n    if (item.Value &gt; threshold) {  // 50/50 unpredictable\n        Process(item);\n    }\n}\n\n// Sorted data: comparisons are predictable\nArray.Sort(items, (a, b) =&gt; a.Value.CompareTo(b.Value));\nforeach (var item in items) {\n    if (item.Value &gt; threshold) {  // First all false, then all true\n        Process(item);  // Predictable pattern!\n    }\n}\n</code></pre>\n<p><strong>Why it works</strong>: With sorted data, comparisons follow a pattern (all false, then all true). Predictors learn this quickly.</p>\n<p><strong>Trade-off</strong>: Sorting has a cost. Only worth it if you process the data multiple times or if sorting is cheap.</p>\n<h3 id=\"technique-5-use-lookup-tables-instead-of-switches\">Technique 5: Use Lookup Tables Instead of Switches</h3>\n<p><strong>Principle</strong>: For small, dense switch statements, lookup tables can avoid branches.</p>\n<pre><code class=\"language-csharp\">//  Many branches (switch)\nint result;\nswitch (value) {\n    case 0: result = Function0(); break;\n    case 1: result = Function1(); break;\n    case 2: result = Function2(); break;\n    // ... many cases\n}\n\n//  Lookup table (no branches if predictable)\nvar functions = new Func&lt;int&gt;[] { Function0, Function1, Function2, ... };\nint result = functions[value]();  // Direct jump, no branches\n</code></pre>\n<p><strong>Why it works</strong>: Direct array access and function call, no conditional branches to predict.</p>\n<p><strong>Trade-off</strong>: Only works for dense, small ranges. Sparse switches might be better as switches.</p>\n<h3 id=\"technique-6-loop-unrolling-for-predictable-patterns\">Technique 6: Loop Unrolling for Predictable Patterns</h3>\n<p><strong>Principle</strong>: Reduce the number of loop branches by processing multiple items per iteration.</p>\n<pre><code class=\"language-csharp\">// Standard loop: branch every iteration\nfor (int i = 0; i &lt; array.Length; i++) {\n    Process(array[i]);\n}\n\n// Unrolled: branch every 4 iterations\nfor (int i = 0; i &lt; array.Length - 3; i += 4) {\n    Process(array[i]);\n    Process(array[i + 1]);\n    Process(array[i + 2]);\n    Process(array[i + 3]);\n}\n// Handle remainder...\n\n// Or let the compiler do it (modern compilers auto-unroll when beneficial)\n</code></pre>\n<p><strong>Why it works</strong>: Fewer loop branches = fewer misprediction opportunities.</p>\n<p><strong>Trade-off</strong>: More code, might hurt instruction cache. Modern compilers often do this automatically.</p>\n<hr>\n<h2 id=\"example-scenarios\">Example Scenarios</h2>\n<h3 id=\"scenario-1-processing-items-with-filters\">Scenario 1: Processing Items with Filters</h3>\n<p><strong>Problem</strong>: Loop with unpredictable filtering condition.</p>\n<p><strong>Solution</strong>: Separate filtering from processing.</p>\n<pre><code class=\"language-csharp\">// Before: Unpredictable branch in hot loop\npublic int ProcessValidItems(List&lt;Item&gt; items, int threshold) {\n    int count = 0;\n    foreach (var item in items) {\n        if (item.IsValid &amp;&amp; item.Value &gt; threshold) {  // Unpredictable\n            ProcessItem(item);\n            count++;\n        }\n    }\n    return count;\n}\n\n// After: Predictable (or no branches in hot loop)\npublic int ProcessValidItems(List&lt;Item&gt; items, int threshold) {\n    var validItems = items\n        .Where(i =&gt; i.IsValid &amp;&amp; i.Value &gt; threshold)\n        .ToList();\n    \n    foreach (var item in validItems) {  // No branches!\n        ProcessItem(item);\n    }\n    return validItems.Count;\n}\n</code></pre>\n<p><strong>Performance</strong>: 10-20% improvement when the branch was unpredictable.</p>\n<h3 id=\"scenario-2-error-handling\">Scenario 2: Error Handling</h3>\n<p><strong>Problem</strong>: Error checks in hot path, but errors are rare.</p>\n<p><strong>Solution</strong>: Common case first, or extract error handling.</p>\n<pre><code class=\"language-csharp\">// Before: Rare case first\npublic void ProcessRequest(Request req) {\n    if (req == null || !req.IsValid) {  // Rare\n        throw new ArgumentException();\n    }\n    // Common case...\n}\n\n// After: Common case first, or extract\npublic void ProcessRequest(Request req) {\n    if (req != null &amp;&amp; req.IsValid) {  // Common case first\n        // Process...\n    } else {\n        throw new ArgumentException();\n    }\n}\n</code></pre>\n<p><strong>Performance</strong>: 5-10% improvement in hot request processing path.</p>\n<h3 id=\"scenario-3-counting-with-conditions\">Scenario 3: Counting with Conditions</h3>\n<p><strong>Problem</strong>: Branch in counting loop.</p>\n<p><strong>Solution</strong>: Branchless counting.</p>\n<pre><code class=\"language-csharp\">// Before: Branch\nint count = 0;\nforeach (var value in values) {\n    if (value &gt; threshold) {\n        count++;\n    }\n}\n\n// After: Branchless\nint count = 0;\nforeach (var value in values) {\n    count += (value &gt; threshold) ? 1 : 0;\n}\n\n// Or LINQ (compiler optimizes)\nint count = values.Count(v =&gt; v &gt; threshold);\n</code></pre>\n<p><strong>Performance</strong>: 15-25% improvement in tight counting loops.</p>\n','6954500debcfaae683eec9b2','Write code with predictable control flow to minimize branch mispredictions and maximize CPU pipeline efficiency.\n\n\n\n\nExecutive Summary (TL;DR)\n\n\nBranch prediction is a CPU optimization technique that guesses which path code will take at conditional statements (if/else, loops, switches) before the condition is actually evaluated. When predictions are correct, the CPU pipeline continues smoothly. When predictions are wrong, the CPU must discard speculative work and restart, costing 10-20 CPU cycles per misprediction. To optimize for branch prediction, write code with predictable patterns: move common cases first, use branchless operations where possible, and separate unpredictable branches. This typically improves performance by 5-30% in code-heavy with conditionals. The trade-off is that it may reduce code readability and requires profiling to identify problematic branches. Apply this optimization primarily to hot paths in performance-critical code after profiling confirms branch mispredictions.\n\n\n\n\nProblem Context\n\n\nModern CPUs can execute multiple instructions simultaneously through pipeliningthink of it like an assembly line where different stages process different instructions at the same time. However, conditional branches (if/else, loops, switches) create a problem: the CPU doesn\'t know which path to take until it evaluates the condition, but it needs to know now to keep the pipeline full.\n\n\nWhat is pipelining? Imagine a factory assembly line. Instead of building one car completely before starting the next, you have stages: frame, engine, wheels, paint. While one car is being painted, the next is getting wheels, and the one after that is getting an engine. Similarly, a CPU pipeline has stages like: fetch instruction, decode, execute, write result. Modern CPUs have 10-20 pipeline stages, and when full, they can process multiple instructions simultaneously.\n\n\nWhat is a branch? Any point in code where execution can take different paths: if/else statements, loops (should we continue?), switch statements, function calls, etc.\n\n\nThe problem: When the CPU encounters a branch, it must wait for the condition to be evaluated before knowing which instructions to load next. This stalls the pipeline, wasting cycles. To avoid this, CPUs use branch predictionthey guess which path will be taken based on historical patterns.\n\n\n\nCommon Misconceptions\n\n\n\"Branch prediction is something I control\"\n\n\n * Branch prediction happens automatically in CPU hardware. You don\'t explicitly control it, but your code patterns influence how well it works.\n\n\n\"All branches are equally expensive\"\n\n\n * Branches that are predictable (always true, always false, or follow patterns) have near-zero cost. Unpredictable branches cost 10-20 cycles per misprediction.\n\n\n\"Modern CPUs are so fast, branches don\'t matter\"\n\n\n * Modern CPUs are fast because of optimizations like branch prediction. When prediction fails, you still pay the penalty. In tight loops with branches, this adds up quickly.\n\n\n\"I should eliminate all branches\"\n\n\n * Not necessary. Eliminate or optimize unpredictable branches in hot paths. Predictable branches have minimal cost.\n\n\n\nWhy Naive Solutions Fail\n\n\nAdding more branches to \"clarify\" logic\n\n\n * More branches mean more prediction opportunities. If they\'re unpredictable, performance gets worse.\n\n\nComplex nested conditionals\n\n\n * Nested branches compound the problem. If the outer branch is mispredicted, inner branches may be evaluated speculatively (wrong path), wasting more cycles.\n\n\nAssuming the compiler optimizes everything\n\n\n * Compilers do optimize, but they can\'t fix fundamentally unpredictable control flow patterns. The CPU\'s hardware predictor works with patterns, not logic.\n\n\nIgnoring profiling data\n\n\n * Without profiling, you might optimize the wrong branches or miss the ones causing real performance problems.\n\n\n\n\nHow It Works\n\n\n\nUnderstanding CPU Pipelines\n\n\nWhat is a CPU pipeline? Modern CPUs break instruction execution into stages. While one instruction is being executed, the next is being decoded, and the one after that is being fetched from memory. This parallelism allows CPUs to process multiple instructions simultaneously.\n\n\nPipeline stages (simplified):\n\n\n 1. Fetch: Load instruction from memory\n 2. Decode: Determine what the instruction does\n 3. Execute: Perform the operation\n 4. Memory: Access data memory (if needed)\n 5. Write-back: Store result\n\n\nWith a 5-stage pipeline, theoretically 5 instructions can be in flight at once. Modern CPUs have 10-20 stages.\n\n\nThe pipeline problem: To keep the pipeline full, the CPU must fetch the next instruction before the current one finishes. But with branches, the CPU doesn\'t know which instruction comes next until the branch condition is evaluated. This creates a hazarda situation that prevents the pipeline from proceeding.\n\n\nWhat is a hazard? A situation where the pipeline cannot continue because it lacks necessary information. Branch hazards occur because we don\'t know which instruction to fetch next until the branch is resolved.\n\n\n\nBranch Prediction Explained\n\n\nWhat is branch prediction? The CPU guesses which path a branch will take based on:\n\n\n * Static prediction: Simple heuristics (e.g., forward branches are usually not taken, backward branches usually are)\n * Dynamic prediction: Historical patterns (e.g., this branch was taken 90% of the time recently, so predict \"taken\")\n * Branch target prediction: Predicting the target address for indirect branches\n\n\nModern branch predictors use sophisticated algorithms:\n\n\n * Pattern history tables: Track taken/not-taken patterns\n * Branch target buffers: Cache target addresses\n * Global vs. local history: Consider recent branches globally or per-branch\n\n\nWhat happens when prediction is correct?\n\n\n * Pipeline continues smoothly\n * Next instructions are already being processed\n * Near-zero penalty (maybe 1 cycle for prediction logic)\n\n\nWhat happens when prediction is wrong?\n\n\n * CPU must flush the pipelinediscard all speculatively executed instructions\n * Restart from the correct path\n * Branch misprediction penalty: 10-20 cycles on modern CPUs\n * All the work done speculatively is wasted\n\n\nWhy is the penalty so high? The pipeline is deep (10-20 stages). When a misprediction is discovered, all instructions in the wrong path must be discarded, and the pipeline must restart. The deeper the pipeline, the higher the cost.\n\n\n\nReal-World Example: Loop with Condition\n\n\n// Loop that processes items, conditionally incrementing a counter\nint count = 0;\nforeach (var item in items) {\n    if (item.Value > threshold) {  // Branch inside hot loop\n        count++;\n    }\n}\n\n\n\nWhat the CPU sees:\n\n\n * For each iteration, must predict: will item.Value > threshold be true or false?\n * If items are mostly above threshold, predictor learns \"taken\"\n * If items are mostly below, predictor learns \"not taken\"\n * If it\'s random, predictor fails frequently  many mispredictions\n\n\nCost calculation:\n\n\n * 1,000,000 items\n * If branch is 50/50 unpredictable: ~500,000 mispredictions\n * 500,000  15 cycles = 7,500,000 wasted cycles\n * On a 3GHz CPU: 2.5ms wasted (significant in a tight loop!)\n\n\n\n\nWhy This Becomes a Bottleneck\n\n\n\nPipeline Stalls\n\n\nWhat happens: When a branch is mispredicted, the pipeline must flush and restart. During this time, no useful work is done.\n\n\nImpact: In tight loops with unpredictable branches, you might spend 10-20% of CPU time on misprediction penalties instead of actual computation.\n\n\nExample: A loop that processes 1 million items with a 50/50 unpredictable branch might waste 2-3ms just on branch mispredictions. In a function that should complete in 10ms, this is a 20-30% overhead.\n\n\n\nSpeculative Execution Waste\n\n\nWhat is speculative execution? When the CPU predicts a branch will be taken, it speculatively executes instructions from that path before confirming the prediction.\n\n\nThe waste: If prediction is wrong, all speculatively executed instructions are discarded:\n\n\n * Decoded instructions: wasted\n * Executed operations: wasted (unless side-effect free)\n * Cache loads: might still help (prefetched data)\n * Memory bandwidth: partially wasted\n\n\nImpact: Not just the misprediction penalty, but also wasted work and resources.\n\n\n\nCompounding Effects\n\n\nMultiple branches in sequence:\n\n\n * If Branch A is mispredicted, Branch B might be evaluated on the wrong path\n * When the pipeline corrects, Branch B must be re-evaluated\n * Cascading waste from multiple mispredictions\n\n\nNested branches:\n\n\n * Outer branch misprediction causes inner branches to be evaluated speculatively on wrong path\n * More instructions wasted, larger penalty\n\n\nImpact: Complex control flow with unpredictable branches can amplify the problem.\n\n\n\nCache and Memory Effects\n\n\nInstruction cache misses: When a branch is mispredicted, the CPU might load instructions from the wrong path into the instruction cache. When it corrects, it must load the correct instructions, potentially causing cache misses.\n\n\nData prefetching: Speculative execution might prefetch data from the wrong path, wasting memory bandwidth.\n\n\nImpact: Additional penalties beyond the direct misprediction cost.\n\n\n\n\nWhen to Use This Approach\n\n\nHot paths in performance-critical code: Code that executes frequently (millions of times per second) and is on the critical path for latency or throughput.\n\n\nLoops with conditions: Especially tight loops with branches inside. The branch is evaluated many times, so mispredictions compound.\n\n\nAfter profiling confirms branch mispredictions: Use profiling tools (perf, VTune) to identify branches with high misprediction rates. Optimize those, not all branches.\n\n\nCode with predictable patterns: When you can make branches more predictable (e.g., process common cases first, sort data to make comparisons predictable).\n\n\nLatency-sensitive applications: Where consistent, low latency matters (game engines, trading systems, real-time systems).\n\n\nHigh-throughput processing: Where processing speed directly impacts business metrics (data processing, request handling).\n\n\nWhy these scenarios: Branch optimization only matters when branches are frequently executed. In cold code, the optimization cost (readability, maintenance) isn\'t worth it.\n\n\n\n\nWhen Not to Use It\n\n\nCold code: Code that executes rarely. The optimization cost (readability) isn\'t worth the negligible performance benefit.\n\n\nAlready predictable branches: If profiling shows branches are already well-predicted (low misprediction rate), optimization won\'t help.\n\n\nOne-time initialization: Code that runs once at startup. Branch mispredictions here don\'t matter.\n\n\nReadability is more important: When code clarity and maintainability are priorities over micro-optimizations.\n\n\nCompiler already optimizes it: Modern compilers do branch optimization. Manual optimization might be redundant or conflict with compiler decisions.\n\n\nNo profiling data: Don\'t optimize branches without data showing they\'re a problem. You might optimize the wrong thing.\n\n\nComplex optimization for minimal gain: If the optimization makes code much more complex for a 1-2% gain, it\'s probably not worth it.\n\n\nWhy avoid these: Branch optimization has costs (readability, maintenance). Only apply when benefits clearly outweigh costs.\n\n\n\n\nHow to Measure and Validate\n\n\n\nProfiling Tools\n\n\nperf (Linux):\n\n\n# Measure branch mispredictions\nperf stat -e branches,branch-misses ./your_application\n\n# Detailed branch analysis\nperf record -e branch-misses ./your_application\nperf report\n\n\n\nIntel VTune:\n\n\n * \"Microarchitecture Exploration\" analysis\n * Shows branch misprediction hotspots\n * Visual representation of branch efficiency\n\n\nVisual Studio Profiler (Windows):\n\n\n * \"CPU Usage\" profiling\n * Can show branch misprediction events\n * Timeline view of performance issues\n\n\n\nKey Metrics\n\n\nBranch misprediction rate:\n\n\n * Formula: (branch-misses / branches)  100%\n * Target: < 5% for hot code\n * Action: If > 10%, investigate and optimize\n\n\nCycles lost to mispredictions:\n\n\n * Calculate: branch-misses  misprediction-penalty\n * Compare to total cycles to see impact\n\n\nInstruction-per-cycle (IPC):\n\n\n * Higher is better (more work per cycle)\n * Branch mispredictions reduce IPC\n * Monitor IPC before/after optimization\n\n\n\nDetection Strategies\n\n\n 1. Profile your hot paths: Use profiling tools on code that executes frequently\n 2. Look for high misprediction rates: Branches with >10% misprediction rate are candidates\n 3. Identify unpredictable patterns: Look for branches that alternate unpredictably\n 4. Measure before/after: Profile, optimize, profile again to verify improvement\n\n\n\n\nOptimization Techniques\n\n\n\nTechnique 1: Common Case First\n\n\nPrinciple: Put the most likely path first in if/else statements.\n\n\nWhy it works: CPU predictors often favor the first path (static prediction) or learn that the first path is more common.\n\n\n//  Bad: Rare case first\nif (errorOccurred) {  // Rare case\n    HandleError();\n} else {  // Common case\n    ProcessNormal();\n}\n\n//  Good: Common case first\nif (!errorOccurred) {  // Common case\n    ProcessNormal();\n} else {  // Rare case\n    HandleError();\n}\n\n\n\nBenefit: Predictor learns the common path faster, fewer mispredictions.\n\n\n\nTechnique 2: Separate Unpredictable Branches\n\n\nPrinciple: If you have a loop with an unpredictable branch, separate the filtering from the processing.\n\n\n//  Bad: Unpredictable branch in hot loop\nint count = 0;\nforeach (var item in items) {\n    if (item.IsValid && item.Value > threshold) {  // Unpredictable\n        Process(item);\n        count++;\n    }\n}\n\n//  Good: Separate filtering (branch once per item)\nvar validItems = items\n    .Where(i => i.IsValid && i.Value > threshold)\n    .ToList();  // Branch here, but only once per item\n\nforeach (var item in validItems) {  // No branches in hot loop!\n    Process(item);\n}\n\n\n\nBenefit: Branch happens during filtering (once), not in the hot processing loop (many times).\n\n\n\nTechnique 3: Branchless Operations\n\n\nPrinciple: Use arithmetic operations instead of branches when possible.\n\n\n//  Bad: Branch in hot loop\nint count = 0;\nforeach (var value in values) {\n    if (value > threshold) {\n        count++;\n    }\n}\n\n//  Good: Branchless\nint count = 0;\nforeach (var value in values) {\n    count += (value > threshold) ? 1 : 0;  // No branch, just arithmetic\n}\n\n// Or using LINQ (compiler may optimize)\nint count = values.Count(v => v > threshold);\n\n\n\nWhy it works: Conditional expressions can be compiled to branchless code (conditional moves, bitwise operations). No branch = no misprediction.\n\n\nTrade-off: Might be slightly less readable. Use when profiling shows the branch is a problem.\n\n\n\nTechnique 4: Sort Data for Predictable Comparisons\n\n\nPrinciple: When comparing values in a loop, sorted data makes branches predictable.\n\n\n// Unsorted data: comparisons are unpredictable\nforeach (var item in unsortedItems) {\n    if (item.Value > threshold) {  // 50/50 unpredictable\n        Process(item);\n    }\n}\n\n// Sorted data: comparisons are predictable\nArray.Sort(items, (a, b) => a.Value.CompareTo(b.Value));\nforeach (var item in items) {\n    if (item.Value > threshold) {  // First all false, then all true\n        Process(item);  // Predictable pattern!\n    }\n}\n\n\n\nWhy it works: With sorted data, comparisons follow a pattern (all false, then all true). Predictors learn this quickly.\n\n\nTrade-off: Sorting has a cost. Only worth it if you process the data multiple times or if sorting is cheap.\n\n\n\nTechnique 5: Use Lookup Tables Instead of Switches\n\n\nPrinciple: For small, dense switch statements, lookup tables can avoid branches.\n\n\n//  Many branches (switch)\nint result;\nswitch (value) {\n    case 0: result = Function0(); break;\n    case 1: result = Function1(); break;\n    case 2: result = Function2(); break;\n    // ... many cases\n}\n\n//  Lookup table (no branches if predictable)\nvar functions = new Func<int>[] { Function0, Function1, Function2, ... };\nint result = functions[value]();  // Direct jump, no branches\n\n\n\nWhy it works: Direct array access and function call, no conditional branches to predict.\n\n\nTrade-off: Only works for dense, small ranges. Sparse switches might be better as switches.\n\n\n\nTechnique 6: Loop Unrolling for Predictable Patterns\n\n\nPrinciple: Reduce the number of loop branches by processing multiple items per iteration.\n\n\n// Standard loop: branch every iteration\nfor (int i = 0; i < array.Length; i++) {\n    Process(array[i]);\n}\n\n// Unrolled: branch every 4 iterations\nfor (int i = 0; i < array.Length - 3; i += 4) {\n    Process(array[i]);\n    Process(array[i + 1]);\n    Process(array[i + 2]);\n    Process(array[i + 3]);\n}\n// Handle remainder...\n\n// Or let the compiler do it (modern compilers auto-unroll when beneficial)\n\n\n\nWhy it works: Fewer loop branches = fewer misprediction opportunities.\n\n\nTrade-off: More code, might hurt instruction cache. Modern compilers often do this automatically.\n\n\n\n\nExample Scenarios\n\n\n\nScenario 1: Processing Items with Filters\n\n\nProblem: Loop with unpredictable filtering condition.\n\n\nSolution: Separate filtering from processing.\n\n\n// Before: Unpredictable branch in hot loop\npublic int ProcessValidItems(List<Item> items, int threshold) {\n    int count = 0;\n    foreach (var item in items) {\n        if (item.IsValid && item.Value > threshold) {  // Unpredictable\n            ProcessItem(item);\n            count++;\n        }\n    }\n    return count;\n}\n\n// After: Predictable (or no branches in hot loop)\npublic int ProcessValidItems(List<Item> items, int threshold) {\n    var validItems = items\n        .Where(i => i.IsValid && i.Value > threshold)\n        .ToList();\n    \n    foreach (var item in validItems) {  // No branches!\n        ProcessItem(item);\n    }\n    return validItems.Count;\n}\n\n\n\nPerformance: 10-20% improvement when the branch was unpredictable.\n\n\n\nScenario 2: Error Handling\n\n\nProblem: Error checks in hot path, but errors are rare.\n\n\nSolution: Common case first, or extract error handling.\n\n\n// Before: Rare case first\npublic void ProcessRequest(Request req) {\n    if (req == null || !req.IsValid) {  // Rare\n        throw new ArgumentException();\n    }\n    // Common case...\n}\n\n// After: Common case first, or extract\npublic void ProcessRequest(Request req) {\n    if (req != null && req.IsValid) {  // Common case first\n        // Process...\n    } else {\n        throw new ArgumentException();\n    }\n}\n\n\n\nPerformance: 5-10% improvement in hot request processing path.\n\n\n\nScenario 3: Counting with Conditions\n\n\nProblem: Branch in counting loop.\n\n\nSolution: Branchless counting.\n\n\n// Before: Branch\nint count = 0;\nforeach (var value in values) {\n    if (value > threshold) {\n        count++;\n    }\n}\n\n// After: Branchless\nint count = 0;\nforeach (var value in values) {\n    count += (value > threshold) ? 1 : 0;\n}\n\n// Or LINQ (compiler optimizes)\nint count = values.Count(v => v > threshold);\n\n\n\nPerformance: 15-25% improvement in tight counting loops.\n',NULL,0,'post','published',NULL,'public','all','2025-12-30 22:19:57','2025-12-30 22:20:36','2025-12-30 22:19:57','694f2665c3e44e650f0e5dd8',NULL,NULL,NULL,NULL,NULL,NULL,1),('6956d503ebcfaae683eec9c9','4420bf87-f921-4374-8102-a6057ad02fdc','Avoid Busy-Wait Loops: Use Proper Synchronization Primitives','avoid-busy-wait-loops-use-proper-synchronization-primitives',NULL,'{\"root\":{\"children\":[{\"type\":\"markdown\",\"version\":1,\"markdown\":\"**Replace CPU-wasting busy-wait loops with appropriate synchronization mechanisms to free CPU cycles for useful work and improve system efficiency.**\\n\\n---\\n\\n## Executive Summary (TL;DR)\\n\\nBusy-wait loops continuously consume CPU cycles while waiting for a condition to become true, without yielding control to the operating system scheduler. This wastes CPU resources, increases power consumption, prevents other threads from running, and can degrade overall system performance. The solution is to use proper synchronization primitives (events, semaphores, condition variables) that allow the thread to sleep until the condition is met, or use async/await for non-blocking waits. For very short waits (microseconds), `SpinWait` provides an optimized alternative. Eliminating busy-wait loops can free 10-100% of CPU depending on the situation. Always prefer blocking synchronization or async patterns over busy-waiting, except in extremely latency-critical scenarios with guaranteed short wait times.\\n\\n---\\n\\n## Problem Context\\n\\n**What is a busy-wait loop?** A loop that repeatedly checks a condition in a tight cycle, consuming CPU continuously while waiting. The thread never gives up the CPU, so it burns cycles doing nothing useful.\\n\\n**Example of busy-wait**:\\n```csharp\\nwhile (!condition) {\\n    // Empty loop - just checking condition over and over\\n    // CPU is at 100% doing nothing useful!\\n}\\n```\\n\\n**The problem**: While the thread is busy-waiting, it:\\n- Consumes CPU cycles that could be used by other threads\\n- Wastes power (especially important in mobile/embedded systems)\\n- Prevents the operating system from scheduling other work\\n- Can cause thermal issues (CPU heating up from constant work)\\n- May starve other threads of CPU time\\n\\n### Key Terms Explained\\n**Context Switch**: When the OS scheduler stops one thread and starts another. This has overhead (saving/restoring thread state), but allows the CPU to do useful work instead of busy-waiting.\\n\\n**Yield**: When a thread voluntarily gives up the CPU, allowing the scheduler to run other threads. Threads can yield explicitly or by calling blocking operations.\\n\\n**Blocking**: When a thread waits for something (I/O, synchronization event) and is suspended by the OS. The thread doesn\'t consume CPU while blocked.\\n\\n### Why Naive Solutions Fail\\n\\n**Adding `Thread.Sleep()` to busy-wait loops**\\n```csharp\\nwhile (!condition) {\\n    Thread.Sleep(1);  // Sleep 1ms\\n}\\n```\\n- **Why it\'s better but still wrong**: Reduces CPU usage but introduces unnecessary latency. The thread wakes up every millisecond to check, even if the condition changed immediately after sleeping. Still not ideal.\\n\\n**Using very short sleeps**\\n```csharp\\nwhile (!condition) {\\n    Thread.Sleep(0);  // Yield to other threads\\n}\\n```\\n- **Why it\'s better**: Yields to other threads, but still has overhead. Better than pure busy-wait, but proper synchronization is cleaner and more efficient.\\n\\n**Assuming the problem will fix itself**\\n- **Why it fails**: Busy-wait doesn\'t fix itself. It continues wasting resources until you fix the code.\\n\\n---\\n\\n## How It Works\\n\\n### Understanding Thread Scheduling\\n\\n**What happens when a thread runs**: The OS scheduler assigns a CPU core to the thread. The thread executes instructions until:\\n- It completes its work\\n- It yields the CPU (explicitly or by blocking)\\n- The scheduler preempts it (time slice expires, higher priority thread needs CPU)\\n\\n**What happens during busy-wait**: The thread never yields. It executes the loop instructions continuously:\\n1. Check condition (read memory/register)\\n2. Compare with expected value\\n3. Branch back to step 1 if false\\n4. Repeat millions of times per second\\n\\n**CPU perspective**: The CPU executes these instructions at full speed (billions per second). Even though the thread isn\'t doing useful work, the CPU is working hard, consuming power and generating heat.\\n\\n### Operating System Behavior\\n\\n**When a thread blocks** (waits on a synchronization primitive):\\n1. Thread calls a blocking operation (e.g., `Wait()`, `Sleep()`)\\n2. OS scheduler removes thread from \\\"runnable\\\" queue\\n3. Thread is marked as \\\"waiting\\\" and doesn\'t consume CPU\\n4. OS schedules other threads to use the CPU\\n5. When the condition is met, OS moves thread back to \\\"runnable\\\" queue\\n6. Thread eventually gets scheduled and continues execution\\n\\n**Context switch overhead**: Switching threads takes ~1-10 microseconds on modern systems. This is minimal compared to wasting thousands of cycles in busy-wait.\\n\\n**Wake-up latency**: Modern OS kernels can wake waiting threads very quickly (often microseconds). Proper synchronization primitives use efficient kernel mechanisms (events, futexes) for fast wake-up.\\n\\n### Synchronization Primitives Explained\\n\\n**Event/Semaphore/Mutex**: These are OS-provided synchronization mechanisms that:\\n- Allow threads to wait without consuming CPU\\n- Wake threads efficiently when conditions change\\n- Use kernel mechanisms for fast signaling\\n\\n**How they work**:\\n1. Thread calls `Wait()`  OS suspends thread, doesn\'t consume CPU\\n2. Another thread calls `Set()`/`Signal()`  OS wakes waiting thread(s)\\n3. Woken thread resumes execution\\n\\n**Performance**: Wake-up is typically microseconds. The overhead is negligible compared to busy-wait waste.\\n\\n---\\n\\n## Why This Becomes a Bottleneck\\n\\n### CPU Starvation\\n\\n**What happens**: Busy-wait threads consume CPU cores, preventing other threads from running. If you have N CPU cores and M threads busy-waiting (where M > N), some threads can\'t run at all.\\n\\n**Impact**: System throughput decreases. Actual work threads compete for fewer available cores, causing slowdowns.\\n\\n**Example**: 8-core server, 10 threads doing real work, 5 threads busy-waiting. The 5 busy-wait threads consume 5 cores continuously. Only 3 cores available for the 10 work threads  severe contention and poor performance.\\n\\n### Power Consumption\\n\\n**What happens**: CPUs consume more power when executing instructions. Busy-wait loops execute instructions continuously, keeping the CPU active and consuming power.\\n\\n**Impact**: \\n- Higher electricity costs in data centers\\n- Reduced battery life in mobile devices\\n- Thermal issues (CPU heating up)\\n- Need for better cooling systems\\n\\n**Why it\'s significant**: In a data center with thousands of servers, busy-wait loops can significantly increase power consumption and cooling costs.\\n\\n### Thermal Throttling\\n\\n**What happens**: When CPUs get too hot, they reduce clock frequency to cool down (thermal throttling). Busy-wait loops generate heat, potentially triggering throttling.\\n\\n**Impact**: Reduced CPU performance system-wide. Even threads doing useful work slow down because the CPU is throttled.\\n\\n### Scheduler Inefficiency\\n\\n**What happens**: The OS scheduler can\'t make optimal scheduling decisions when threads don\'t yield. It can\'t balance load effectively or prioritize important work.\\n\\n**Impact**: Suboptimal resource utilization, longer response times for important work, poor system responsiveness.\\n\\n### False Parallelism\\n\\n**What happens**: Multiple threads busy-waiting appear to be \\\"running\\\" (they consume CPU), but they\'re not doing useful work. This creates an illusion of parallelism without actual progress.\\n\\n**Impact**: System appears busy but throughput is low. Monitoring tools show high CPU usage but low actual work completion.\\n\\n---\\n\\n## Optimization Techniques\\n\\n### Technique 1: Use Events/Semaphores for Coordination\\n\\n**When**: Threads need to wait for conditions or events from other threads.\\n\\n```csharp\\n//  Bad: Busy-wait\\npublic class BadWait {\\n    private bool _flag = false;\\n    \\n    public void WaitForFlag() {\\n        while (!_flag) {\\n            // Wasting CPU!\\n        }\\n    }\\n    \\n    public void SetFlag() {\\n        _flag = true;\\n    }\\n}\\n\\n//  Good: Use ManualResetEventSlim\\npublic class GoodWait {\\n    private readonly ManualResetEventSlim _event = new ManualResetEventSlim(false);\\n    \\n    public void WaitForFlag() {\\n        _event.Wait();  // Blocks, doesn\'t consume CPU\\n    }\\n    \\n    public void SetFlag() {\\n        _event.Set();  // Wakes waiting thread\\n    }\\n}\\n```\\n\\n**Why it works**: `ManualResetEventSlim` uses efficient OS mechanisms. Thread blocks (doesn\'t consume CPU) until `Set()` is called, then wakes quickly (microseconds).\\n\\n**Performance**: Eliminates CPU waste. Wake-up latency is microseconds, negligible for most scenarios.\\n\\n### Technique 2: Use async/await for I/O\\n\\n**When**: Waiting for I/O operations (file, network, database).\\n\\n```csharp\\n//  Bad: Busy-wait for I/O\\npublic void ProcessFile(string path) {\\n    while (!File.Exists(path)) {\\n        // Wasting CPU waiting for file!\\n    }\\n    // Process file...\\n}\\n\\n//  Good: Use async/await\\npublic async Task ProcessFileAsync(string path) {\\n    // Wait for file without blocking thread\\n    while (!File.Exists(path)) {\\n        await Task.Delay(100);  // Yield to other work\\n    }\\n    // Process file...\\n}\\n\\n//  Better: Use FileSystemWatcher or proper async I/O\\npublic async Task ProcessFileAsync(string path) {\\n    using var fileStream = File.OpenRead(path);\\n    // Use async I/O methods\\n    var buffer = new byte[4096];\\n    await fileStream.ReadAsync(buffer, 0, buffer.Length);\\n}\\n```\\n\\n**Why it works**: `async/await` doesn\'t block threads. The thread can do other work while waiting for I/O. When I/O completes, execution resumes.\\n\\n**Performance**: Doesn\'t waste threads on I/O waits. Allows more concurrent operations.\\n\\n### Technique 3: Use SpinWait for Very Short Waits\\n\\n**When**: Wait time is guaranteed to be extremely short (nanoseconds to microseconds).\\n\\n```csharp\\n//  For very short waits, use SpinWait\\npublic class OptimizedWait {\\n    private volatile bool _flag = false;\\n    \\n    public void WaitForFlag() {\\n        var spinWait = new SpinWait();\\n        while (!_flag) {\\n            spinWait.SpinOnce();  // Optimized for short waits\\n            // After some spins, yields to other threads\\n        }\\n    }\\n}\\n```\\n\\n**Why it works**: `SpinWait` uses CPU hints and gradually increases back-off. For very short waits, it avoids context switch overhead. For longer waits, it yields to prevent waste.\\n\\n**When to use**: Only when you\'re absolutely certain the wait will be nanoseconds/microseconds. For longer waits, use blocking primitives.\\n\\n**Trade-off**: Still consumes some CPU, but optimized. Use sparingly.\\n\\n### Technique 4: Use TaskCompletionSource for Async Coordination\\n\\n**When**: Coordinating async operations without blocking threads.\\n\\n```csharp\\n//  Good: TaskCompletionSource for async coordination\\npublic class AsyncWait {\\n    private readonly TaskCompletionSource<bool> _tcs = new TaskCompletionSource<bool>();\\n    \\n    public async Task WaitForFlagAsync() {\\n        await _tcs.Task;  // Non-blocking wait\\n    }\\n    \\n    public void SetFlag() {\\n        _tcs.SetResult(true);  // Completes the task\\n    }\\n}\\n```\\n\\n**Why it works**: `TaskCompletionSource` creates a task that completes when `SetResult()` is called. `await` doesn\'t block threadsit schedules continuation when the task completes.\\n\\n**Performance**: No thread blocking, efficient scheduling, allows high concurrency.\\n\\n### Technique 5: Use Producer-Consumer Patterns\\n\\n**When**: Threads need to wait for work items.\\n\\n```csharp\\n//  Good: Use BlockingCollection for producer-consumer\\npublic class WorkQueue {\\n    private readonly BlockingCollection<WorkItem> _queue = new BlockingCollection<WorkItem>();\\n    \\n    // Producer\\n    public void AddWork(WorkItem item) {\\n        _queue.Add(item);  // Wakes waiting consumers\\n    }\\n    \\n    // Consumer\\n    public WorkItem GetWork() {\\n        return _queue.Take();  // Blocks until work available\\n    }\\n}\\n```\\n\\n**Why it works**: `BlockingCollection` uses efficient blocking. Consumers block (no CPU waste) until producers add work, then wake immediately.\\n\\n**Performance**: Efficient coordination without busy-wait.\\n\\n---\\n\\n## Example Scenarios\\n\\n### Scenario 1: Waiting for a Flag\\n\\n**Problem**: Thread needs to wait for a boolean flag to become true.\\n\\n**Bad solution**: Busy-wait loop.\\n\\n**Good solution**: Use `ManualResetEventSlim` or `TaskCompletionSource`.\\n\\n```csharp\\n//  Recommended\\nprivate readonly ManualResetEventSlim _ready = new ManualResetEventSlim(false);\\n\\npublic void WaitForReady() {\\n    _ready.Wait();  // Blocks efficiently\\n}\\n\\npublic void SetReady() {\\n    _ready.Set();  // Wakes waiting threads\\n}\\n```\\n\\n**Performance impact**: Eliminates CPU waste. Thread blocks until ready, then wakes in microseconds.\\n\\n### Scenario 2: Waiting for I/O\\n\\n**Problem**: Waiting for file to be created or network response.\\n\\n**Bad solution**: Busy-wait checking file existence or response.\\n\\n**Good solution**: Use async/await with proper async I/O.\\n\\n```csharp\\n//  Recommended\\npublic async Task<string> ReadFileAsync(string path) {\\n    using var reader = new StreamReader(path);\\n    return await reader.ReadToEndAsync();  // Non-blocking I/O\\n}\\n```\\n\\n**Performance impact**: Doesn\'t block threads. Allows thousands of concurrent I/O operations.\\n\\n### Scenario 3: Producer-Consumer Pattern\\n\\n**Problem**: Consumer threads need to wait for work items.\\n\\n**Bad solution**: Busy-wait checking if queue has items.\\n\\n**Good solution**: Use `BlockingCollection` or channels.\\n\\n```csharp\\n//  Recommended\\nprivate readonly BlockingCollection<WorkItem> _workQueue = new BlockingCollection<WorkItem>();\\n\\npublic void ProcessWork() {\\n    foreach (var item in _workQueue.GetConsumingEnumerable()) {\\n        Process(item);  // Blocks until work available\\n    }\\n}\\n```\\n\\n**Performance impact**: Efficient coordination. Consumers block until work available, no CPU waste.\\n\\n---\\n\\n## Summary and Key Takeaways\\n\\nBusy-wait loops continuously consume CPU cycles while waiting for conditions, wasting resources and preventing other threads from running. Replace them with proper synchronization primitives (events, semaphores, async/await) that allow threads to block efficiently until conditions are met.\\n\\n**Core Principle**: Never busy-wait unless you\'re absolutely certain the wait will be nanoseconds. Use blocking synchronization or async patterns for all other cases.\\n\\n**Main Trade-off**: Tiny wake-up latency (microseconds) for blocking vs. massive CPU waste for busy-wait. The trade-off almost always favors blocking.\"}],\"direction\":null,\"format\":\"\",\"indent\":0,\"type\":\"root\",\"version\":1}}','<p><strong>Replace CPU-wasting busy-wait loops with appropriate synchronization mechanisms to free CPU cycles for useful work and improve system efficiency.</strong></p>\n<hr>\n<h2 id=\"executive-summary-tldr\">Executive Summary (TL;DR)</h2>\n<p>Busy-wait loops continuously consume CPU cycles while waiting for a condition to become true, without yielding control to the operating system scheduler. This wastes CPU resources, increases power consumption, prevents other threads from running, and can degrade overall system performance. The solution is to use proper synchronization primitives (events, semaphores, condition variables) that allow the thread to sleep until the condition is met, or use async/await for non-blocking waits. For very short waits (microseconds), <code>SpinWait</code> provides an optimized alternative. Eliminating busy-wait loops can free 10-100% of CPU depending on the situation. Always prefer blocking synchronization or async patterns over busy-waiting, except in extremely latency-critical scenarios with guaranteed short wait times.</p>\n<hr>\n<h2 id=\"problem-context\">Problem Context</h2>\n<p><strong>What is a busy-wait loop?</strong> A loop that repeatedly checks a condition in a tight cycle, consuming CPU continuously while waiting. The thread never gives up the CPU, so it burns cycles doing nothing useful.</p>\n<p><strong>Example of busy-wait</strong>:</p>\n<pre><code class=\"language-csharp\">while (!condition) {\n    // Empty loop - just checking condition over and over\n    // CPU is at 100% doing nothing useful!\n}\n</code></pre>\n<p><strong>The problem</strong>: While the thread is busy-waiting, it:</p>\n<ul>\n<li>Consumes CPU cycles that could be used by other threads</li>\n<li>Wastes power (especially important in mobile/embedded systems)</li>\n<li>Prevents the operating system from scheduling other work</li>\n<li>Can cause thermal issues (CPU heating up from constant work)</li>\n<li>May starve other threads of CPU time</li>\n</ul>\n<h3 id=\"key-terms-explained\">Key Terms Explained</h3>\n<p><strong>Context Switch</strong>: When the OS scheduler stops one thread and starts another. This has overhead (saving/restoring thread state), but allows the CPU to do useful work instead of busy-waiting.</p>\n<p><strong>Yield</strong>: When a thread voluntarily gives up the CPU, allowing the scheduler to run other threads. Threads can yield explicitly or by calling blocking operations.</p>\n<p><strong>Blocking</strong>: When a thread waits for something (I/O, synchronization event) and is suspended by the OS. The thread doesn\'t consume CPU while blocked.</p>\n<h3 id=\"why-naive-solutions-fail\">Why Naive Solutions Fail</h3>\n<p><strong>Adding <code>Thread.Sleep()</code> to busy-wait loops</strong></p>\n<pre><code class=\"language-csharp\">while (!condition) {\n    Thread.Sleep(1);  // Sleep 1ms\n}\n</code></pre>\n<ul>\n<li><strong>Why it\'s better but still wrong</strong>: Reduces CPU usage but introduces unnecessary latency. The thread wakes up every millisecond to check, even if the condition changed immediately after sleeping. Still not ideal.</li>\n</ul>\n<p><strong>Using very short sleeps</strong></p>\n<pre><code class=\"language-csharp\">while (!condition) {\n    Thread.Sleep(0);  // Yield to other threads\n}\n</code></pre>\n<ul>\n<li><strong>Why it\'s better</strong>: Yields to other threads, but still has overhead. Better than pure busy-wait, but proper synchronization is cleaner and more efficient.</li>\n</ul>\n<p><strong>Assuming the problem will fix itself</strong></p>\n<ul>\n<li><strong>Why it fails</strong>: Busy-wait doesn\'t fix itself. It continues wasting resources until you fix the code.</li>\n</ul>\n<hr>\n<h2 id=\"how-it-works\">How It Works</h2>\n<h3 id=\"understanding-thread-scheduling\">Understanding Thread Scheduling</h3>\n<p><strong>What happens when a thread runs</strong>: The OS scheduler assigns a CPU core to the thread. The thread executes instructions until:</p>\n<ul>\n<li>It completes its work</li>\n<li>It yields the CPU (explicitly or by blocking)</li>\n<li>The scheduler preempts it (time slice expires, higher priority thread needs CPU)</li>\n</ul>\n<p><strong>What happens during busy-wait</strong>: The thread never yields. It executes the loop instructions continuously:</p>\n<ol>\n<li>Check condition (read memory/register)</li>\n<li>Compare with expected value</li>\n<li>Branch back to step 1 if false</li>\n<li>Repeat millions of times per second</li>\n</ol>\n<p><strong>CPU perspective</strong>: The CPU executes these instructions at full speed (billions per second). Even though the thread isn\'t doing useful work, the CPU is working hard, consuming power and generating heat.</p>\n<h3 id=\"operating-system-behavior\">Operating System Behavior</h3>\n<p><strong>When a thread blocks</strong> (waits on a synchronization primitive):</p>\n<ol>\n<li>Thread calls a blocking operation (e.g., <code>Wait()</code>, <code>Sleep()</code>)</li>\n<li>OS scheduler removes thread from \"runnable\" queue</li>\n<li>Thread is marked as \"waiting\" and doesn\'t consume CPU</li>\n<li>OS schedules other threads to use the CPU</li>\n<li>When the condition is met, OS moves thread back to \"runnable\" queue</li>\n<li>Thread eventually gets scheduled and continues execution</li>\n</ol>\n<p><strong>Context switch overhead</strong>: Switching threads takes ~1-10 microseconds on modern systems. This is minimal compared to wasting thousands of cycles in busy-wait.</p>\n<p><strong>Wake-up latency</strong>: Modern OS kernels can wake waiting threads very quickly (often microseconds). Proper synchronization primitives use efficient kernel mechanisms (events, futexes) for fast wake-up.</p>\n<h3 id=\"synchronization-primitives-explained\">Synchronization Primitives Explained</h3>\n<p><strong>Event/Semaphore/Mutex</strong>: These are OS-provided synchronization mechanisms that:</p>\n<ul>\n<li>Allow threads to wait without consuming CPU</li>\n<li>Wake threads efficiently when conditions change</li>\n<li>Use kernel mechanisms for fast signaling</li>\n</ul>\n<p><strong>How they work</strong>:</p>\n<ol>\n<li>Thread calls <code>Wait()</code>  OS suspends thread, doesn\'t consume CPU</li>\n<li>Another thread calls <code>Set()</code>/<code>Signal()</code>  OS wakes waiting thread(s)</li>\n<li>Woken thread resumes execution</li>\n</ol>\n<p><strong>Performance</strong>: Wake-up is typically microseconds. The overhead is negligible compared to busy-wait waste.</p>\n<hr>\n<h2 id=\"why-this-becomes-a-bottleneck\">Why This Becomes a Bottleneck</h2>\n<h3 id=\"cpu-starvation\">CPU Starvation</h3>\n<p><strong>What happens</strong>: Busy-wait threads consume CPU cores, preventing other threads from running. If you have N CPU cores and M threads busy-waiting (where M &gt; N), some threads can\'t run at all.</p>\n<p><strong>Impact</strong>: System throughput decreases. Actual work threads compete for fewer available cores, causing slowdowns.</p>\n<p><strong>Example</strong>: 8-core server, 10 threads doing real work, 5 threads busy-waiting. The 5 busy-wait threads consume 5 cores continuously. Only 3 cores available for the 10 work threads  severe contention and poor performance.</p>\n<h3 id=\"power-consumption\">Power Consumption</h3>\n<p><strong>What happens</strong>: CPUs consume more power when executing instructions. Busy-wait loops execute instructions continuously, keeping the CPU active and consuming power.</p>\n<p><strong>Impact</strong>:</p>\n<ul>\n<li>Higher electricity costs in data centers</li>\n<li>Reduced battery life in mobile devices</li>\n<li>Thermal issues (CPU heating up)</li>\n<li>Need for better cooling systems</li>\n</ul>\n<p><strong>Why it\'s significant</strong>: In a data center with thousands of servers, busy-wait loops can significantly increase power consumption and cooling costs.</p>\n<h3 id=\"thermal-throttling\">Thermal Throttling</h3>\n<p><strong>What happens</strong>: When CPUs get too hot, they reduce clock frequency to cool down (thermal throttling). Busy-wait loops generate heat, potentially triggering throttling.</p>\n<p><strong>Impact</strong>: Reduced CPU performance system-wide. Even threads doing useful work slow down because the CPU is throttled.</p>\n<h3 id=\"scheduler-inefficiency\">Scheduler Inefficiency</h3>\n<p><strong>What happens</strong>: The OS scheduler can\'t make optimal scheduling decisions when threads don\'t yield. It can\'t balance load effectively or prioritize important work.</p>\n<p><strong>Impact</strong>: Suboptimal resource utilization, longer response times for important work, poor system responsiveness.</p>\n<h3 id=\"false-parallelism\">False Parallelism</h3>\n<p><strong>What happens</strong>: Multiple threads busy-waiting appear to be \"running\" (they consume CPU), but they\'re not doing useful work. This creates an illusion of parallelism without actual progress.</p>\n<p><strong>Impact</strong>: System appears busy but throughput is low. Monitoring tools show high CPU usage but low actual work completion.</p>\n<hr>\n<h2 id=\"optimization-techniques\">Optimization Techniques</h2>\n<h3 id=\"technique-1-use-eventssemaphores-for-coordination\">Technique 1: Use Events/Semaphores for Coordination</h3>\n<p><strong>When</strong>: Threads need to wait for conditions or events from other threads.</p>\n<pre><code class=\"language-csharp\">//  Bad: Busy-wait\npublic class BadWait {\n    private bool _flag = false;\n    \n    public void WaitForFlag() {\n        while (!_flag) {\n            // Wasting CPU!\n        }\n    }\n    \n    public void SetFlag() {\n        _flag = true;\n    }\n}\n\n//  Good: Use ManualResetEventSlim\npublic class GoodWait {\n    private readonly ManualResetEventSlim _event = new ManualResetEventSlim(false);\n    \n    public void WaitForFlag() {\n        _event.Wait();  // Blocks, doesn\'t consume CPU\n    }\n    \n    public void SetFlag() {\n        _event.Set();  // Wakes waiting thread\n    }\n}\n</code></pre>\n<p><strong>Why it works</strong>: <code>ManualResetEventSlim</code> uses efficient OS mechanisms. Thread blocks (doesn\'t consume CPU) until <code>Set()</code> is called, then wakes quickly (microseconds).</p>\n<p><strong>Performance</strong>: Eliminates CPU waste. Wake-up latency is microseconds, negligible for most scenarios.</p>\n<h3 id=\"technique-2-use-asyncawait-for-io\">Technique 2: Use async/await for I/O</h3>\n<p><strong>When</strong>: Waiting for I/O operations (file, network, database).</p>\n<pre><code class=\"language-csharp\">//  Bad: Busy-wait for I/O\npublic void ProcessFile(string path) {\n    while (!File.Exists(path)) {\n        // Wasting CPU waiting for file!\n    }\n    // Process file...\n}\n\n//  Good: Use async/await\npublic async Task ProcessFileAsync(string path) {\n    // Wait for file without blocking thread\n    while (!File.Exists(path)) {\n        await Task.Delay(100);  // Yield to other work\n    }\n    // Process file...\n}\n\n//  Better: Use FileSystemWatcher or proper async I/O\npublic async Task ProcessFileAsync(string path) {\n    using var fileStream = File.OpenRead(path);\n    // Use async I/O methods\n    var buffer = new byte[4096];\n    await fileStream.ReadAsync(buffer, 0, buffer.Length);\n}\n</code></pre>\n<p><strong>Why it works</strong>: <code>async/await</code> doesn\'t block threads. The thread can do other work while waiting for I/O. When I/O completes, execution resumes.</p>\n<p><strong>Performance</strong>: Doesn\'t waste threads on I/O waits. Allows more concurrent operations.</p>\n<h3 id=\"technique-3-use-spinwait-for-very-short-waits\">Technique 3: Use SpinWait for Very Short Waits</h3>\n<p><strong>When</strong>: Wait time is guaranteed to be extremely short (nanoseconds to microseconds).</p>\n<pre><code class=\"language-csharp\">//  For very short waits, use SpinWait\npublic class OptimizedWait {\n    private volatile bool _flag = false;\n    \n    public void WaitForFlag() {\n        var spinWait = new SpinWait();\n        while (!_flag) {\n            spinWait.SpinOnce();  // Optimized for short waits\n            // After some spins, yields to other threads\n        }\n    }\n}\n</code></pre>\n<p><strong>Why it works</strong>: <code>SpinWait</code> uses CPU hints and gradually increases back-off. For very short waits, it avoids context switch overhead. For longer waits, it yields to prevent waste.</p>\n<p><strong>When to use</strong>: Only when you\'re absolutely certain the wait will be nanoseconds/microseconds. For longer waits, use blocking primitives.</p>\n<p><strong>Trade-off</strong>: Still consumes some CPU, but optimized. Use sparingly.</p>\n<h3 id=\"technique-4-use-taskcompletionsource-for-async-coordination\">Technique 4: Use TaskCompletionSource for Async Coordination</h3>\n<p><strong>When</strong>: Coordinating async operations without blocking threads.</p>\n<pre><code class=\"language-csharp\">//  Good: TaskCompletionSource for async coordination\npublic class AsyncWait {\n    private readonly TaskCompletionSource&lt;bool&gt; _tcs = new TaskCompletionSource&lt;bool&gt;();\n    \n    public async Task WaitForFlagAsync() {\n        await _tcs.Task;  // Non-blocking wait\n    }\n    \n    public void SetFlag() {\n        _tcs.SetResult(true);  // Completes the task\n    }\n}\n</code></pre>\n<p><strong>Why it works</strong>: <code>TaskCompletionSource</code> creates a task that completes when <code>SetResult()</code> is called. <code>await</code> doesn\'t block threadsit schedules continuation when the task completes.</p>\n<p><strong>Performance</strong>: No thread blocking, efficient scheduling, allows high concurrency.</p>\n<h3 id=\"technique-5-use-producer-consumer-patterns\">Technique 5: Use Producer-Consumer Patterns</h3>\n<p><strong>When</strong>: Threads need to wait for work items.</p>\n<pre><code class=\"language-csharp\">//  Good: Use BlockingCollection for producer-consumer\npublic class WorkQueue {\n    private readonly BlockingCollection&lt;WorkItem&gt; _queue = new BlockingCollection&lt;WorkItem&gt;();\n    \n    // Producer\n    public void AddWork(WorkItem item) {\n        _queue.Add(item);  // Wakes waiting consumers\n    }\n    \n    // Consumer\n    public WorkItem GetWork() {\n        return _queue.Take();  // Blocks until work available\n    }\n}\n</code></pre>\n<p><strong>Why it works</strong>: <code>BlockingCollection</code> uses efficient blocking. Consumers block (no CPU waste) until producers add work, then wake immediately.</p>\n<p><strong>Performance</strong>: Efficient coordination without busy-wait.</p>\n<hr>\n<h2 id=\"example-scenarios\">Example Scenarios</h2>\n<h3 id=\"scenario-1-waiting-for-a-flag\">Scenario 1: Waiting for a Flag</h3>\n<p><strong>Problem</strong>: Thread needs to wait for a boolean flag to become true.</p>\n<p><strong>Bad solution</strong>: Busy-wait loop.</p>\n<p><strong>Good solution</strong>: Use <code>ManualResetEventSlim</code> or <code>TaskCompletionSource</code>.</p>\n<pre><code class=\"language-csharp\">//  Recommended\nprivate readonly ManualResetEventSlim _ready = new ManualResetEventSlim(false);\n\npublic void WaitForReady() {\n    _ready.Wait();  // Blocks efficiently\n}\n\npublic void SetReady() {\n    _ready.Set();  // Wakes waiting threads\n}\n</code></pre>\n<p><strong>Performance impact</strong>: Eliminates CPU waste. Thread blocks until ready, then wakes in microseconds.</p>\n<h3 id=\"scenario-2-waiting-for-io\">Scenario 2: Waiting for I/O</h3>\n<p><strong>Problem</strong>: Waiting for file to be created or network response.</p>\n<p><strong>Bad solution</strong>: Busy-wait checking file existence or response.</p>\n<p><strong>Good solution</strong>: Use async/await with proper async I/O.</p>\n<pre><code class=\"language-csharp\">//  Recommended\npublic async Task&lt;string&gt; ReadFileAsync(string path) {\n    using var reader = new StreamReader(path);\n    return await reader.ReadToEndAsync();  // Non-blocking I/O\n}\n</code></pre>\n<p><strong>Performance impact</strong>: Doesn\'t block threads. Allows thousands of concurrent I/O operations.</p>\n<h3 id=\"scenario-3-producer-consumer-pattern\">Scenario 3: Producer-Consumer Pattern</h3>\n<p><strong>Problem</strong>: Consumer threads need to wait for work items.</p>\n<p><strong>Bad solution</strong>: Busy-wait checking if queue has items.</p>\n<p><strong>Good solution</strong>: Use <code>BlockingCollection</code> or channels.</p>\n<pre><code class=\"language-csharp\">//  Recommended\nprivate readonly BlockingCollection&lt;WorkItem&gt; _workQueue = new BlockingCollection&lt;WorkItem&gt;();\n\npublic void ProcessWork() {\n    foreach (var item in _workQueue.GetConsumingEnumerable()) {\n        Process(item);  // Blocks until work available\n    }\n}\n</code></pre>\n<p><strong>Performance impact</strong>: Efficient coordination. Consumers block until work available, no CPU waste.</p>\n<hr>\n<h2 id=\"summary-and-key-takeaways\">Summary and Key Takeaways</h2>\n<p>Busy-wait loops continuously consume CPU cycles while waiting for conditions, wasting resources and preventing other threads from running. Replace them with proper synchronization primitives (events, semaphores, async/await) that allow threads to block efficiently until conditions are met.</p>\n<p><strong>Core Principle</strong>: Never busy-wait unless you\'re absolutely certain the wait will be nanoseconds. Use blocking synchronization or async patterns for all other cases.</p>\n<p><strong>Main Trade-off</strong>: Tiny wake-up latency (microseconds) for blocking vs. massive CPU waste for busy-wait. The trade-off almost always favors blocking.</p>\n','6956d503ebcfaae683eec9c9','Replace CPU-wasting busy-wait loops with appropriate synchronization mechanisms to free CPU cycles for useful work and improve system efficiency.\n\n\n\n\nExecutive Summary (TL;DR)\n\n\nBusy-wait loops continuously consume CPU cycles while waiting for a condition to become true, without yielding control to the operating system scheduler. This wastes CPU resources, increases power consumption, prevents other threads from running, and can degrade overall system performance. The solution is to use proper synchronization primitives (events, semaphores, condition variables) that allow the thread to sleep until the condition is met, or use async/await for non-blocking waits. For very short waits (microseconds), SpinWait provides an optimized alternative. Eliminating busy-wait loops can free 10-100% of CPU depending on the situation. Always prefer blocking synchronization or async patterns over busy-waiting, except in extremely latency-critical scenarios with guaranteed short wait times.\n\n\n\n\nProblem Context\n\n\nWhat is a busy-wait loop? A loop that repeatedly checks a condition in a tight cycle, consuming CPU continuously while waiting. The thread never gives up the CPU, so it burns cycles doing nothing useful.\n\n\nExample of busy-wait:\n\n\nwhile (!condition) {\n    // Empty loop - just checking condition over and over\n    // CPU is at 100% doing nothing useful!\n}\n\n\n\nThe problem: While the thread is busy-waiting, it:\n\n\n * Consumes CPU cycles that could be used by other threads\n * Wastes power (especially important in mobile/embedded systems)\n * Prevents the operating system from scheduling other work\n * Can cause thermal issues (CPU heating up from constant work)\n * May starve other threads of CPU time\n\n\n\nKey Terms Explained\n\n\nContext Switch: When the OS scheduler stops one thread and starts another. This has overhead (saving/restoring thread state), but allows the CPU to do useful work instead of busy-waiting.\n\n\nYield: When a thread voluntarily gives up the CPU, allowing the scheduler to run other threads. Threads can yield explicitly or by calling blocking operations.\n\n\nBlocking: When a thread waits for something (I/O, synchronization event) and is suspended by the OS. The thread doesn\'t consume CPU while blocked.\n\n\n\nWhy Naive Solutions Fail\n\n\nAdding Thread.Sleep() to busy-wait loops\n\n\nwhile (!condition) {\n    Thread.Sleep(1);  // Sleep 1ms\n}\n\n\n\n * Why it\'s better but still wrong: Reduces CPU usage but introduces unnecessary latency. The thread wakes up every millisecond to check, even if the condition changed immediately after sleeping. Still not ideal.\n\n\nUsing very short sleeps\n\n\nwhile (!condition) {\n    Thread.Sleep(0);  // Yield to other threads\n}\n\n\n\n * Why it\'s better: Yields to other threads, but still has overhead. Better than pure busy-wait, but proper synchronization is cleaner and more efficient.\n\n\nAssuming the problem will fix itself\n\n\n * Why it fails: Busy-wait doesn\'t fix itself. It continues wasting resources until you fix the code.\n\n\n\n\nHow It Works\n\n\n\nUnderstanding Thread Scheduling\n\n\nWhat happens when a thread runs: The OS scheduler assigns a CPU core to the thread. The thread executes instructions until:\n\n\n * It completes its work\n * It yields the CPU (explicitly or by blocking)\n * The scheduler preempts it (time slice expires, higher priority thread needs CPU)\n\n\nWhat happens during busy-wait: The thread never yields. It executes the loop instructions continuously:\n\n\n 1. Check condition (read memory/register)\n 2. Compare with expected value\n 3. Branch back to step 1 if false\n 4. Repeat millions of times per second\n\n\nCPU perspective: The CPU executes these instructions at full speed (billions per second). Even though the thread isn\'t doing useful work, the CPU is working hard, consuming power and generating heat.\n\n\n\nOperating System Behavior\n\n\nWhen a thread blocks (waits on a synchronization primitive):\n\n\n 1. Thread calls a blocking operation (e.g., Wait(), Sleep())\n 2. OS scheduler removes thread from \"runnable\" queue\n 3. Thread is marked as \"waiting\" and doesn\'t consume CPU\n 4. OS schedules other threads to use the CPU\n 5. When the condition is met, OS moves thread back to \"runnable\" queue\n 6. Thread eventually gets scheduled and continues execution\n\n\nContext switch overhead: Switching threads takes ~1-10 microseconds on modern systems. This is minimal compared to wasting thousands of cycles in busy-wait.\n\n\nWake-up latency: Modern OS kernels can wake waiting threads very quickly (often microseconds). Proper synchronization primitives use efficient kernel mechanisms (events, futexes) for fast wake-up.\n\n\n\nSynchronization Primitives Explained\n\n\nEvent/Semaphore/Mutex: These are OS-provided synchronization mechanisms that:\n\n\n * Allow threads to wait without consuming CPU\n * Wake threads efficiently when conditions change\n * Use kernel mechanisms for fast signaling\n\n\nHow they work:\n\n\n 1. Thread calls Wait()  OS suspends thread, doesn\'t consume CPU\n 2. Another thread calls Set()/Signal()  OS wakes waiting thread(s)\n 3. Woken thread resumes execution\n\n\nPerformance: Wake-up is typically microseconds. The overhead is negligible compared to busy-wait waste.\n\n\n\n\nWhy This Becomes a Bottleneck\n\n\n\nCPU Starvation\n\n\nWhat happens: Busy-wait threads consume CPU cores, preventing other threads from running. If you have N CPU cores and M threads busy-waiting (where M > N), some threads can\'t run at all.\n\n\nImpact: System throughput decreases. Actual work threads compete for fewer available cores, causing slowdowns.\n\n\nExample: 8-core server, 10 threads doing real work, 5 threads busy-waiting. The 5 busy-wait threads consume 5 cores continuously. Only 3 cores available for the 10 work threads  severe contention and poor performance.\n\n\n\nPower Consumption\n\n\nWhat happens: CPUs consume more power when executing instructions. Busy-wait loops execute instructions continuously, keeping the CPU active and consuming power.\n\n\nImpact:\n\n\n * Higher electricity costs in data centers\n * Reduced battery life in mobile devices\n * Thermal issues (CPU heating up)\n * Need for better cooling systems\n\n\nWhy it\'s significant: In a data center with thousands of servers, busy-wait loops can significantly increase power consumption and cooling costs.\n\n\n\nThermal Throttling\n\n\nWhat happens: When CPUs get too hot, they reduce clock frequency to cool down (thermal throttling). Busy-wait loops generate heat, potentially triggering throttling.\n\n\nImpact: Reduced CPU performance system-wide. Even threads doing useful work slow down because the CPU is throttled.\n\n\n\nScheduler Inefficiency\n\n\nWhat happens: The OS scheduler can\'t make optimal scheduling decisions when threads don\'t yield. It can\'t balance load effectively or prioritize important work.\n\n\nImpact: Suboptimal resource utilization, longer response times for important work, poor system responsiveness.\n\n\n\nFalse Parallelism\n\n\nWhat happens: Multiple threads busy-waiting appear to be \"running\" (they consume CPU), but they\'re not doing useful work. This creates an illusion of parallelism without actual progress.\n\n\nImpact: System appears busy but throughput is low. Monitoring tools show high CPU usage but low actual work completion.\n\n\n\n\nOptimization Techniques\n\n\n\nTechnique 1: Use Events/Semaphores for Coordination\n\n\nWhen: Threads need to wait for conditions or events from other threads.\n\n\n//  Bad: Busy-wait\npublic class BadWait {\n    private bool _flag = false;\n    \n    public void WaitForFlag() {\n        while (!_flag) {\n            // Wasting CPU!\n        }\n    }\n    \n    public void SetFlag() {\n        _flag = true;\n    }\n}\n\n//  Good: Use ManualResetEventSlim\npublic class GoodWait {\n    private readonly ManualResetEventSlim _event = new ManualResetEventSlim(false);\n    \n    public void WaitForFlag() {\n        _event.Wait();  // Blocks, doesn\'t consume CPU\n    }\n    \n    public void SetFlag() {\n        _event.Set();  // Wakes waiting thread\n    }\n}\n\n\n\nWhy it works: ManualResetEventSlim uses efficient OS mechanisms. Thread blocks (doesn\'t consume CPU) until Set() is called, then wakes quickly (microseconds).\n\n\nPerformance: Eliminates CPU waste. Wake-up latency is microseconds, negligible for most scenarios.\n\n\n\nTechnique 2: Use async/await for I/O\n\n\nWhen: Waiting for I/O operations (file, network, database).\n\n\n//  Bad: Busy-wait for I/O\npublic void ProcessFile(string path) {\n    while (!File.Exists(path)) {\n        // Wasting CPU waiting for file!\n    }\n    // Process file...\n}\n\n//  Good: Use async/await\npublic async Task ProcessFileAsync(string path) {\n    // Wait for file without blocking thread\n    while (!File.Exists(path)) {\n        await Task.Delay(100);  // Yield to other work\n    }\n    // Process file...\n}\n\n//  Better: Use FileSystemWatcher or proper async I/O\npublic async Task ProcessFileAsync(string path) {\n    using var fileStream = File.OpenRead(path);\n    // Use async I/O methods\n    var buffer = new byte[4096];\n    await fileStream.ReadAsync(buffer, 0, buffer.Length);\n}\n\n\n\nWhy it works: async/await doesn\'t block threads. The thread can do other work while waiting for I/O. When I/O completes, execution resumes.\n\n\nPerformance: Doesn\'t waste threads on I/O waits. Allows more concurrent operations.\n\n\n\nTechnique 3: Use SpinWait for Very Short Waits\n\n\nWhen: Wait time is guaranteed to be extremely short (nanoseconds to microseconds).\n\n\n//  For very short waits, use SpinWait\npublic class OptimizedWait {\n    private volatile bool _flag = false;\n    \n    public void WaitForFlag() {\n        var spinWait = new SpinWait();\n        while (!_flag) {\n            spinWait.SpinOnce();  // Optimized for short waits\n            // After some spins, yields to other threads\n        }\n    }\n}\n\n\n\nWhy it works: SpinWait uses CPU hints and gradually increases back-off. For very short waits, it avoids context switch overhead. For longer waits, it yields to prevent waste.\n\n\nWhen to use: Only when you\'re absolutely certain the wait will be nanoseconds/microseconds. For longer waits, use blocking primitives.\n\n\nTrade-off: Still consumes some CPU, but optimized. Use sparingly.\n\n\n\nTechnique 4: Use TaskCompletionSource for Async Coordination\n\n\nWhen: Coordinating async operations without blocking threads.\n\n\n//  Good: TaskCompletionSource for async coordination\npublic class AsyncWait {\n    private readonly TaskCompletionSource<bool> _tcs = new TaskCompletionSource<bool>();\n    \n    public async Task WaitForFlagAsync() {\n        await _tcs.Task;  // Non-blocking wait\n    }\n    \n    public void SetFlag() {\n        _tcs.SetResult(true);  // Completes the task\n    }\n}\n\n\n\nWhy it works: TaskCompletionSource creates a task that completes when SetResult() is called. await doesn\'t block threadsit schedules continuation when the task completes.\n\n\nPerformance: No thread blocking, efficient scheduling, allows high concurrency.\n\n\n\nTechnique 5: Use Producer-Consumer Patterns\n\n\nWhen: Threads need to wait for work items.\n\n\n//  Good: Use BlockingCollection for producer-consumer\npublic class WorkQueue {\n    private readonly BlockingCollection<WorkItem> _queue = new BlockingCollection<WorkItem>();\n    \n    // Producer\n    public void AddWork(WorkItem item) {\n        _queue.Add(item);  // Wakes waiting consumers\n    }\n    \n    // Consumer\n    public WorkItem GetWork() {\n        return _queue.Take();  // Blocks until work available\n    }\n}\n\n\n\nWhy it works: BlockingCollection uses efficient blocking. Consumers block (no CPU waste) until producers add work, then wake immediately.\n\n\nPerformance: Efficient coordination without busy-wait.\n\n\n\n\nExample Scenarios\n\n\n\nScenario 1: Waiting for a Flag\n\n\nProblem: Thread needs to wait for a boolean flag to become true.\n\n\nBad solution: Busy-wait loop.\n\n\nGood solution: Use ManualResetEventSlim or TaskCompletionSource.\n\n\n//  Recommended\nprivate readonly ManualResetEventSlim _ready = new ManualResetEventSlim(false);\n\npublic void WaitForReady() {\n    _ready.Wait();  // Blocks efficiently\n}\n\npublic void SetReady() {\n    _ready.Set();  // Wakes waiting threads\n}\n\n\n\nPerformance impact: Eliminates CPU waste. Thread blocks until ready, then wakes in microseconds.\n\n\n\nScenario 2: Waiting for I/O\n\n\nProblem: Waiting for file to be created or network response.\n\n\nBad solution: Busy-wait checking file existence or response.\n\n\nGood solution: Use async/await with proper async I/O.\n\n\n//  Recommended\npublic async Task<string> ReadFileAsync(string path) {\n    using var reader = new StreamReader(path);\n    return await reader.ReadToEndAsync();  // Non-blocking I/O\n}\n\n\n\nPerformance impact: Doesn\'t block threads. Allows thousands of concurrent I/O operations.\n\n\n\nScenario 3: Producer-Consumer Pattern\n\n\nProblem: Consumer threads need to wait for work items.\n\n\nBad solution: Busy-wait checking if queue has items.\n\n\nGood solution: Use BlockingCollection or channels.\n\n\n//  Recommended\nprivate readonly BlockingCollection<WorkItem> _workQueue = new BlockingCollection<WorkItem>();\n\npublic void ProcessWork() {\n    foreach (var item in _workQueue.GetConsumingEnumerable()) {\n        Process(item);  // Blocks until work available\n    }\n}\n\n\n\nPerformance impact: Efficient coordination. Consumers block until work available, no CPU waste.\n\n\n\n\nSummary and Key Takeaways\n\n\nBusy-wait loops continuously consume CPU cycles while waiting for conditions, wasting resources and preventing other threads from running. Replace them with proper synchronization primitives (events, semaphores, async/await) that allow threads to block efficiently until conditions are met.\n\n\nCore Principle: Never busy-wait unless you\'re absolutely certain the wait will be nanoseconds. Use blocking synchronization or async patterns for all other cases.\n\n\nMain Trade-off: Tiny wake-up latency (microseconds) for blocking vs. massive CPU waste for busy-wait. The trade-off almost always favors blocking.\n',NULL,0,'post','published',NULL,'public','all','2026-01-01 20:11:47','2026-01-01 20:12:08','2026-01-01 20:11:47','694f2665c3e44e650f0e5dd8',NULL,NULL,NULL,NULL,NULL,NULL,1);
/*!40000 ALTER TABLE `posts` ENABLE KEYS */;
UNLOCK TABLES;

--
-- Table structure for table `posts_authors`
--

DROP TABLE IF EXISTS `posts_authors`;
/*!40101 SET @saved_cs_client     = @@character_set_client */;
/*!50503 SET character_set_client = utf8mb4 */;
CREATE TABLE `posts_authors` (
  `id` varchar(24) NOT NULL,
  `post_id` varchar(24) NOT NULL,
  `author_id` varchar(24) NOT NULL,
  `sort_order` int unsigned NOT NULL DEFAULT '0',
  PRIMARY KEY (`id`),
  KEY `posts_authors_post_id_foreign` (`post_id`),
  KEY `posts_authors_author_id_foreign` (`author_id`),
  CONSTRAINT `posts_authors_author_id_foreign` FOREIGN KEY (`author_id`) REFERENCES `users` (`id`),
  CONSTRAINT `posts_authors_post_id_foreign` FOREIGN KEY (`post_id`) REFERENCES `posts` (`id`)
) ENGINE=InnoDB DEFAULT CHARSET=utf8mb4 COLLATE=utf8mb4_0900_ai_ci;
/*!40101 SET character_set_client = @saved_cs_client */;

--
-- Dumping data for table `posts_authors`
--

LOCK TABLES `posts_authors` WRITE;
/*!40000 ALTER TABLE `posts_authors` DISABLE KEYS */;
INSERT INTO `posts_authors` VALUES ('694f2666c3e44e650f0e5e6b','694f2666c3e44e650f0e5e6a','694f2665c3e44e650f0e5dd8',0),('694ff07f19c94fae00e3734c','694ff07e19c94fae00e37345','694f2665c3e44e650f0e5dd8',0),('6950348e19c94fae00e37365','6950348e19c94fae00e37362','694f2665c3e44e650f0e5dd8',0),('6951521f19c94fae00e3739e','6951521f19c94fae00e3739b','694f2665c3e44e650f0e5dd8',0),('6951b07819c94fae00e373ae','6951b07819c94fae00e373ad','694f2665c3e44e650f0e5dd8',0),('6951b0ef19c94fae00e373b6','6951b0ef19c94fae00e373b5','694f2665c3e44e650f0e5dd8',0),('6951b1c119c94fae00e373be','6951b1c119c94fae00e373bd','694f2665c3e44e650f0e5dd8',0),('69532d9aebcfaae683eec9a2','69532d9aebcfaae683eec996','694f2665c3e44e650f0e5dd8',0),('6954500debcfaae683eec9bc','6954500debcfaae683eec9b2','694f2665c3e44e650f0e5dd8',0),('6956d503ebcfaae683eec9d4','6956d503ebcfaae683eec9c9','694f2665c3e44e650f0e5dd8',0);
/*!40000 ALTER TABLE `posts_authors` ENABLE KEYS */;
UNLOCK TABLES;

--
-- Table structure for table `posts_meta`
--

DROP TABLE IF EXISTS `posts_meta`;
/*!40101 SET @saved_cs_client     = @@character_set_client */;
/*!50503 SET character_set_client = utf8mb4 */;
CREATE TABLE `posts_meta` (
  `id` varchar(24) NOT NULL,
  `post_id` varchar(24) NOT NULL,
  `og_image` varchar(2000) DEFAULT NULL,
  `og_title` varchar(300) DEFAULT NULL,
  `og_description` varchar(500) DEFAULT NULL,
  `twitter_image` varchar(2000) DEFAULT NULL,
  `twitter_title` varchar(300) DEFAULT NULL,
  `twitter_description` varchar(500) DEFAULT NULL,
  `meta_title` varchar(2000) DEFAULT NULL,
  `meta_description` varchar(2000) DEFAULT NULL,
  `email_subject` varchar(300) DEFAULT NULL,
  `frontmatter` text,
  `feature_image_alt` varchar(2000) DEFAULT NULL,
  `feature_image_caption` text,
  `email_only` tinyint(1) NOT NULL DEFAULT '0',
  PRIMARY KEY (`id`),
  UNIQUE KEY `posts_meta_post_id_unique` (`post_id`),
  CONSTRAINT `posts_meta_post_id_foreign` FOREIGN KEY (`post_id`) REFERENCES `posts` (`id`)
) ENGINE=InnoDB DEFAULT CHARSET=utf8mb4 COLLATE=utf8mb4_0900_ai_ci;
/*!40101 SET character_set_client = @saved_cs_client */;

--
-- Dumping data for table `posts_meta`
--

LOCK TABLES `posts_meta` WRITE;
/*!40000 ALTER TABLE `posts_meta` DISABLE KEYS */;
/*!40000 ALTER TABLE `posts_meta` ENABLE KEYS */;
UNLOCK TABLES;

--
-- Table structure for table `posts_products`
--

DROP TABLE IF EXISTS `posts_products`;
/*!40101 SET @saved_cs_client     = @@character_set_client */;
/*!50503 SET character_set_client = utf8mb4 */;
CREATE TABLE `posts_products` (
  `id` varchar(24) NOT NULL,
  `post_id` varchar(24) NOT NULL,
  `product_id` varchar(24) NOT NULL,
  `sort_order` int unsigned NOT NULL DEFAULT '0',
  PRIMARY KEY (`id`),
  KEY `posts_products_post_id_foreign` (`post_id`),
  KEY `posts_products_product_id_foreign` (`product_id`),
  CONSTRAINT `posts_products_post_id_foreign` FOREIGN KEY (`post_id`) REFERENCES `posts` (`id`) ON DELETE CASCADE,
  CONSTRAINT `posts_products_product_id_foreign` FOREIGN KEY (`product_id`) REFERENCES `products` (`id`) ON DELETE CASCADE
) ENGINE=InnoDB DEFAULT CHARSET=utf8mb4 COLLATE=utf8mb4_0900_ai_ci;
/*!40101 SET character_set_client = @saved_cs_client */;

--
-- Dumping data for table `posts_products`
--

LOCK TABLES `posts_products` WRITE;
/*!40000 ALTER TABLE `posts_products` DISABLE KEYS */;
INSERT INTO `posts_products` VALUES ('694ff0a319c94fae00e37350','694ff07e19c94fae00e37345','694f2665c3e44e650f0e5de8',0),('695036d119c94fae00e37368','6950348e19c94fae00e37362','694f2665c3e44e650f0e5de8',0),('6951b09719c94fae00e373b1','6951b07819c94fae00e373ad','694f2665c3e44e650f0e5de8',0),('6951b0f819c94fae00e373b9','6951b0ef19c94fae00e373b5','694f2665c3e44e650f0e5de8',0),('6951b1c819c94fae00e373c1','6951b1c119c94fae00e373bd','694f2665c3e44e650f0e5de8',0),('69520df919c94fae00e373d4','6951521f19c94fae00e3739b','694f2665c3e44e650f0e5de8',0),('69532daaebcfaae683eec9a5','69532d9aebcfaae683eec996','694f2665c3e44e650f0e5de8',0),('69545028ebcfaae683eec9bf','6954500debcfaae683eec9b2','694f2665c3e44e650f0e5de8',0),('6956d50eebcfaae683eec9d7','6956d503ebcfaae683eec9c9','694f2665c3e44e650f0e5de8',0);
/*!40000 ALTER TABLE `posts_products` ENABLE KEYS */;
UNLOCK TABLES;

--
-- Table structure for table `posts_tags`
--

DROP TABLE IF EXISTS `posts_tags`;
/*!40101 SET @saved_cs_client     = @@character_set_client */;
/*!50503 SET character_set_client = utf8mb4 */;
CREATE TABLE `posts_tags` (
  `id` varchar(24) NOT NULL,
  `post_id` varchar(24) NOT NULL,
  `tag_id` varchar(24) NOT NULL,
  `sort_order` int unsigned NOT NULL DEFAULT '0',
  PRIMARY KEY (`id`),
  KEY `posts_tags_tag_id_foreign` (`tag_id`),
  KEY `posts_tags_post_id_tag_id_index` (`post_id`,`tag_id`),
  CONSTRAINT `posts_tags_post_id_foreign` FOREIGN KEY (`post_id`) REFERENCES `posts` (`id`),
  CONSTRAINT `posts_tags_tag_id_foreign` FOREIGN KEY (`tag_id`) REFERENCES `tags` (`id`)
) ENGINE=InnoDB DEFAULT CHARSET=utf8mb4 COLLATE=utf8mb4_0900_ai_ci;
/*!40101 SET character_set_client = @saved_cs_client */;

--
-- Dumping data for table `posts_tags`
--

LOCK TABLES `posts_tags` WRITE;
/*!40000 ALTER TABLE `posts_tags` DISABLE KEYS */;
INSERT INTO `posts_tags` VALUES ('694ff07f19c94fae00e37346','694ff07e19c94fae00e37345','694f2cf27f4d7d685707e3f7',0),('694ff07f19c94fae00e37347','694ff07e19c94fae00e37345','694f2cf37f4d7d685707e3f9',1),('694ff07f19c94fae00e37348','694ff07e19c94fae00e37345','694f2cf37f4d7d685707e3fb',2),('694ff07f19c94fae00e37349','694ff07e19c94fae00e37345','694f2cf37f4d7d685707e3fd',3),('694ff07f19c94fae00e3734a','694ff07e19c94fae00e37345','694f2cf37f4d7d685707e3ff',4),('694ff07f19c94fae00e3734b','694ff07e19c94fae00e37345','694f2cf37f4d7d685707e401',5),('6950348e19c94fae00e37363','6950348e19c94fae00e37362','694f2cf37f4d7d685707e3fb',0),('6950348e19c94fae00e37364','6950348e19c94fae00e37362','694f2cf37f4d7d685707e3fd',1),('6951521f19c94fae00e3739c','6951521f19c94fae00e3739b','694f2cf37f4d7d685707e3fb',0),('6951521f19c94fae00e3739d','6951521f19c94fae00e3739b','694f2cf37f4d7d685707e3fd',1),('69532d9aebcfaae683eec997','69532d9aebcfaae683eec996','694f2cf27f4d7d685707e3f7',0),('69532d9aebcfaae683eec998','69532d9aebcfaae683eec996','694f2cf37f4d7d685707e3f9',1),('69532d9aebcfaae683eec999','69532d9aebcfaae683eec996','694f2cf37f4d7d685707e3fb',2),('69532d9aebcfaae683eec99a','69532d9aebcfaae683eec996','694f2cf37f4d7d685707e3fd',3),('69532d9aebcfaae683eec99b','69532d9aebcfaae683eec996','69532d98ebcfaae683eec988',4),('69532d9aebcfaae683eec99c','69532d9aebcfaae683eec996','69532d98ebcfaae683eec98a',5),('69532d9aebcfaae683eec99d','69532d9aebcfaae683eec996','69532d98ebcfaae683eec98c',6),('69532d9aebcfaae683eec99e','69532d9aebcfaae683eec996','69532d99ebcfaae683eec98e',7),('69532d9aebcfaae683eec99f','69532d9aebcfaae683eec996','69532d99ebcfaae683eec990',8),('69532d9aebcfaae683eec9a0','69532d9aebcfaae683eec996','69532d9aebcfaae683eec992',9),('69532d9aebcfaae683eec9a1','69532d9aebcfaae683eec996','69532d9aebcfaae683eec994',10),('6954500debcfaae683eec9b3','6954500debcfaae683eec9b2','694f2cf27f4d7d685707e3f7',0),('6954500debcfaae683eec9b4','6954500debcfaae683eec9b2','694f2cf37f4d7d685707e3f9',1),('6954500debcfaae683eec9b5','6954500debcfaae683eec9b2','694f2cf37f4d7d685707e3fb',2),('6954500debcfaae683eec9b6','6954500debcfaae683eec9b2','694f2cf37f4d7d685707e3fd',3),('6954500debcfaae683eec9b7','6954500debcfaae683eec9b2','6954500cebcfaae683eec9ac',4),('6954500debcfaae683eec9b8','6954500debcfaae683eec9b2','69532d9aebcfaae683eec992',5),('6954500debcfaae683eec9b9','6954500debcfaae683eec9b2','69532d9aebcfaae683eec994',6),('6954500debcfaae683eec9ba','6954500debcfaae683eec9b2','6954500debcfaae683eec9ae',7),('6954500debcfaae683eec9bb','6954500debcfaae683eec9b2','6954500debcfaae683eec9b0',8),('6956d503ebcfaae683eec9ca','6956d503ebcfaae683eec9c9','69532d98ebcfaae683eec988',0),('6956d503ebcfaae683eec9cb','6956d503ebcfaae683eec9c9','69532d98ebcfaae683eec98a',1),('6956d503ebcfaae683eec9cc','6956d503ebcfaae683eec9c9','694f2cf37f4d7d685707e3f9',2),('6956d503ebcfaae683eec9cd','6956d503ebcfaae683eec9c9','694f2cf37f4d7d685707e3fb',3),('6956d503ebcfaae683eec9ce','6956d503ebcfaae683eec9c9','694f2cf37f4d7d685707e3fd',4),('6956d503ebcfaae683eec9cf','6956d503ebcfaae683eec9c9','69532d9aebcfaae683eec992',5),('6956d503ebcfaae683eec9d0','6956d503ebcfaae683eec9c9','69532d9aebcfaae683eec994',6),('6956d503ebcfaae683eec9d1','6956d503ebcfaae683eec9c9','6956d502ebcfaae683eec9c3',7),('6956d503ebcfaae683eec9d2','6956d503ebcfaae683eec9c9','6956d502ebcfaae683eec9c5',8),('6956d503ebcfaae683eec9d3','6956d503ebcfaae683eec9c9','6956d502ebcfaae683eec9c7',9);
/*!40000 ALTER TABLE `posts_tags` ENABLE KEYS */;
UNLOCK TABLES;

--
-- Table structure for table `products`
--

DROP TABLE IF EXISTS `products`;
/*!40101 SET @saved_cs_client     = @@character_set_client */;
/*!50503 SET character_set_client = utf8mb4 */;
CREATE TABLE `products` (
  `id` varchar(24) NOT NULL,
  `name` varchar(191) NOT NULL,
  `slug` varchar(191) NOT NULL,
  `active` tinyint(1) NOT NULL DEFAULT '1',
  `welcome_page_url` varchar(2000) DEFAULT NULL,
  `visibility` varchar(50) NOT NULL DEFAULT 'none',
  `trial_days` int unsigned NOT NULL DEFAULT '0',
  `description` varchar(191) DEFAULT NULL,
  `type` varchar(50) NOT NULL DEFAULT 'paid',
  `currency` varchar(50) DEFAULT NULL,
  `monthly_price` int unsigned DEFAULT NULL,
  `yearly_price` int unsigned DEFAULT NULL,
  `created_at` datetime NOT NULL,
  `updated_at` datetime DEFAULT NULL,
  `monthly_price_id` varchar(24) DEFAULT NULL,
  `yearly_price_id` varchar(24) DEFAULT NULL,
  PRIMARY KEY (`id`),
  UNIQUE KEY `products_slug_unique` (`slug`)
) ENGINE=InnoDB DEFAULT CHARSET=utf8mb4 COLLATE=utf8mb4_0900_ai_ci;
/*!40101 SET character_set_client = @saved_cs_client */;

--
-- Dumping data for table `products`
--

LOCK TABLES `products` WRITE;
/*!40000 ALTER TABLE `products` DISABLE KEYS */;
INSERT INTO `products` VALUES ('694f2665c3e44e650f0e5de7','Free','free',1,NULL,'public',0,NULL,'free',NULL,NULL,NULL,'2025-12-27 00:20:53','2025-12-27 00:20:53',NULL,NULL),('694f2665c3e44e650f0e5de8','Performance Engineering','default-product',1,NULL,'public',0,NULL,'paid','USD',500,5000,'2025-12-27 00:20:53','2025-12-27 00:21:16',NULL,NULL);
/*!40000 ALTER TABLE `products` ENABLE KEYS */;
UNLOCK TABLES;

--
-- Table structure for table `products_benefits`
--

DROP TABLE IF EXISTS `products_benefits`;
/*!40101 SET @saved_cs_client     = @@character_set_client */;
/*!50503 SET character_set_client = utf8mb4 */;
CREATE TABLE `products_benefits` (
  `id` varchar(24) NOT NULL,
  `product_id` varchar(24) NOT NULL,
  `benefit_id` varchar(24) NOT NULL,
  `sort_order` int unsigned NOT NULL DEFAULT '0',
  PRIMARY KEY (`id`),
  KEY `products_benefits_product_id_foreign` (`product_id`),
  KEY `products_benefits_benefit_id_foreign` (`benefit_id`),
  CONSTRAINT `products_benefits_benefit_id_foreign` FOREIGN KEY (`benefit_id`) REFERENCES `benefits` (`id`) ON DELETE CASCADE,
  CONSTRAINT `products_benefits_product_id_foreign` FOREIGN KEY (`product_id`) REFERENCES `products` (`id`) ON DELETE CASCADE
) ENGINE=InnoDB DEFAULT CHARSET=utf8mb4 COLLATE=utf8mb4_0900_ai_ci;
/*!40101 SET character_set_client = @saved_cs_client */;

--
-- Dumping data for table `products_benefits`
--

LOCK TABLES `products_benefits` WRITE;
/*!40000 ALTER TABLE `products_benefits` DISABLE KEYS */;
/*!40000 ALTER TABLE `products_benefits` ENABLE KEYS */;
UNLOCK TABLES;

--
-- Table structure for table `recommendation_click_events`
--

DROP TABLE IF EXISTS `recommendation_click_events`;
/*!40101 SET @saved_cs_client     = @@character_set_client */;
/*!50503 SET character_set_client = utf8mb4 */;
CREATE TABLE `recommendation_click_events` (
  `id` varchar(24) NOT NULL,
  `recommendation_id` varchar(24) NOT NULL,
  `member_id` varchar(24) DEFAULT NULL,
  `created_at` datetime NOT NULL,
  PRIMARY KEY (`id`),
  KEY `recommendation_click_events_recommendation_id_foreign` (`recommendation_id`),
  KEY `recommendation_click_events_member_id_foreign` (`member_id`),
  CONSTRAINT `recommendation_click_events_member_id_foreign` FOREIGN KEY (`member_id`) REFERENCES `members` (`id`) ON DELETE SET NULL,
  CONSTRAINT `recommendation_click_events_recommendation_id_foreign` FOREIGN KEY (`recommendation_id`) REFERENCES `recommendations` (`id`) ON DELETE CASCADE
) ENGINE=InnoDB DEFAULT CHARSET=utf8mb4 COLLATE=utf8mb4_0900_ai_ci;
/*!40101 SET character_set_client = @saved_cs_client */;

--
-- Dumping data for table `recommendation_click_events`
--

LOCK TABLES `recommendation_click_events` WRITE;
/*!40000 ALTER TABLE `recommendation_click_events` DISABLE KEYS */;
/*!40000 ALTER TABLE `recommendation_click_events` ENABLE KEYS */;
UNLOCK TABLES;

--
-- Table structure for table `recommendation_subscribe_events`
--

DROP TABLE IF EXISTS `recommendation_subscribe_events`;
/*!40101 SET @saved_cs_client     = @@character_set_client */;
/*!50503 SET character_set_client = utf8mb4 */;
CREATE TABLE `recommendation_subscribe_events` (
  `id` varchar(24) NOT NULL,
  `recommendation_id` varchar(24) NOT NULL,
  `member_id` varchar(24) DEFAULT NULL,
  `created_at` datetime NOT NULL,
  PRIMARY KEY (`id`),
  KEY `recommendation_subscribe_events_recommendation_id_foreign` (`recommendation_id`),
  KEY `recommendation_subscribe_events_member_id_foreign` (`member_id`),
  CONSTRAINT `recommendation_subscribe_events_member_id_foreign` FOREIGN KEY (`member_id`) REFERENCES `members` (`id`) ON DELETE SET NULL,
  CONSTRAINT `recommendation_subscribe_events_recommendation_id_foreign` FOREIGN KEY (`recommendation_id`) REFERENCES `recommendations` (`id`) ON DELETE CASCADE
) ENGINE=InnoDB DEFAULT CHARSET=utf8mb4 COLLATE=utf8mb4_0900_ai_ci;
/*!40101 SET character_set_client = @saved_cs_client */;

--
-- Dumping data for table `recommendation_subscribe_events`
--

LOCK TABLES `recommendation_subscribe_events` WRITE;
/*!40000 ALTER TABLE `recommendation_subscribe_events` DISABLE KEYS */;
/*!40000 ALTER TABLE `recommendation_subscribe_events` ENABLE KEYS */;
UNLOCK TABLES;

--
-- Table structure for table `recommendations`
--

DROP TABLE IF EXISTS `recommendations`;
/*!40101 SET @saved_cs_client     = @@character_set_client */;
/*!50503 SET character_set_client = utf8mb4 */;
CREATE TABLE `recommendations` (
  `id` varchar(24) NOT NULL,
  `url` varchar(2000) NOT NULL,
  `title` varchar(2000) NOT NULL,
  `excerpt` varchar(2000) DEFAULT NULL,
  `featured_image` varchar(2000) DEFAULT NULL,
  `favicon` varchar(2000) DEFAULT NULL,
  `description` varchar(2000) DEFAULT NULL,
  `one_click_subscribe` tinyint(1) NOT NULL DEFAULT '0',
  `created_at` datetime NOT NULL,
  `updated_at` datetime DEFAULT NULL,
  PRIMARY KEY (`id`)
) ENGINE=InnoDB DEFAULT CHARSET=utf8mb4 COLLATE=utf8mb4_0900_ai_ci;
/*!40101 SET character_set_client = @saved_cs_client */;

--
-- Dumping data for table `recommendations`
--

LOCK TABLES `recommendations` WRITE;
/*!40000 ALTER TABLE `recommendations` DISABLE KEYS */;
/*!40000 ALTER TABLE `recommendations` ENABLE KEYS */;
UNLOCK TABLES;

--
-- Table structure for table `redirects`
--

DROP TABLE IF EXISTS `redirects`;
/*!40101 SET @saved_cs_client     = @@character_set_client */;
/*!50503 SET character_set_client = utf8mb4 */;
CREATE TABLE `redirects` (
  `id` varchar(24) NOT NULL,
  `from` varchar(191) NOT NULL,
  `to` varchar(2000) NOT NULL,
  `post_id` varchar(24) DEFAULT NULL,
  `created_at` datetime NOT NULL,
  `updated_at` datetime DEFAULT NULL,
  PRIMARY KEY (`id`),
  KEY `redirects_from_index` (`from`),
  KEY `redirects_post_id_foreign` (`post_id`),
  CONSTRAINT `redirects_post_id_foreign` FOREIGN KEY (`post_id`) REFERENCES `posts` (`id`) ON DELETE SET NULL
) ENGINE=InnoDB DEFAULT CHARSET=utf8mb4 COLLATE=utf8mb4_0900_ai_ci;
/*!40101 SET character_set_client = @saved_cs_client */;

--
-- Dumping data for table `redirects`
--

LOCK TABLES `redirects` WRITE;
/*!40000 ALTER TABLE `redirects` DISABLE KEYS */;
/*!40000 ALTER TABLE `redirects` ENABLE KEYS */;
UNLOCK TABLES;

--
-- Table structure for table `roles`
--

DROP TABLE IF EXISTS `roles`;
/*!40101 SET @saved_cs_client     = @@character_set_client */;
/*!50503 SET character_set_client = utf8mb4 */;
CREATE TABLE `roles` (
  `id` varchar(24) NOT NULL,
  `name` varchar(50) NOT NULL,
  `description` varchar(2000) DEFAULT NULL,
  `created_at` datetime NOT NULL,
  `updated_at` datetime DEFAULT NULL,
  PRIMARY KEY (`id`),
  UNIQUE KEY `roles_name_unique` (`name`)
) ENGINE=InnoDB DEFAULT CHARSET=utf8mb4 COLLATE=utf8mb4_0900_ai_ci;
/*!40101 SET character_set_client = @saved_cs_client */;

--
-- Dumping data for table `roles`
--

LOCK TABLES `roles` WRITE;
/*!40000 ALTER TABLE `roles` DISABLE KEYS */;
INSERT INTO `roles` VALUES ('694f2665c3e44e650f0e5dd9','Administrator','Administrators','2025-12-27 00:20:53','2025-12-27 00:20:53'),('694f2665c3e44e650f0e5dda','Editor','Editors','2025-12-27 00:20:53','2025-12-27 00:20:53'),('694f2665c3e44e650f0e5ddb','Author','Authors','2025-12-27 00:20:53','2025-12-27 00:20:53'),('694f2665c3e44e650f0e5ddc','Contributor','Contributors','2025-12-27 00:20:53','2025-12-27 00:20:53'),('694f2665c3e44e650f0e5ddd','Owner','Blog Owner','2025-12-27 00:20:53','2025-12-27 00:20:53'),('694f2665c3e44e650f0e5dde','Admin Integration','External Apps','2025-12-27 00:20:53','2025-12-27 00:20:53'),('694f2665c3e44e650f0e5ddf','Ghost Explore Integration','Internal Integration for the Ghost Explore directory','2025-12-27 00:20:53','2025-12-27 00:20:53'),('694f2665c3e44e650f0e5de0','Self-Serve Migration Integration','Internal Integration for the Self-Serve migration tool','2025-12-27 00:20:53','2025-12-27 00:20:53'),('694f2665c3e44e650f0e5de1','DB Backup Integration','Internal DB Backup Client','2025-12-27 00:20:53','2025-12-27 00:20:53'),('694f2665c3e44e650f0e5de2','Scheduler Integration','Internal Scheduler Client','2025-12-27 00:20:53','2025-12-27 00:20:53'),('694f2665c3e44e650f0e5de3','Super Editor','Super Editors','2025-12-27 00:20:53','2025-12-27 00:20:53');
/*!40000 ALTER TABLE `roles` ENABLE KEYS */;
UNLOCK TABLES;

--
-- Table structure for table `roles_users`
--

DROP TABLE IF EXISTS `roles_users`;
/*!40101 SET @saved_cs_client     = @@character_set_client */;
/*!50503 SET character_set_client = utf8mb4 */;
CREATE TABLE `roles_users` (
  `id` varchar(24) NOT NULL,
  `role_id` varchar(24) NOT NULL,
  `user_id` varchar(24) NOT NULL,
  PRIMARY KEY (`id`)
) ENGINE=InnoDB DEFAULT CHARSET=utf8mb4 COLLATE=utf8mb4_0900_ai_ci;
/*!40101 SET character_set_client = @saved_cs_client */;

--
-- Dumping data for table `roles_users`
--

LOCK TABLES `roles_users` WRITE;
/*!40000 ALTER TABLE `roles_users` DISABLE KEYS */;
INSERT INTO `roles_users` VALUES ('694f2665c3e44e650f0e5de4','694f2665c3e44e650f0e5ddd','694f2665c3e44e650f0e5dd8');
/*!40000 ALTER TABLE `roles_users` ENABLE KEYS */;
UNLOCK TABLES;

--
-- Table structure for table `sessions`
--

DROP TABLE IF EXISTS `sessions`;
/*!40101 SET @saved_cs_client     = @@character_set_client */;
/*!50503 SET character_set_client = utf8mb4 */;
CREATE TABLE `sessions` (
  `id` varchar(24) NOT NULL,
  `session_id` varchar(32) NOT NULL,
  `user_id` varchar(24) NOT NULL,
  `session_data` varchar(2000) NOT NULL,
  `created_at` datetime NOT NULL,
  `updated_at` datetime DEFAULT NULL,
  PRIMARY KEY (`id`),
  UNIQUE KEY `sessions_session_id_unique` (`session_id`)
) ENGINE=InnoDB DEFAULT CHARSET=utf8mb4 COLLATE=utf8mb4_0900_ai_ci;
/*!40101 SET character_set_client = @saved_cs_client */;

--
-- Dumping data for table `sessions`
--

LOCK TABLES `sessions` WRITE;
/*!40000 ALTER TABLE `sessions` DISABLE KEYS */;
INSERT INTO `sessions` VALUES ('694f267dc3e44e650f0e60ad','putoKY_Z--3Hf3czNO8o3JGCCtlTvwp8','694f2665c3e44e650f0e5dd8','{\"cookie\":{\"originalMaxAge\":15552000000,\"expires\":\"2026-06-25T00:21:17.168Z\",\"secure\":false,\"httpOnly\":true,\"path\":\"/ghost\",\"sameSite\":\"lax\"},\"user_id\":\"694f2665c3e44e650f0e5dd8\",\"origin\":\"http://66.179.188.92\",\"user_agent\":\"Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/143.0.0.0 Safari/537.36\",\"ip\":\"71.197.40.32\",\"verified\":true}','2025-12-27 00:21:17','2025-12-27 00:21:17'),('694f2c1e7f4d7d685707e3f0','zDWnWIEJdLIqc1Ui6_m3xShJ7uK1shmS','694f2665c3e44e650f0e5dd8','{\"cookie\":{\"originalMaxAge\":15552000000,\"expires\":\"2026-06-25T00:45:45.076Z\",\"secure\":true,\"httpOnly\":true,\"path\":\"/ghost\",\"sameSite\":\"none\"},\"user_id\":\"694f2665c3e44e650f0e5dd8\",\"origin\":\"https://perfstack.dev\",\"user_agent\":\"Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/143.0.0.0 Safari/537.36\",\"ip\":\"2601:346:900:235d:e93a:71e3:7e64:1c37\",\"verified\":true}','2025-12-27 00:45:19','2025-12-27 00:45:45');
/*!40000 ALTER TABLE `sessions` ENABLE KEYS */;
UNLOCK TABLES;

--
-- Table structure for table `settings`
--

DROP TABLE IF EXISTS `settings`;
/*!40101 SET @saved_cs_client     = @@character_set_client */;
/*!50503 SET character_set_client = utf8mb4 */;
CREATE TABLE `settings` (
  `id` varchar(24) NOT NULL,
  `group` varchar(50) NOT NULL DEFAULT 'core',
  `key` varchar(50) NOT NULL,
  `value` text,
  `type` varchar(50) NOT NULL,
  `flags` varchar(50) DEFAULT NULL,
  `created_at` datetime NOT NULL,
  `updated_at` datetime DEFAULT NULL,
  PRIMARY KEY (`id`),
  UNIQUE KEY `settings_key_unique` (`key`)
) ENGINE=InnoDB DEFAULT CHARSET=utf8mb4 COLLATE=utf8mb4_0900_ai_ci;
/*!40101 SET character_set_client = @saved_cs_client */;

--
-- Dumping data for table `settings`
--

LOCK TABLES `settings` WRITE;
/*!40000 ALTER TABLE `settings` DISABLE KEYS */;
INSERT INTO `settings` VALUES ('694f2666c3e44e650f0e6021','core','last_mentions_report_email_timestamp',NULL,'number',NULL,'2025-12-26 19:20:54','2025-12-26 19:20:54'),('694f2666c3e44e650f0e6022','core','db_hash','f65b9a89-47db-44a6-975a-27ddbd1bc743','string',NULL,'2025-12-26 19:20:54','2025-12-26 19:20:54'),('694f2666c3e44e650f0e6023','core','routes_hash','3d180d52c663d173a6be791ef411ed01','string',NULL,'2025-12-26 19:20:54','2025-12-27 00:20:55'),('694f2666c3e44e650f0e6024','core','next_update_check','1767188740','number',NULL,'2025-12-26 19:20:54','2025-12-30 13:45:39'),('694f2666c3e44e650f0e6025','core','notifications','[{\"dismissible\":true,\"location\":\"bottom\",\"status\":\"alert\",\"id\":\"694f267ec3e44e650f0e60b0\",\"createdAtVersion\":\"6.10.3\",\"type\":\"warn\",\"message\":\"Ghost is currently unable to send email. See https://ghost.org/docs/concepts/config/#mail for instructions.\",\"seen\":false,\"addedAt\":\"2025-12-27T00:21:18.189Z\"}]','array',NULL,'2025-12-26 19:20:54','2025-12-27 00:21:18'),('694f2666c3e44e650f0e6026','core','version_notifications','[]','array',NULL,'2025-12-26 19:20:54','2025-12-26 19:20:54'),('694f2666c3e44e650f0e6027','core','admin_session_secret','00c38d0768005006f6e0558cdf8985dbcb887ec1c1d9a040adfbdb51e2a9e172','string',NULL,'2025-12-26 19:20:54','2025-12-26 19:20:54'),('694f2666c3e44e650f0e6028','core','theme_session_secret','31c19922583bce601bad6b00fc8f5cd38ed16842239add05f536e271e8076f5a','string',NULL,'2025-12-26 19:20:54','2025-12-26 19:20:54'),('694f2666c3e44e650f0e6029','core','ghost_public_key','-----BEGIN RSA PUBLIC KEY-----\nMIGJAoGBAJUy8d4yNoonmeHuGJHcAFIZQ5E7+Pyf5Nxj5qlb3ZztIsKxiFj9kyLI8+vi0Jmh\nc13W499zkKS3MV4dygyKrhEfE6q0OapUNV5ONNTpRxPfDr9Q6MLn84wrC/CxiInGG0+6GV90\nFIdzYYzw3h48DguWCvGsYPZKCHaOTTk4n3E9AgMBAAE=\n-----END RSA PUBLIC KEY-----\n','string',NULL,'2025-12-26 19:20:54','2025-12-26 19:20:54'),('694f2666c3e44e650f0e602a','core','ghost_private_key','-----BEGIN RSA PRIVATE KEY-----\nMIICXAIBAAKBgQCVMvHeMjaKJ5nh7hiR3ABSGUORO/j8n+TcY+apW92c7SLCsYhY/ZMiyPPr\n4tCZoXNd1uPfc5CktzFeHcoMiq4RHxOqtDmqVDVeTjTU6UcT3w6/UOjC5/OMKwvwsYiJxhtP\nuhlfdBSHc2GM8N4ePA4LlgrxrGD2Sgh2jk05OJ9xPQIDAQABAoGAELNrL5wfTsewCgJE7c7a\nmlYJojA8ZsrfK2xjXYHTBMXmpjSJNqzr03D7HOR+xTO1XkO3rkOZZVa8R/S9sTm00jly3P2m\nWJy46XbMW28WwpUIU1uQpsufHaA929HiAijXACrpO53FS1Zt10P8JhJc0S1qHd5PnPpd9AvY\nYuPfSAECQQDm9XiWQNYW93sj0KQLGlvFIVtj2GhXy6OmG+jXg/FbRvSG6VQHDPEY4yQaszZU\nSJN/wJqJxbxGX1VqFb9V017BAkEApWAidzC5VOsJbBKYUfREq9UWvS+J34Nu0rRs6654NpWB\n8CNBD8EyjzdNIoqW/2ZtZpOl2La/8odntYKeB5JtfQJASzNkJzBHRUUhoEDAGSlk9iPtWO5O\n0z9oMp7lyr6YaMsKtZ7lkEIjgqH60ILHztuuwysYZoGu0w8Zhag9G5nuQQJABiEbBenjZJdV\ngRDrXlY8WVYLgrWJWaRobhzL0KxlvjgqWbkkxy207wPwsvAKCqtEMMIWKoMTeCy2lk6iBsvR\njQJBAJ1OlplFIgu8kbb/kxLVIDkjWUBbGnkSN/HqldJoplO6ixXqJeVLZQ9Y6cXjnlUiWI1b\nnx3s5SzffZS7M+8K+wY=\n-----END RSA PRIVATE KEY-----\n','string',NULL,'2025-12-26 19:20:54','2025-12-26 19:20:54'),('694f2666c3e44e650f0e602b','core','members_public_key','-----BEGIN RSA PUBLIC KEY-----\nMIGJAoGBAKb05zWZPPLAbfxWd6RpccCCkfSEjkkMN5MrOFMOihtJOukSRWy0/EiY8DCl5bIZ\nLv3MEigo4k3dXmreUO52B2l21At74MHTCUeZHmUaNozi5yBq1skHjHwmyQ7qqQ5noaWGUeTz\nr1BmUyRUIfN5DdZekgCXmrOD7r/uFS0SjB/JAgMBAAE=\n-----END RSA PUBLIC KEY-----\n','string',NULL,'2025-12-26 19:20:54','2025-12-26 19:20:54'),('694f2666c3e44e650f0e602c','core','members_private_key','-----BEGIN RSA PRIVATE KEY-----\nMIICXQIBAAKBgQCm9Oc1mTzywG38VnekaXHAgpH0hI5JDDeTKzhTDoobSTrpEkVstPxImPAw\npeWyGS79zBIoKOJN3V5q3lDudgdpdtQLe+DB0wlHmR5lGjaM4ucgatbJB4x8JskO6qkOZ6Gl\nhlHk869QZlMkVCHzeQ3WXpIAl5qzg+6/7hUtEowfyQIDAQABAoGBAI8HxkA7zGQnCNKh/ROE\nJiiHc6ToHsFjzwSshS9aiGSB4w8jUQilMHC9XSBAL7zZKFHTj0aKZ/3PBQFG/kycHxIPKezo\nvAggS3jyZEFlkADwXeXEYjtQ8BBPTidwyZsoa+jSF/jpmGH6rm6OYNXyodausq8PMgku3K9/\nCKmqNgLRAkEA2rTJPi0hN+eHs348hcQz8GZ1R3wVcAFF6W7Py/wQ7wRqxAmMAGXjD7RSAVGy\nF8G2gIU+DzZvtLfwkp3x7JY+1QJBAMNtFix/nKDlo5UUNy48gVEFD6PYLwd/wJ4yYN2o7YDT\nZGW6T4XkGb3wh2e7M76iQRrF8gyVc83ydtQ1WIt0XyUCQQDMX4h3G8Eg9rEuoFZ6QgCvM2+A\n34lA40+rdauk2OYSFTSYALvdW1i1wjGnneoonRKoQtrdaGc2n3Sl7ga2qSgJAkBaW6/q9z4W\nbtdJ8MTqzfME4RAwM57bGsiW3LhJBPh7nkJHfvOR9ruoRPR2k+oC9MaheHDIPLoEuV0UFW5R\nXIc5AkAejZTd+whjGu0xEyzQ3w86Yfol6CjSvZRq4sBoODapWCxkNpJ0j9MepuEmwW72Xph+\nufN+kPJ+P6lIVeiMzZRQ\n-----END RSA PRIVATE KEY-----\n','string',NULL,'2025-12-26 19:20:54','2025-12-26 19:20:54'),('694f2666c3e44e650f0e602d','core','members_email_auth_secret','6e2ab2ea64490a4d42cab0d00932e51ec643e17e23b23f2cf0b8dc68ccceb186393022b45d2a464b8b8eaf7686b2949f3a2917baa40a016df7d27f0be2b65a2a','string',NULL,'2025-12-26 19:20:54','2025-12-26 19:20:54'),('694f2666c3e44e650f0e602e','core','members_stripe_webhook_id',NULL,'string',NULL,'2025-12-26 19:20:54','2025-12-26 19:20:54'),('694f2666c3e44e650f0e602f','core','members_stripe_webhook_secret',NULL,'string',NULL,'2025-12-26 19:20:54','2025-12-26 19:20:54'),('694f2666c3e44e650f0e6030','core','members_otc_secret','a73ac404f65ad3b57aace5cd70862c30f25bfe7a3cfc0b2b86071f546f05596577f9056a4c4138c3b0c7ed5e058ca2cd15b22c154dc6975ecf58b566f8874e03','string',NULL,'2025-12-26 19:20:54','2025-12-26 19:20:54'),('694f2666c3e44e650f0e6031','core','site_uuid','9432f6a0-3ed4-411f-a68d-fa288071ec6a','string','PUBLIC,RO','2025-12-26 19:20:54','2025-12-26 19:20:54'),('694f2666c3e44e650f0e6032','site','title','PerfStack  Performance Engineering, Backend & Web Speed','string','PUBLIC','2025-12-26 19:20:54','2025-12-27 01:00:45'),('694f2666c3e44e650f0e6033','site','description','Performance engineering, backend optimization, scalability, caching, and web speed explained clearly for engineers.','string','PUBLIC','2025-12-26 19:20:54','2025-12-27 01:00:45'),('694f2666c3e44e650f0e6034','site','heading_font','','string',NULL,'2025-12-26 19:20:54','2025-12-26 19:20:54'),('694f2666c3e44e650f0e6035','site','body_font','','string',NULL,'2025-12-26 19:20:54','2025-12-26 19:20:54'),('694f2666c3e44e650f0e6036','site','logo','__GHOST_URL__/content/images/2025/12/freepik__minimal_tech_logo-_performance_engineering-_b--3-.png','string',NULL,'2025-12-26 19:20:54','2025-12-30 13:47:10'),('694f2666c3e44e650f0e6037','site','cover_image','https://images.unsplash.com/photo-1766324934839-313529832615?crop=entropy&cs=tinysrgb&fit=max&fm=jpg&ixid=M3wxMTc3M3wwfDF8YWxsfDc1fHx8fHx8fHwxNzY2NzkzODE0fA&ixlib=rb-4.1.0&q=80&w=2000','string',NULL,'2025-12-26 19:20:54','2025-12-27 00:21:47'),('694f2666c3e44e650f0e6038','site','icon','__GHOST_URL__/content/images/2025/12/freepik__minimal_tech_logo-_performance_engineering-_b--3--1.png','string',NULL,'2025-12-26 19:20:54','2025-12-30 13:47:10'),('694f2666c3e44e650f0e6039','site','accent_color','#FF1A75','string','PUBLIC','2025-12-26 19:20:54','2025-12-26 19:20:54'),('694f2666c3e44e650f0e603a','site','locale','en','string',NULL,'2025-12-26 19:20:54','2025-12-26 19:20:54'),('694f2666c3e44e650f0e603b','site','timezone','Etc/UTC','string',NULL,'2025-12-26 19:20:54','2025-12-26 19:20:54'),('694f2666c3e44e650f0e603c','site','codeinjection_head','<!-- Google Analytics -->\n<script async src=\"https://www.googletagmanager.com/gtag/js?id=G-2XNWVKGRCP\"></script>\n<script>\n  window.dataLayer = window.dataLayer || [];\n  function gtag(){dataLayer.push(arguments);}\n  gtag(\'js\', new Date());\n\n  gtag(\'config\', \'G-2XNWVKGRCP\');\n</script>\n\n<!-- Google AdSense -->\n<script async \n  src=\"https://pagead2.googlesyndication.com/pagead/js/adsbygoogle.js?client=ca-pub-2111779448500294\"\n  crossorigin=\"anonymous\">\n</script>\n','string',NULL,'2025-12-26 19:20:54','2025-12-28 20:39:45'),('694f2666c3e44e650f0e603d','site','codeinjection_foot','','string',NULL,'2025-12-26 19:20:54','2025-12-26 19:20:54'),('694f2666c3e44e650f0e603e','site','facebook','ghost','string',NULL,'2025-12-26 19:20:54','2025-12-26 19:20:54'),('694f2666c3e44e650f0e603f','site','twitter','@ghost','string',NULL,'2025-12-26 19:20:54','2025-12-26 19:20:54'),('694f2666c3e44e650f0e6040','site','navigation','[{\"url\":\"/\",\"label\":\"Home\"},{\"url\":\"/about/\",\"label\":\"About\"},{\"url\":\"/privacy-policy/\",\"label\":\"Privacy Policy\"},{\"url\":\"/terms-of-service/\",\"label\":\"Terms of Service\"},{\"url\":\"/contact/\",\"label\":\"Contact\"}]','array',NULL,'2025-12-26 19:20:54','2025-12-29 00:17:50'),('694f2666c3e44e650f0e6041','site','secondary_navigation','[{\"label\":\"Sign up\",\"url\":\"#/portal/\"}]','array',NULL,'2025-12-26 19:20:54','2025-12-26 19:20:54'),('694f2666c3e44e650f0e6042','site','meta_title',NULL,'string',NULL,'2025-12-26 19:20:54','2025-12-26 19:20:54'),('694f2666c3e44e650f0e6043','site','meta_description',NULL,'string',NULL,'2025-12-26 19:20:54','2025-12-26 19:20:54'),('694f2666c3e44e650f0e6044','site','og_image',NULL,'string',NULL,'2025-12-26 19:20:54','2025-12-26 19:20:54'),('694f2666c3e44e650f0e6045','site','og_title',NULL,'string',NULL,'2025-12-26 19:20:54','2025-12-26 19:20:54'),('694f2666c3e44e650f0e6046','site','og_description',NULL,'string',NULL,'2025-12-26 19:20:54','2025-12-26 19:20:54'),('694f2666c3e44e650f0e6047','site','twitter_image',NULL,'string',NULL,'2025-12-26 19:20:54','2025-12-26 19:20:54'),('694f2666c3e44e650f0e6048','site','twitter_title',NULL,'string',NULL,'2025-12-26 19:20:54','2025-12-26 19:20:54'),('694f2666c3e44e650f0e6049','site','twitter_description',NULL,'string',NULL,'2025-12-26 19:20:54','2025-12-26 19:20:54'),('694f2666c3e44e650f0e604a','theme','active_theme','source','string','RO','2025-12-26 19:20:54','2025-12-28 01:05:59'),('694f2666c3e44e650f0e604b','private','is_private','false','boolean',NULL,'2025-12-26 19:20:54','2025-12-26 19:20:54'),('694f2666c3e44e650f0e604c','private','password','','string',NULL,'2025-12-26 19:20:54','2025-12-26 19:20:54'),('694f2666c3e44e650f0e604d','private','public_hash','f74a92fb9fe69b517b343e10767257','string',NULL,'2025-12-26 19:20:54','2025-12-26 19:20:54'),('694f2666c3e44e650f0e604e','members','default_content_visibility','public','string',NULL,'2025-12-26 19:20:54','2025-12-26 19:20:54'),('694f2666c3e44e650f0e604f','members','default_content_visibility_tiers','[]','array',NULL,'2025-12-26 19:20:54','2025-12-26 19:20:54'),('694f2666c3e44e650f0e6050','members','members_signup_access','all','string',NULL,'2025-12-26 19:20:54','2025-12-26 19:20:54'),('694f2666c3e44e650f0e6051','members','members_support_address','noreply','string','PUBLIC,RO','2025-12-26 19:20:54','2025-12-26 19:20:54'),('694f2666c3e44e650f0e6052','members','stripe_secret_key',NULL,'string',NULL,'2025-12-26 19:20:54','2025-12-26 19:20:54'),('694f2666c3e44e650f0e6053','members','stripe_publishable_key',NULL,'string',NULL,'2025-12-26 19:20:54','2025-12-26 19:20:54'),('694f2666c3e44e650f0e6054','members','stripe_plans','[]','array',NULL,'2025-12-26 19:20:54','2025-12-26 19:20:54'),('694f2666c3e44e650f0e6055','members','stripe_connect_publishable_key',NULL,'string',NULL,'2025-12-26 19:20:54','2025-12-26 19:20:54'),('694f2666c3e44e650f0e6056','members','stripe_connect_secret_key',NULL,'string',NULL,'2025-12-26 19:20:54','2025-12-26 19:20:54'),('694f2666c3e44e650f0e6057','members','stripe_connect_livemode',NULL,'boolean',NULL,'2025-12-26 19:20:54','2025-12-26 19:20:54'),('694f2666c3e44e650f0e6058','members','stripe_connect_display_name',NULL,'string',NULL,'2025-12-26 19:20:54','2025-12-26 19:20:54'),('694f2666c3e44e650f0e6059','members','stripe_connect_account_id',NULL,'string',NULL,'2025-12-26 19:20:54','2025-12-26 19:20:54'),('694f2666c3e44e650f0e605a','members','members_monthly_price_id',NULL,'string',NULL,'2025-12-26 19:20:54','2025-12-26 19:20:54'),('694f2666c3e44e650f0e605b','members','members_yearly_price_id',NULL,'string',NULL,'2025-12-26 19:20:54','2025-12-26 19:20:54'),('694f2666c3e44e650f0e605c','members','members_track_sources','true','boolean',NULL,'2025-12-26 19:20:54','2025-12-26 19:20:54'),('694f2666c3e44e650f0e605d','members','blocked_email_domains','[]','array',NULL,'2025-12-26 19:20:54','2025-12-26 19:20:54'),('694f2666c3e44e650f0e605e','portal','portal_name','true','boolean',NULL,'2025-12-26 19:20:54','2025-12-26 19:20:54'),('694f2666c3e44e650f0e605f','portal','portal_button','false','boolean',NULL,'2025-12-26 19:20:54','2025-12-26 19:20:54'),('694f2666c3e44e650f0e6060','portal','portal_plans','[\"free\"]','array',NULL,'2025-12-26 19:20:54','2025-12-26 19:20:54'),('694f2666c3e44e650f0e6061','portal','portal_default_plan','yearly','string',NULL,'2025-12-26 19:20:54','2025-12-26 19:20:54'),('694f2666c3e44e650f0e6062','portal','portal_products','[]','array',NULL,'2025-12-26 19:20:54','2025-12-26 19:20:54'),('694f2666c3e44e650f0e6063','portal','portal_button_style','icon-and-text','string',NULL,'2025-12-26 19:20:54','2025-12-26 19:20:54'),('694f2666c3e44e650f0e6064','portal','portal_button_icon',NULL,'string',NULL,'2025-12-26 19:20:54','2025-12-26 19:20:54'),('694f2666c3e44e650f0e6065','portal','portal_button_signup_text','Subscribe','string',NULL,'2025-12-26 19:20:54','2025-12-26 19:20:54'),('694f2666c3e44e650f0e6066','portal','portal_signup_terms_html',NULL,'string',NULL,'2025-12-26 19:20:54','2025-12-26 19:20:54'),('694f2666c3e44e650f0e6067','portal','portal_signup_checkbox_required','false','boolean',NULL,'2025-12-26 19:20:54','2025-12-26 19:20:54'),('694f2666c3e44e650f0e6068','email','mailgun_domain',NULL,'string',NULL,'2025-12-26 19:20:54','2025-12-26 19:20:54'),('694f2666c3e44e650f0e6069','email','mailgun_api_key',NULL,'string',NULL,'2025-12-26 19:20:54','2025-12-26 19:20:54'),('694f2666c3e44e650f0e606a','email','mailgun_base_url',NULL,'string',NULL,'2025-12-26 19:20:54','2025-12-26 19:20:54'),('694f2666c3e44e650f0e606b','email','email_track_opens','true','boolean',NULL,'2025-12-26 19:20:54','2025-12-26 19:20:54'),('694f2666c3e44e650f0e606c','email','email_track_clicks','true','boolean',NULL,'2025-12-26 19:20:54','2025-12-26 19:20:54'),('694f2666c3e44e650f0e606d','email','email_verification_required','false','boolean','RO','2025-12-26 19:20:54','2025-12-26 19:20:54'),('694f2666c3e44e650f0e606e','firstpromoter','firstpromoter','false','boolean',NULL,'2025-12-26 19:20:54','2025-12-26 19:20:54'),('694f2666c3e44e650f0e606f','firstpromoter','firstpromoter_id',NULL,'string',NULL,'2025-12-26 19:20:54','2025-12-26 19:20:54'),('694f2666c3e44e650f0e6070','labs','labs','{}','object',NULL,'2025-12-26 19:20:54','2025-12-26 19:20:54'),('694f2666c3e44e650f0e6071','slack','slack_url','','string',NULL,'2025-12-26 19:20:54','2025-12-26 19:20:54'),('694f2666c3e44e650f0e6072','slack','slack_username','Ghost','string',NULL,'2025-12-26 19:20:54','2025-12-26 19:20:54'),('694f2666c3e44e650f0e6073','unsplash','unsplash','true','boolean',NULL,'2025-12-26 19:20:54','2025-12-26 19:20:54'),('694f2666c3e44e650f0e6074','views','shared_views','[]','array',NULL,'2025-12-26 19:20:54','2025-12-26 19:20:54'),('694f2666c3e44e650f0e6075','editor','editor_default_email_recipients','visibility','string',NULL,'2025-12-26 19:20:54','2025-12-26 19:20:54'),('694f2666c3e44e650f0e6076','editor','editor_default_email_recipients_filter','all','string',NULL,'2025-12-26 19:20:54','2025-12-26 19:20:54'),('694f2666c3e44e650f0e6077','announcement','announcement_content',NULL,'string','PUBLIC','2025-12-26 19:20:54','2025-12-26 19:20:54'),('694f2666c3e44e650f0e6078','announcement','announcement_visibility','[]','array',NULL,'2025-12-26 19:20:54','2025-12-26 19:20:54'),('694f2666c3e44e650f0e6079','announcement','announcement_background','dark','string','PUBLIC','2025-12-26 19:20:54','2025-12-26 19:20:54'),('694f2666c3e44e650f0e607a','comments','comments_enabled','off','string',NULL,'2025-12-26 19:20:54','2025-12-26 19:20:54'),('694f2666c3e44e650f0e607b','analytics','outbound_link_tagging','true','boolean',NULL,'2025-12-26 19:20:54','2025-12-26 19:20:54'),('694f2666c3e44e650f0e607c','analytics','web_analytics','true','boolean',NULL,'2025-12-26 19:20:54','2025-12-26 19:20:54'),('694f2666c3e44e650f0e607d','pintura','pintura','true','boolean',NULL,'2025-12-26 19:20:54','2025-12-26 19:20:54'),('694f2666c3e44e650f0e607e','pintura','pintura_js_url',NULL,'string',NULL,'2025-12-26 19:20:54','2025-12-26 19:20:54'),('694f2666c3e44e650f0e607f','pintura','pintura_css_url',NULL,'string',NULL,'2025-12-26 19:20:54','2025-12-26 19:20:54'),('694f2666c3e44e650f0e6080','donations','donations_currency','USD','string',NULL,'2025-12-26 19:20:54','2025-12-26 19:20:54'),('694f2666c3e44e650f0e6081','donations','donations_suggested_amount','500','number',NULL,'2025-12-26 19:20:54','2025-12-26 19:20:54'),('694f2666c3e44e650f0e6082','recommendations','recommendations_enabled','false','boolean',NULL,'2025-12-26 19:20:54','2025-12-26 19:20:54'),('694f2666c3e44e650f0e6083','security','require_email_mfa','0','boolean',NULL,'2025-12-26 19:20:54','2025-12-26 19:20:54'),('694f2666c3e44e650f0e6084','social_web','social_web','true','boolean',NULL,'2025-12-26 19:20:54','2025-12-26 19:20:54'),('694f2666c3e44e650f0e6085','explore','explore_ping','true','boolean',NULL,'2025-12-26 19:20:54','2025-12-26 19:20:54'),('694f2666c3e44e650f0e6086','explore','explore_ping_growth','false','boolean',NULL,'2025-12-26 19:20:54','2025-12-26 19:20:54');
/*!40000 ALTER TABLE `settings` ENABLE KEYS */;
UNLOCK TABLES;

--
-- Table structure for table `snippets`
--

DROP TABLE IF EXISTS `snippets`;
/*!40101 SET @saved_cs_client     = @@character_set_client */;
/*!50503 SET character_set_client = utf8mb4 */;
CREATE TABLE `snippets` (
  `id` varchar(24) NOT NULL,
  `name` varchar(191) NOT NULL,
  `mobiledoc` longtext NOT NULL,
  `lexical` longtext,
  `created_at` datetime NOT NULL,
  `updated_at` datetime DEFAULT NULL,
  PRIMARY KEY (`id`),
  UNIQUE KEY `snippets_name_unique` (`name`)
) ENGINE=InnoDB DEFAULT CHARSET=utf8mb4 COLLATE=utf8mb4_0900_ai_ci;
/*!40101 SET character_set_client = @saved_cs_client */;

--
-- Dumping data for table `snippets`
--

LOCK TABLES `snippets` WRITE;
/*!40000 ALTER TABLE `snippets` DISABLE KEYS */;
/*!40000 ALTER TABLE `snippets` ENABLE KEYS */;
UNLOCK TABLES;

--
-- Table structure for table `stripe_prices`
--

DROP TABLE IF EXISTS `stripe_prices`;
/*!40101 SET @saved_cs_client     = @@character_set_client */;
/*!50503 SET character_set_client = utf8mb4 */;
CREATE TABLE `stripe_prices` (
  `id` varchar(24) NOT NULL,
  `stripe_price_id` varchar(255) NOT NULL,
  `stripe_product_id` varchar(255) NOT NULL,
  `active` tinyint(1) NOT NULL,
  `nickname` varchar(255) DEFAULT NULL,
  `currency` varchar(191) NOT NULL,
  `amount` int NOT NULL,
  `type` varchar(50) NOT NULL DEFAULT 'recurring',
  `interval` varchar(50) DEFAULT NULL,
  `description` varchar(191) DEFAULT NULL,
  `created_at` datetime NOT NULL,
  `updated_at` datetime DEFAULT NULL,
  PRIMARY KEY (`id`),
  UNIQUE KEY `stripe_prices_stripe_price_id_unique` (`stripe_price_id`),
  KEY `stripe_prices_stripe_product_id_foreign` (`stripe_product_id`),
  CONSTRAINT `stripe_prices_stripe_product_id_foreign` FOREIGN KEY (`stripe_product_id`) REFERENCES `stripe_products` (`stripe_product_id`)
) ENGINE=InnoDB DEFAULT CHARSET=utf8mb4 COLLATE=utf8mb4_0900_ai_ci;
/*!40101 SET character_set_client = @saved_cs_client */;

--
-- Dumping data for table `stripe_prices`
--

LOCK TABLES `stripe_prices` WRITE;
/*!40000 ALTER TABLE `stripe_prices` DISABLE KEYS */;
/*!40000 ALTER TABLE `stripe_prices` ENABLE KEYS */;
UNLOCK TABLES;

--
-- Table structure for table `stripe_products`
--

DROP TABLE IF EXISTS `stripe_products`;
/*!40101 SET @saved_cs_client     = @@character_set_client */;
/*!50503 SET character_set_client = utf8mb4 */;
CREATE TABLE `stripe_products` (
  `id` varchar(24) NOT NULL,
  `product_id` varchar(24) DEFAULT NULL,
  `stripe_product_id` varchar(255) NOT NULL,
  `created_at` datetime NOT NULL,
  `updated_at` datetime DEFAULT NULL,
  PRIMARY KEY (`id`),
  UNIQUE KEY `stripe_products_stripe_product_id_unique` (`stripe_product_id`),
  KEY `stripe_products_product_id_foreign` (`product_id`),
  CONSTRAINT `stripe_products_product_id_foreign` FOREIGN KEY (`product_id`) REFERENCES `products` (`id`)
) ENGINE=InnoDB DEFAULT CHARSET=utf8mb4 COLLATE=utf8mb4_0900_ai_ci;
/*!40101 SET character_set_client = @saved_cs_client */;

--
-- Dumping data for table `stripe_products`
--

LOCK TABLES `stripe_products` WRITE;
/*!40000 ALTER TABLE `stripe_products` DISABLE KEYS */;
/*!40000 ALTER TABLE `stripe_products` ENABLE KEYS */;
UNLOCK TABLES;

--
-- Table structure for table `subscriptions`
--

DROP TABLE IF EXISTS `subscriptions`;
/*!40101 SET @saved_cs_client     = @@character_set_client */;
/*!50503 SET character_set_client = utf8mb4 */;
CREATE TABLE `subscriptions` (
  `id` varchar(24) NOT NULL,
  `type` varchar(50) NOT NULL,
  `status` varchar(50) NOT NULL,
  `member_id` varchar(24) NOT NULL,
  `tier_id` varchar(24) NOT NULL,
  `cadence` varchar(50) DEFAULT NULL,
  `currency` varchar(50) DEFAULT NULL,
  `amount` int DEFAULT NULL,
  `payment_provider` varchar(50) DEFAULT NULL,
  `payment_subscription_url` varchar(2000) DEFAULT NULL,
  `payment_user_url` varchar(2000) DEFAULT NULL,
  `offer_id` varchar(24) DEFAULT NULL,
  `expires_at` datetime DEFAULT NULL,
  `created_at` datetime NOT NULL,
  `updated_at` datetime DEFAULT NULL,
  PRIMARY KEY (`id`),
  KEY `subscriptions_member_id_foreign` (`member_id`),
  KEY `subscriptions_tier_id_foreign` (`tier_id`),
  KEY `subscriptions_offer_id_foreign` (`offer_id`),
  CONSTRAINT `subscriptions_member_id_foreign` FOREIGN KEY (`member_id`) REFERENCES `members` (`id`) ON DELETE CASCADE,
  CONSTRAINT `subscriptions_offer_id_foreign` FOREIGN KEY (`offer_id`) REFERENCES `offers` (`id`),
  CONSTRAINT `subscriptions_tier_id_foreign` FOREIGN KEY (`tier_id`) REFERENCES `products` (`id`)
) ENGINE=InnoDB DEFAULT CHARSET=utf8mb4 COLLATE=utf8mb4_0900_ai_ci;
/*!40101 SET character_set_client = @saved_cs_client */;

--
-- Dumping data for table `subscriptions`
--

LOCK TABLES `subscriptions` WRITE;
/*!40000 ALTER TABLE `subscriptions` DISABLE KEYS */;
/*!40000 ALTER TABLE `subscriptions` ENABLE KEYS */;
UNLOCK TABLES;

--
-- Table structure for table `suppressions`
--

DROP TABLE IF EXISTS `suppressions`;
/*!40101 SET @saved_cs_client     = @@character_set_client */;
/*!50503 SET character_set_client = utf8mb4 */;
CREATE TABLE `suppressions` (
  `id` varchar(24) NOT NULL,
  `email` varchar(191) NOT NULL,
  `email_id` varchar(24) DEFAULT NULL,
  `reason` varchar(50) NOT NULL,
  `created_at` datetime NOT NULL,
  PRIMARY KEY (`id`),
  UNIQUE KEY `suppressions_email_unique` (`email`),
  KEY `suppressions_email_id_foreign` (`email_id`),
  CONSTRAINT `suppressions_email_id_foreign` FOREIGN KEY (`email_id`) REFERENCES `emails` (`id`)
) ENGINE=InnoDB DEFAULT CHARSET=utf8mb4 COLLATE=utf8mb4_0900_ai_ci;
/*!40101 SET character_set_client = @saved_cs_client */;

--
-- Dumping data for table `suppressions`
--

LOCK TABLES `suppressions` WRITE;
/*!40000 ALTER TABLE `suppressions` DISABLE KEYS */;
/*!40000 ALTER TABLE `suppressions` ENABLE KEYS */;
UNLOCK TABLES;

--
-- Table structure for table `tags`
--

DROP TABLE IF EXISTS `tags`;
/*!40101 SET @saved_cs_client     = @@character_set_client */;
/*!50503 SET character_set_client = utf8mb4 */;
CREATE TABLE `tags` (
  `id` varchar(24) NOT NULL,
  `name` varchar(191) NOT NULL,
  `slug` varchar(191) NOT NULL,
  `description` text,
  `feature_image` varchar(2000) DEFAULT NULL,
  `parent_id` varchar(191) DEFAULT NULL,
  `visibility` varchar(50) NOT NULL DEFAULT 'public',
  `og_image` varchar(2000) DEFAULT NULL,
  `og_title` varchar(300) DEFAULT NULL,
  `og_description` varchar(500) DEFAULT NULL,
  `twitter_image` varchar(2000) DEFAULT NULL,
  `twitter_title` varchar(300) DEFAULT NULL,
  `twitter_description` varchar(500) DEFAULT NULL,
  `meta_title` varchar(2000) DEFAULT NULL,
  `meta_description` varchar(2000) DEFAULT NULL,
  `codeinjection_head` text,
  `codeinjection_foot` text,
  `canonical_url` varchar(2000) DEFAULT NULL,
  `accent_color` varchar(50) DEFAULT NULL,
  `created_at` datetime NOT NULL,
  `updated_at` datetime DEFAULT NULL,
  PRIMARY KEY (`id`),
  UNIQUE KEY `tags_slug_unique` (`slug`)
) ENGINE=InnoDB DEFAULT CHARSET=utf8mb4 COLLATE=utf8mb4_0900_ai_ci;
/*!40101 SET character_set_client = @saved_cs_client */;

--
-- Dumping data for table `tags`
--

LOCK TABLES `tags` WRITE;
/*!40000 ALTER TABLE `tags` DISABLE KEYS */;
INSERT INTO `tags` VALUES ('694f2665c3e44e650f0e5dea','News','news',NULL,NULL,NULL,'public',NULL,NULL,NULL,NULL,NULL,NULL,NULL,NULL,NULL,NULL,NULL,NULL,'2025-12-27 00:20:53','2025-12-27 00:20:53'),('694f2cf27f4d7d685707e3f7','Hardware & Operating System','hardware-operating-system',NULL,NULL,NULL,'public',NULL,NULL,NULL,NULL,NULL,NULL,NULL,NULL,NULL,NULL,NULL,NULL,'2025-12-27 00:48:50','2025-12-27 00:48:50'),('694f2cf37f4d7d685707e3f9','CPU Optimization','cpu-optimization',NULL,NULL,NULL,'public',NULL,NULL,NULL,NULL,NULL,NULL,NULL,NULL,NULL,NULL,NULL,NULL,'2025-12-27 00:48:51','2025-12-27 00:48:51'),('694f2cf37f4d7d685707e3fb','Performance','performance',NULL,NULL,NULL,'public',NULL,NULL,NULL,NULL,NULL,NULL,NULL,NULL,NULL,NULL,NULL,NULL,'2025-12-27 00:48:51','2025-12-27 00:48:51'),('694f2cf37f4d7d685707e3fd','Optimization','optimization',NULL,NULL,NULL,'public',NULL,NULL,NULL,NULL,NULL,NULL,NULL,NULL,NULL,NULL,NULL,NULL,'2025-12-27 00:48:51','2025-12-27 00:48:51'),('694f2cf37f4d7d685707e3ff','System Design','system-design',NULL,NULL,NULL,'public',NULL,NULL,NULL,NULL,NULL,NULL,NULL,NULL,NULL,NULL,NULL,NULL,'2025-12-27 00:48:51','2025-12-27 00:48:51'),('694f2cf37f4d7d685707e401','Architecture','architecture',NULL,NULL,NULL,'public',NULL,NULL,NULL,NULL,NULL,NULL,NULL,NULL,NULL,NULL,NULL,NULL,'2025-12-27 00:48:51','2025-12-27 00:48:51'),('69532d98ebcfaae683eec988','Concurrency','concurrency',NULL,NULL,NULL,'public',NULL,NULL,NULL,NULL,NULL,NULL,NULL,NULL,NULL,NULL,NULL,NULL,'2025-12-30 01:40:40','2025-12-30 01:40:40'),('69532d98ebcfaae683eec98a','Threading','threading',NULL,NULL,NULL,'public',NULL,NULL,NULL,NULL,NULL,NULL,NULL,NULL,NULL,NULL,NULL,NULL,'2025-12-30 01:40:40','2025-12-30 01:40:40'),('69532d98ebcfaae683eec98c','Memory Management','memory-management',NULL,NULL,NULL,'public',NULL,NULL,NULL,NULL,NULL,NULL,NULL,NULL,NULL,NULL,NULL,NULL,'2025-12-30 01:40:40','2025-12-30 01:40:40'),('69532d99ebcfaae683eec98e','Cache Strategies','cache-strategies',NULL,NULL,NULL,'public',NULL,NULL,NULL,NULL,NULL,NULL,NULL,NULL,NULL,NULL,NULL,NULL,'2025-12-30 01:40:41','2025-12-30 01:40:41'),('69532d99ebcfaae683eec990','Scalability','scalability',NULL,NULL,NULL,'public',NULL,NULL,NULL,NULL,NULL,NULL,NULL,NULL,NULL,NULL,NULL,NULL,'2025-12-30 01:40:41','2025-12-30 01:40:41'),('69532d9aebcfaae683eec992','.NET Performance','net-performance',NULL,NULL,NULL,'public',NULL,NULL,NULL,NULL,NULL,NULL,NULL,NULL,NULL,NULL,NULL,NULL,'2025-12-30 01:40:42','2025-12-30 01:40:42'),('69532d9aebcfaae683eec994','C# Performance','c-performance',NULL,NULL,NULL,'public',NULL,NULL,NULL,NULL,NULL,NULL,NULL,NULL,NULL,NULL,NULL,NULL,'2025-12-30 01:40:42','2025-12-30 01:40:42'),('6954500cebcfaae683eec9ac','Compiler Optimization','compiler-optimization',NULL,NULL,NULL,'public',NULL,NULL,NULL,NULL,NULL,NULL,NULL,NULL,NULL,NULL,NULL,NULL,'2025-12-30 22:19:56','2025-12-30 22:19:56'),('6954500debcfaae683eec9ae','Profiling','profiling',NULL,NULL,NULL,'public',NULL,NULL,NULL,NULL,NULL,NULL,NULL,NULL,NULL,NULL,NULL,NULL,'2025-12-30 22:19:57','2025-12-30 22:19:57'),('6954500debcfaae683eec9b0','Algorithms','algorithms',NULL,NULL,NULL,'public',NULL,NULL,NULL,NULL,NULL,NULL,NULL,NULL,NULL,NULL,NULL,NULL,'2025-12-30 22:19:57','2025-12-30 22:19:57'),('6956d502ebcfaae683eec9c3','Operating System Tuning','operating-system-tuning',NULL,NULL,NULL,'public',NULL,NULL,NULL,NULL,NULL,NULL,NULL,NULL,NULL,NULL,NULL,NULL,'2026-01-01 20:11:46','2026-01-01 20:11:46'),('6956d502ebcfaae683eec9c5','Thread Pools','thread-pools',NULL,NULL,NULL,'public',NULL,NULL,NULL,NULL,NULL,NULL,NULL,NULL,NULL,NULL,NULL,NULL,'2026-01-01 20:11:46','2026-01-01 20:11:46'),('6956d502ebcfaae683eec9c7','Async/Await','async-await',NULL,NULL,NULL,'public',NULL,NULL,NULL,NULL,NULL,NULL,NULL,NULL,NULL,NULL,NULL,NULL,'2026-01-01 20:11:46','2026-01-01 20:11:46');
/*!40000 ALTER TABLE `tags` ENABLE KEYS */;
UNLOCK TABLES;

--
-- Table structure for table `tokens`
--

DROP TABLE IF EXISTS `tokens`;
/*!40101 SET @saved_cs_client     = @@character_set_client */;
/*!50503 SET character_set_client = utf8mb4 */;
CREATE TABLE `tokens` (
  `id` varchar(24) NOT NULL,
  `token` varchar(32) NOT NULL,
  `uuid` varchar(36) NOT NULL,
  `data` varchar(2000) DEFAULT NULL,
  `created_at` datetime NOT NULL,
  `updated_at` datetime DEFAULT NULL,
  `first_used_at` datetime DEFAULT NULL,
  `used_count` int unsigned NOT NULL DEFAULT '0',
  `otc_used_count` int unsigned NOT NULL DEFAULT '0',
  PRIMARY KEY (`id`),
  UNIQUE KEY `tokens_uuid_unique` (`uuid`),
  KEY `tokens_token_index` (`token`)
) ENGINE=InnoDB DEFAULT CHARSET=utf8mb4 COLLATE=utf8mb4_0900_ai_ci;
/*!40101 SET character_set_client = @saved_cs_client */;

--
-- Dumping data for table `tokens`
--

LOCK TABLES `tokens` WRITE;
/*!40000 ALTER TABLE `tokens` DISABLE KEYS */;
/*!40000 ALTER TABLE `tokens` ENABLE KEYS */;
UNLOCK TABLES;

--
-- Table structure for table `users`
--

DROP TABLE IF EXISTS `users`;
/*!40101 SET @saved_cs_client     = @@character_set_client */;
/*!50503 SET character_set_client = utf8mb4 */;
CREATE TABLE `users` (
  `id` varchar(24) NOT NULL,
  `name` varchar(191) NOT NULL,
  `slug` varchar(191) NOT NULL,
  `password` varchar(60) NOT NULL,
  `email` varchar(191) NOT NULL,
  `profile_image` varchar(2000) DEFAULT NULL,
  `cover_image` varchar(2000) DEFAULT NULL,
  `bio` text,
  `website` varchar(2000) DEFAULT NULL,
  `location` text,
  `facebook` varchar(2000) DEFAULT NULL,
  `twitter` varchar(2000) DEFAULT NULL,
  `threads` varchar(191) DEFAULT NULL,
  `bluesky` varchar(191) DEFAULT NULL,
  `mastodon` varchar(191) DEFAULT NULL,
  `tiktok` varchar(191) DEFAULT NULL,
  `youtube` varchar(191) DEFAULT NULL,
  `instagram` varchar(191) DEFAULT NULL,
  `linkedin` varchar(191) DEFAULT NULL,
  `accessibility` text,
  `status` varchar(50) NOT NULL DEFAULT 'active',
  `locale` varchar(6) DEFAULT NULL,
  `visibility` varchar(50) NOT NULL DEFAULT 'public',
  `meta_title` varchar(2000) DEFAULT NULL,
  `meta_description` varchar(2000) DEFAULT NULL,
  `tour` text,
  `last_seen` datetime DEFAULT NULL,
  `comment_notifications` tinyint(1) NOT NULL DEFAULT '1',
  `free_member_signup_notification` tinyint(1) NOT NULL DEFAULT '1',
  `paid_subscription_started_notification` tinyint(1) NOT NULL DEFAULT '1',
  `paid_subscription_canceled_notification` tinyint(1) NOT NULL DEFAULT '0',
  `mention_notifications` tinyint(1) NOT NULL DEFAULT '1',
  `recommendation_notifications` tinyint(1) NOT NULL DEFAULT '1',
  `milestone_notifications` tinyint(1) NOT NULL DEFAULT '1',
  `donation_notifications` tinyint(1) NOT NULL DEFAULT '1',
  `created_at` datetime NOT NULL,
  `updated_at` datetime DEFAULT NULL,
  PRIMARY KEY (`id`),
  UNIQUE KEY `users_slug_unique` (`slug`),
  UNIQUE KEY `users_email_unique` (`email`)
) ENGINE=InnoDB DEFAULT CHARSET=utf8mb4 COLLATE=utf8mb4_0900_ai_ci;
/*!40101 SET character_set_client = @saved_cs_client */;

--
-- Dumping data for table `users`
--

LOCK TABLES `users` WRITE;
/*!40000 ALTER TABLE `users` DISABLE KEYS */;
INSERT INTO `users` VALUES ('694f2665c3e44e650f0e5dd8','Manuel A Rodriguez','manuel','$2a$10$DefHeqk9zRpMcLNeh0MwWubOAVgY9cWO7.cKuYtWJukd63x69tVni','rdgztorres19@gmail.com',NULL,NULL,NULL,NULL,NULL,NULL,NULL,NULL,NULL,NULL,NULL,NULL,NULL,NULL,'{\"whatsNew\":{\"lastSeenDate\":\"2025-12-27T00:21:17.383Z\"},\"onboarding\":{\"completedSteps\":[\"customize-design\",\"first-post\",\"build-audience\",\"share-publication\"],\"checklistState\":\"completed\"},\"apOnboarding\":{\"welcomeStepsFinished\":true}}','active',NULL,'public',NULL,NULL,NULL,'2026-01-01 20:11:54',1,1,1,0,1,1,1,1,'2025-12-27 00:20:53','2026-01-01 20:11:54');
/*!40000 ALTER TABLE `users` ENABLE KEYS */;
UNLOCK TABLES;

--
-- Table structure for table `webhooks`
--

DROP TABLE IF EXISTS `webhooks`;
/*!40101 SET @saved_cs_client     = @@character_set_client */;
/*!50503 SET character_set_client = utf8mb4 */;
CREATE TABLE `webhooks` (
  `id` varchar(24) NOT NULL,
  `event` varchar(50) NOT NULL,
  `target_url` varchar(2000) NOT NULL,
  `name` varchar(191) DEFAULT NULL,
  `secret` varchar(191) DEFAULT NULL,
  `api_version` varchar(50) NOT NULL DEFAULT 'v2',
  `integration_id` varchar(24) NOT NULL,
  `last_triggered_at` datetime DEFAULT NULL,
  `last_triggered_status` varchar(50) DEFAULT NULL,
  `last_triggered_error` varchar(50) DEFAULT NULL,
  `created_at` datetime NOT NULL,
  `updated_at` datetime DEFAULT NULL,
  PRIMARY KEY (`id`),
  KEY `webhooks_integration_id_foreign` (`integration_id`),
  CONSTRAINT `webhooks_integration_id_foreign` FOREIGN KEY (`integration_id`) REFERENCES `integrations` (`id`) ON DELETE CASCADE
) ENGINE=InnoDB DEFAULT CHARSET=utf8mb4 COLLATE=utf8mb4_0900_ai_ci;
/*!40101 SET character_set_client = @saved_cs_client */;

--
-- Dumping data for table `webhooks`
--

LOCK TABLES `webhooks` WRITE;
/*!40000 ALTER TABLE `webhooks` DISABLE KEYS */;
/*!40000 ALTER TABLE `webhooks` ENABLE KEYS */;
UNLOCK TABLES;
/*!40103 SET TIME_ZONE=@OLD_TIME_ZONE */;

/*!40101 SET SQL_MODE=@OLD_SQL_MODE */;
/*!40014 SET FOREIGN_KEY_CHECKS=@OLD_FOREIGN_KEY_CHECKS */;
/*!40014 SET UNIQUE_CHECKS=@OLD_UNIQUE_CHECKS */;
/*!40101 SET CHARACTER_SET_CLIENT=@OLD_CHARACTER_SET_CLIENT */;
/*!40101 SET CHARACTER_SET_RESULTS=@OLD_CHARACTER_SET_RESULTS */;
/*!40101 SET COLLATION_CONNECTION=@OLD_COLLATION_CONNECTION */;
/*!40111 SET SQL_NOTES=@OLD_SQL_NOTES */;

-- Dump completed on 2026-01-02 20:29:45
