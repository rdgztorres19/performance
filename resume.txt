Prefer Fewer Fast CPU Cores Over Many Slow Ones for Latency-Sensitive Workloads
A common misconception in performance optimization is that more CPU cores always translate to better performance.
Cache Hierarchy: Fast cores typically feature larger, faster L1 and L2 caches per core
They also have better prefetching logic that predicts memory access patterns.

Sequential Bottlenecks (Amdahl's Law)
Every parallel algorithm has sequential portions that cannot be parallelized
The sequential portion becomes the bottleneck.

Context Switching Overhead
When a thread is paused, the OS must save registers, stack, and CPU state.
Fewer instructions per second
Context switches take longer in real time
With many threads, context switching overhead dominates

Lock / Mutex Contention
Shared resources require locks. Only one thread can enter the critical section at a time.
Hold locks longer (execute critical section code slowly)
Thread queues grow quickly
Fast cores: Enter, execute, and release locks quickly, Less waiting time for other threads

Cache Misses (Critical Factor)
Context switching evicts cache lines
Other threads overwrite cache
Execute fewer instructions before being preempted, Lose cache more often (context switches evict cache lines)
Cache misses dominate execution time (each miss costs 100-300ns)

Blocking I/O (Network, Disk, DB)
Many applications are I/O-bound, not CPU-bound. Threads wait for external resources (network responses, disk reads, database queries).
Process I/O results slowly when data arrives
Hold buffers and locks longer while processing

Common Mistakes
Assuming More Cores Always Help: Adding cores to applications with sequential bottlenecks without profiling to identify the actual bottleneck
Over-Parallelization: Creating excessive threads that contend for fast cores, leading to context switching overhead that negates single-thread advantages
Mismatched Architecture Patterns: Using request-per-thread models (many threads) with fast-core architectures instead of event-driven or async models
Cache-Unaware Algorithms: Implementing algorithms that ignore cache locality, wasting the cache advantages of fast cores

---------------------------------------
Reduce Context Switching
A common misconception is that increasing the number of threads automatically improves performance
Slower execution means threads stay active longer.
Blocking behavior worsens the situation. Threads that wait for I/O or locks stop and later resume. Each stop and resume adds more switching and more coordination work.
Longer Uninterrupted Execution Means Less Rework ()

Fragmented work often requires:
Re-checking conditions
Re-entering code paths
Re-establishing execution flow

When execution switches to another thread, different data is accessed and the previous data becomes less immediately available.
When the original thread resumes, accessing its data again takes longer than if it had continued running.

Less Time Spent Coordinating, More Time Doing Work
Context switches often interrupt threads that are actively making progress. These interruptions break the natural flow of execution and reduce efficiency.
When threads are interrupted frequently, tasks take longer to complete because they are repeatedly paused and resumed.

A time quantum (also called time slice) is the amount of CPU time a runnable thread is allowed to execute
Its effective length varies depending on:
-The operating system scheduler implementation
-The scheduling policy in use
-The number of runnable threads in the run queue
-Thread priorities
-Overall system load

What Causes a Thread to Context Switch
1. Time Quantum Expiration (Preemption)
2. Blocking Operations (Voluntary Switching)
3. Lock Contention (When a thread attempts to acquire a lock that is already held:)
4. Higher-Priority Threads Becoming Runnable
5. Thread Migration Between CPU Cores

How to Reduce Context Switching
Use Thread Pools Instead of Creating Threads
Limit Concurrency to CPU Cores for CPU-Bound Work (If there are more CPU-bound threads than CPU cores, the operating system must constantly rotate between them, interrupting each one even while it is making progress.)
Use Async / Non-Blocking I/O Instead of Blocking I/O (With blocking I/O, a thread starts an operation (like a network or disk call) and then stops doing anything while waiting for the result. That thread must later be restarted when the operation completes, causing additional context switches.
-With async I/O, the thread starts the operation and immediately returns. The operating system notifies the application later when the I/O is complete, without stopping a thread just to wait.
Minimize Lock Contention 
-When multiple threads need the same lock, some threads must wait. Waiting threads stop running and later resume when the lock becomes available. Each wait and resume introduces context switches.
Reduce Thread Migration Between CPU Cores
Avoid sleep, yield, and Busy Waiting

-----------------
Threads Affinity
the operating system is free to move threads between cores to balance load
While this is usually beneficial, it can hurt performance for latency-sensitive or cache-sensitive workloads.
Modern operating systems use schedulers designed to keep all CPU cores busy. To achieve this, they dynamically move processes and threads between cores to balance load
By default, operating systems do not strictly bind threads to specific CPU cores.

Instead, the scheduler uses a soft affinity strategy:
-It prefers to run a thread on the same core it ran on last
-But it is free to move the thread whenever it decides it is beneficial

Why Threads Are Migrated
To prevent some cores from being overloaded while others are idle
To ensure all runnable threads get CPU time
To spread heat and optimize energy usage

Why Thread Affinity Improves Performance
1. Preserving CPU Cache Locality

When a thread migrates to another core:
-Its data is no longer in the local cache
-The new core must fetch data again
-Execution stalls while data is reloaded

2. Fewer Cache Misses Means Fewer CPU Stalls
Cache misses force the CPU to wait for data. These waits add latency even if no context switch occurs.

4. Reduced Context Switching Pressure
Although affinity does not eliminate context switching, it reduces unnecessary switches caused by migration and rebalancing.

----------
Avoid False Sharing and Cache Line Contention
False sharing happens when multiple threads write to different variables that live inside the same CPU cache line (usually 64 bytes).
Even though the variables are logically independent, the CPU cache works with cache lines, not variables.
When one thread writes, other cores must invalidate their copy of the entire cache line. The cache line then bounces between cores, creating hidden serialization.
Instead, it loads memory in fixed-size blocks called cache lines
2 thread running in 2 different cores
If two threads write to anything inside the same cache line, they compete. This is false sharing.
Each CPU core has its own cache. Only one core can modify a cache line at a time,  Other cores must invalidate their copy before writing
The CPU's cache coherency protocol (MESI) controls everything automatically:

Why This Creates Serialization
Even though Thread 0 and Thread 1 are running on different cores (true parallelism), they cannot write simultaneously because:

Only one core can have the cache line in Modified state
The other core must wait for the transfer to complete
This creates implicit serialization at the hardware level

Result: What looks like parallel execution is actually serial execution with expensive synchronization.

Lock-free data structures avoid blocking but can suffer from false sharing when multiple threads update adjacent fields. Atomic operations still trigger cache invalidations.

How to Avoid False Sharing (General Principles)
Strategy 1: Per-Thread Data (Preferred)
Best approach: Give each thread its own copy of data. No sharing = no false sharing.

Strategy 2: Padding and Alignment
When per-thread data isn't feasible: Use padding to separate shared data into different cache lines

Strategy 3: Separate Data Structures
Design approach: Design data structures so hot fields written by different threads are naturally separated.

Strategy 4: Reduce Write Frequency
Optimization: Reduce how often threads write to shared data.

How to Avoid False Sharing in C#
Method 1: ThreadLocal (Best for Per-Thread Data)
Use when: Each thread needs its own accumulator, counter, or state.

-------
Branch prediction is a CPU optimization technique that guesses which path code will take at conditional statements (if/else, loops, switches) before the condition is actually evaluated.
CPU doesn't know which path to take until it evaluates the condition, but it needs to know now to keep the pipeline full.
CPU pipeline has stages like: fetch instruction, decode, execute, write result.
When the CPU encounters a branch, it must wait for the condition to be evaluated before knowing which instructions to load next. This stalls the pipeline, wasting cycles. To avoid this, CPUs use branch prediction‚Äîthey guess which path will be taken based on historical patterns.
Modern CPUs are fast because of optimizations like branch prediction. When prediction fails, you still pay the penalty. In tight loops with branches, this adds up quickly.
What is a CPU pipeline? Modern CPUs break instruction execution into stages. While one instruction is being executed, the next is being decoded, and the one after that is being fetched from memory.
Why is the penalty so high? The pipeline is deep (10-20 stages). When a misprediction is discovered, all instructions in the wrong path must be discarded, and the pipeline must restart. The deeper the pipeline, the higher the cost.
Impact: In tight loops with unpredictable branches, you might spend 10-20% of CPU time on misprediction penalties instead of actual computation.
Instruction cache misses: When a branch is mispredicted, the CPU might load instructions from the wrong path into the instruction cache. When it corrects, it must load the correct instructions, potentially causing cache misses.
Technique 1: Common Case First Principle: Put the most likely path first in if/else statements.
Technique 2: Separate Unpredictable Branches Principle: If you have a loop with an unpredictable branch, separate the filtering from the processing.

// ‚ùå Bad: Unpredictable branch in hot loop
int count = 0;
foreach (var item in items) {
    if (item.IsValid && item.Value > threshold) {  // Unpredictable
        Process(item);
        count++;
    }
}

// ‚úÖ Good: Separate filtering (branch once per item)
var validItems = items
    .Where(i => i.IsValid && i.Value > threshold)
    .ToList();  // Branch here, but only once per item

foreach (var item in validItems) {  // No branches in hot loop!
    Process(item);
}

Technique 3: Branchless Operations
Principle: Use arithmetic operations instead of branches when possible.

// ‚ùå Bad: Branch in hot loop
int count = 0;
foreach (var value in values) {
    if (value > threshold) {
        count++;
    }
}

// ‚úÖ Good: Branchless
int count = 0;
foreach (var value in values) {
    count += (value > threshold) ? 1 : 0;  // No branch, just arithmetic
}

// Or using LINQ (compiler may optimize)
int count = values.Count(v => v > threshold);

Technique 4: Sort Data for Predictable Comparisons
Principle: When comparing values in a loop, sorted data makes branches predictable.

Technique 5: Use Lookup Tables Instead of Switches
--------------
Avoid Busy-Wait Loops: Use Proper Synchronization Primitives
usy-wait loops continuously consume CPU cycles while waiting for a condition to become true, without yielding control to the operating system scheduler
Porque el scheduler s√≠ reparte tiempo‚Ä¶ pero el busy-wait nunca se duerme. Entonces cada vez que le toca correr, corre a m√°xima velocidad.
RUNNING ‚Üí RUNNABLE ‚Üí RUNNING
The solution is to use proper synchronization primitives (events, semaphores, condition variables) that allow the thread to sleep until the condition is met, or use async/await for non-blocking waits.
Yield: When a thread voluntarily gives up the CPU, allowing the scheduler to run other threads. Threads can yield explicitly or by calling blocking operations.
Adding Thread.Sleep() to busy-wait loops Why it's better but still wrong: Reduces CPU usage but introduces unnecessary latency. The thread wakes up every millisecond to check, even if the condition changed immediately after sleeping. Still not ideal.

Understanding Thread Scheduling
What happens when a thread runs: The OS scheduler assigns a CPU core to the thread. The thread executes instructions until:

It completes its work
It yields the CPU (explicitly or by blocking)
The scheduler preempts it (time slice expires, higher priority thread needs CPU)
What happens during busy-wait: The thread never yields. It executes the loop instructions continuously:

Event/Semaphore/Mutex: These are OS-provided synchronization mechanisms that:

Allow threads to wait without consuming CPU
Wake threads efficiently when conditions change
Use kernel mechanisms for fast signaling
Thread calls Wait() ‚Üí OS suspends thread, doesn't consume CPU

// ‚úÖ Good: Use ManualResetEventSlim
public class GoodWait {
    private readonly ManualResetEventSlim _event = new ManualResetEventSlim(false);
    
    public void WaitForFlag() {
        _event.Wait();  // Blocks, doesn't consume CPU
    }
    
    public void SetFlag() {
        _event.Set();  // Wakes waiting thread
    }
}

Technique 2: Use async/await for I/O

Technique 3: Use SpinWait for Very Short Waits
When: Wait time is guaranteed to be extremely short (nanoseconds to microseconds).
// ‚úÖ For very short waits, use SpinWait
public class OptimizedWait {
    private volatile bool _flag = false;
    
    public void WaitForFlag() {
        var spinWait = new SpinWait();
        while (!_flag) {
            spinWait.SpinOnce();  // Optimized for short waits
            // After some spins, yields to other threads
        }
    }
}
Why it works: SpinWait uses CPU hints and gradually increases back-off. For very short waits, it avoids context switch overhead. For longer waits, it yields to prevent waste.

Technique 4: Use TaskCompletionSource for Async Coordination
When: Coordinating async operations without blocking threads
// ‚úÖ Good: TaskCompletionSource for async coordination
public class AsyncWait {
    private readonly TaskCompletionSource<bool> _tcs = new TaskCompletionSource<bool>();
    
    public async Task WaitForFlagAsync() {
        await _tcs.Task;  // Non-blocking wait
    }
    
    public void SetFlag() {
        _tcs.SetResult(true);  // Completes the task
    }
}

Technique 5: Use Producer-Consumer Patterns
When: Threads need to wait for work items.
// ‚úÖ Good: Use BlockingCollection for producer-consumer

public class WorkQueue {
    private readonly BlockingCollection<WorkItem> _queue = new BlockingCollection<WorkItem>();
    
    // Producer
    public void AddWork(WorkItem item) {
        _queue.Add(item);  // Wakes waiting consumers
    }
    
    // Consumer
    public WorkItem GetWork() {
        return _queue.Take();  // Blocks until work available
    }
}

--------------------
Process Data in Batches Instead of Per-Item Execution
 This reduces function call overhead, improves CPU cache efficiency by keeping related data together, enables compiler optimizations like vectorization, and dramatically improves throughput for I/O and database operations (often 2-10x, sometimes 10-100x for I/O).
 The trade-off is increased latency for individual items (they wait for the batch to fill), higher memory usage to hold batches

The problem with per-item processing: Each item requires:

A separate function call (overhead: saving registers, stack management, parameter passing)
Separate memory access (poor cache locality)
Separate I/O operation (network round-trips, disk seeks)
Separate database query (connection overhead, query parsing, result handling)

Function call overhead: The cost of invoking a function: saving CPU registers, managing the call stack, passing parameters, and jumping to the function code. Modern CPUs minimize this, but it's still measurable, especially when called millions of times.

Cache locality: The principle that accessing data that's close together in memory is faster. CPUs cache memory in blocks (cache lines), so accessing nearby data means it's likely already in cache.

Vectorization: A CPU optimization where the processor performs the same operation on multiple data items simultaneously using SIMD (Single Instruction, Multiple Data) instructions. Instead of processing items one by one, the CPU processes 4, 8, or 16 items at once.

Function Call Overhead Reduction
With batching: Instead of 1000 function calls for 1000 items, you make 1 function call with 1000 items. The overhead is paid once, not 1000 times.

What is vectorization? Modern CPUs can perform the same operation on multiple data items simultaneously using SIMD instructions. For example, adding 8 integers at once instead of 8 separate additions.
Batch processing: When processing arrays/lists in batches, compilers can see patterns:
Same operation on multiple items
Sequential access
No dependencies between items
This enables automatic vectorization by the compiler.

I/O Operation Batching
Network I/O overhead: Each network operation has overhead:
Network round-trip time (latency)
Protocol overhead (headers, acknowledgments)
Connection management

Per-item I/O: 1000 items = 1000 network round-trips. Even if each round-trip is fast (1ms), total time is 1000ms (1 second) just for network overhead.

Batch I/O: 1000 items in 10 batches = 10 network round-trips. Total time: 10ms. 100x improvement just from reducing round-trips.

I/O Overhead Dominance
The problem: For I/O-bound operations (database, network, file), the overhead of each I/O operation dominates. Network round-trips, database query overhead, and disk seeks are much slower than the actual data processing.

Synchronization Overhead
The problem: In multi-threaded per-item processing, threads must coordinate frequently (synchronize). Each synchronization has overhead (acquiring locks, checking conditions).
Example: 1000 threads processing 1 item each. Each thread might need to:
Acquire a lock (overhead)
Update shared state (overhead)
Release lock (overhead)
Total synchronization overhead might be 50% of total time.

OS / I/O syscalls
(user ‚Üí kernel transitions)
‚ùå Single-item I/O
foreach (var line in lines)
{
    File.AppendAllText("data.log", line + "\n");
}

What happens internally
Each iteration:
open()
write()
close()

One syscall per item
User ‚Üí kernel ‚Üí user context switch every time
File system metadata touched repeatedly

‚úÖ Batched I/O
var sb = new StringBuilder();

foreach (var line in lines)
{
    sb.AppendLine(line);
}

File.AppendAllText("data.log", sb.ToString());

What improves
Single write() syscall
Kernel page cache used efficiently
Minimal context switches

++++
2Ô∏è‚É£ Function call overhead
‚ùå Single-item function calls
foreach (var x in data)
{
    ProcessItem(x);
}

void ProcessItem(int x)
{
    result += x * 2;
}

What happens
Method prologue/epilogue per item
Stack setup/teardown
Poor instruction cache locality
üî¥ High call overhead relative to useful work

‚úÖ Batched function call
ProcessBatch(data);

void ProcessBatch(int[] batch)
{
    for (int i = 0; i < batch.Length; i++)
    {
        result += batch[i] * 2;
    }
}

++++
3Ô∏è‚É£ Cache locality (L1 / L2)
Dont use linkect List use arrays

What improves
Sequential memory access
Cache line prefetching
Very low cache miss rate

+++
4Ô∏è‚É£ Synchronization / lock overhead
‚ùå Lock per item

foreach (var x in data)
{
    lock (_lock)
    {
        shared += x;
    }
}

What happens
Lock acquire/release per iteration
Cache line bouncing between cores
Potential thread contention

‚úÖ Lock once per batch
int local = 0;

foreach (var x in data)
{
    local += x;
}

lock (_lock)
{
    shared += local;
}

+++
5Ô∏è‚É£ Vectorization (SIMD)
‚ùå Scalar, per-item math
for (int i = 0; i < a.Length; i++)
{
    c[i] = a[i] + b[i];
}

What happens
One add per iteration
CPU uses scalar ALU
SIMD units idle
üî¥ Wasted hardware capability

‚úÖ Batched SIMD via System.Numerics
using System.Numerics;

int i = 0;
for (; i <= a.Length - Vector<int>.Count; i += Vector<int>.Count)
{
    var va = new Vector<int>(a, i);
    var vb = new Vector<int>(b, i);
    (va + vb).CopyTo(c, i);
}

for (; i < a.Length; i++)
{
    c[i] = a[i] + b[i];
}

What improves
4‚Äì16 elements
SIMD units fully utilized
üü¢ True data-parallel execution

++++
6Ô∏è‚É£ Allocation / GC pressure
‚ùå Allocate per item
foreach (var x in data)
{
    var obj = new TempObject(x);
    Process(obj);
}

What happens
Thousands of allocations
GC Gen0 pressure
GC pauses
üî¥ GC becomes bottleneck

‚úÖ Batched reuse
var buffer = new TempObject[data.Length];

for (int i = 0; i < data.Length; i++)
{
    buffer[i] = new TempObject(data[i]);
}

ProcessBatch(buffer);
What improves
Fewer allocations
Predictable memory layout
Lower GC frequency
üü¢ Stable memory behavior
+++++
7Ô∏è‚É£ Real OS-visible example (network I/O)
‚ùå Single send
foreach (var msg in messages)
{
    socket.Send(msg);
}
Internals
Syscall per send
TCP overhead per packet

‚úÖ Batched send
var buffers = messages.Select(m => new ArraySegment<byte>(m)).ToList();
socket.Send(buffers);
-------------------
Avoid Page Faults: Keep Working Set in Physical Memory

Page faults occur when the CPU accesses memory that isn't currently mapped in physical RAM, requiring the operating system to load the page from disk (swap) or allocate/map it.
Each page fault can cost thousands of CPU cycles (10,000-100,000+ cycles) and disk I/O latency (milliseconds)
What is a page fault? A page fault is an interrupt that occurs when a program accesses a virtual memory address that isn't currently mapped to physical RAM. The CPU triggers an exception, and the operating system must handle it by either loading the page from disk (swap) or allocating/mapping memory.

Virtual memory: An abstraction where programs see a large, continuous address space (virtual addresses) that the operating system maps to physical RAM. Virtual memory allows programs to use more memory than physically available by swapping unused pages to disk.
Page: A fixed-size block of memory (typically 4KB on x86-64, 16KB on some ARM systems). Virtual memory is divided into pages that can be independently mapped to physical memory or disk.
Page table: A data structure maintained by the OS that maps virtual addresses to physical addresses. The CPU's Memory Management Unit (MMU) uses page tables to translate virtual addresses.
Swap / Page file: Disk space used as virtual memory extension. When physical RAM is full, the OS moves less-used pages to swap, freeing RAM for active pages. Loading from swap is slow (disk I/O).

Types of Page Faults
Minor page fault:
Page exists in physical memory but isn't mapped in the page table
Common causes: Copy-on-write, shared memory mapping
Cost: 1,000-10,000 CPU cycles (just updating page table)
Impact: Moderate‚Äîslower than normal access but not catastrophic

Para qu√© se necesita que est√© mapeado?
Porque la CPU solo sabe leer y escribir en direcciones f√≠sicas de RAM, pero los programas trabajan con direcciones virtuales.


Major page fault:
Page must be loaded from disk (swap or file)
Cost: 10,000-100,000+ CPU cycles + disk I/O latency (5-10ms for disk, <1ms for SSD)
Impact: Severe‚Äîcan block execution for milliseconds
A major page fault is 50,000-100,000x slower than normal memory access!

How virtual memory works:
Program uses virtual addresses (e.g., 0x1000, 0x2000)
CPU's MMU translates virtual address to physical address using page tables
If page is mapped: Access proceeds normally (fast)
If page is not mapped: Page fault occurs (slow)

Memory allocation process:
Program requests memory (e.g., new byte[1MB])
OS reserves virtual address space (fast, just bookkeeping)
OS doesn't immediately allocate physical RAM (lazy allocation)
When program first accesses memory: Page fault occurs
OS allocates physical page and maps it

Why lazy allocation: OS defers physical allocation until first access to avoid wasting RAM on unused allocations. But this means first access triggers a page fault.

Copy-on-write (COW):
When memory is shared (fork, shared memory), OS uses copy-on-write
Initially, pages are shared (read-only in page table)
On write: Page fault occurs, OS copies page, updates page table
Creates minor page faults on first write to shared pages

Swap operation:
OS identifies pages to swap out (least recently used algorithm)
If page is dirty (modified), write to swap
Mark page as swapped in page table
Free physical RAM for new pages

CPU Pipeline Stalls
The problem: Page faults cause CPU pipeline stalls. The CPU must wait for memory access to complete before continuing execution.
What is a pipeline stall? Modern CPUs execute multiple instructions simultaneously in a pipeline. When an instruction needs data that isn't available (waiting for page fault), the pipeline stalls‚Äîno progress until data arrives.

Context Switch Overhead
The problem: Page faults trigger context switches to kernel mode. The OS page fault handler runs, which has overhead (saving/restoring registers, kernel processing).

Disk I/O Contention
The problem: When multiple processes have page faults, they compete for disk I/O bandwidth. Disk I/O becomes a bottleneck.

Memory Bandwidth Saturation
The problem: Frequent page faults can saturate memory bandwidth. Loading pages from swap competes with normal memory access

Technique 1: Pre-load Data into Memory
When: You have large data structures that will be accessed soon. Pre-loading ensures pages are in physical RAM before access

private byte[] _largeArray;
    
    public GoodMemoryAccess()
    {
        _largeArray = new byte[100_000_000];
        // Pre-load all pages by touching each page
        const int pageSize = 4096; // 4KB page size
        for (int i = 0; i < _largeArray.Length; i += pageSize)
        {
            _largeArray[i] = 0; // Touch each page to trigger allocation
        }
    }

Why it works: Touching each page (accessing at least one byte) triggers page allocation. OS allocates physical pages and maps them, eliminating page faults during actual access.
+++++++

Technique 2: Memory Locking (mlock / VirtualLock)
When: You have critical data that must never be swapped to disk. Locking prevents OS from swapping pages.
Why it works: Memory locking tells the OS never to swap these pages to disk. They remain in physical RAM, guaranteeing no page faults from swap.

Technique 3: Sequential Memory Access
When: Processing large datasets. Sequential access improves prefetching and reduces page faults.
Sequential access reduces page faults because the OS and CPU can predict what you will need next and load/map it before you touch it.

Technique 5: Working Set Management
When: Your working set might exceed available RAM. Manage what stays in memory.
// ‚úÖ Good: Keep hot data in memory, swap cold data explicitly
public class WorkingSetManager<T>
{
    private readonly Dictionary<string, T> _hotCache = new(); // Hot data
    private readonly Dictionary<string, T> _coldData = new(); // Cold data (can be swapped)
    
    public T GetData(string key)
    {
        // Hot data is always in memory
        if (_hotCache.TryGetValue(key, out var value))
        {
            return value;
        }
        
        // Cold data loaded on demand (may cause page fault, but acceptable)
        return LoadColdData(key);
    }
    
    public void PromoteToHot(string key, T data)
    {
        // Move to hot cache (ensure it's in memory)
        _hotCache[key] = data;
        // Pre-load to ensure in physical RAM
        EnsureInMemory(data);
    }
}
Why it works: By explicitly managing what's hot (frequently accessed) vs. cold (rarely accessed), you ensure hot data stays in RAM while allowing cold data to be swapped.
Performance: Keeps frequently accessed data in RAM, avoiding page faults on hot paths while allowing OS to manage cold data.

